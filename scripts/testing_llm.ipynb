{
<<<<<<< HEAD
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "obo0zjkwWjiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3ea8a31-07ff-4165-afca-8af4b936d89b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "N1o9Mbd-0eHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28ba2591-0116-4172-ef2e-bb3bf5bc04ae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
            "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.25.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p6bE_XZvWOeM"
      },
      "outputs": [],
      "source": [
        "# # PyTorch clear gpu cache\n",
        "# device = torch.cuda\n",
        "# del device\n",
        "# torch.cuda.empty_cache() # unsure if it really works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "aCdyYEAtWOcL",
        "outputId": "e957ddc2-57ca-4b16-877c-df3b1c483f61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Week Number  Lesson Number                        Lesson Title  \\\n",
              "0            1              1   Natural Language Content Analysis   \n",
              "1            1              2                         Text Access   \n",
              "2            1              3              Text Retrieval Problem   \n",
              "3            1              4  Overview of Text Retrieval Methods   \n",
              "4            1              5     Vector Space Model - Basic Idea   \n",
              "\n",
              "                                          Transcript  \n",
              "0  This lecture is about Natural Language of Cont...  \n",
              "1  In this lecture,\\nwe're going to talk about th...  \n",
              "2  This lecture is about\\nthe text retrieval prob...  \n",
              "3  This lecture is a overview of\\ntext retrieval ...  \n",
              "4  This lecture is about the\\nvector space retrie...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-adf1448c-85e3-42f0-9b03-f851a6cea440\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Week Number</th>\n",
              "      <th>Lesson Number</th>\n",
              "      <th>Lesson Title</th>\n",
              "      <th>Transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Natural Language Content Analysis</td>\n",
              "      <td>This lecture is about Natural Language of Cont...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>Text Access</td>\n",
              "      <td>In this lecture,\\nwe're going to talk about th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Text Retrieval Problem</td>\n",
              "      <td>This lecture is about\\nthe text retrieval prob...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>Overview of Text Retrieval Methods</td>\n",
              "      <td>This lecture is a overview of\\ntext retrieval ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>Vector Space Model - Basic Idea</td>\n",
              "      <td>This lecture is about the\\nvector space retrie...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-adf1448c-85e3-42f0-9b03-f851a6cea440')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-adf1448c-85e3-42f0-9b03-f851a6cea440 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-adf1448c-85e3-42f0-9b03-f851a6cea440');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-09c906cb-a378-44ba-9320-6111391542cd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-09c906cb-a378-44ba-9320-6111391542cd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-09c906cb-a378-44ba-9320-6111391542cd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# import the dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Directory path\n",
        "directory_path = '/content/drive/MyDrive/all_lectures.csv'\n",
        "\n",
        "# Initialize an empty DataFrame\n",
        "df = pd.DataFrame(columns=['Week Number', 'Lesson Number', 'Lesson Title', 'Transcript'])\n",
        "\n",
        "# Read in csv to dataframe\n",
        "df = pd.read_csv(directory_path)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "76EyM65xWOce"
      },
      "outputs": [],
      "source": [
        "# # for testing a sample dataframe\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "\n",
        "# # Testing directory path\n",
        "# directory_path = 'C:\\\\Users\\\\azaan\\\\OneDrive\\\\Documents\\\\GitHub\\\\cs410_LLM_project\\\\sample_data\\\\module_7_sample.csv'\n",
        "\n",
        "# # Initialize an empty DataFrame\n",
        "# df = pd.DataFrame(columns=['Week Number', 'Lesson Number', 'Lesson Title', 'Transcript'])\n",
        "\n",
        "# # Read in csv to dataframe\n",
        "# df = pd.read_csv(directory_path)\n",
        "\n",
        "# # Display the resulting DataFrame\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sNoeF8zWOcg",
        "outputId": "66fabd84-0fe3-4ea6-e473-d62b87d06438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# clean up words in dataset -- this includes removing stopwords\n",
        "import regex as re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords, words, brown\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"words\")\n",
        "nltk.download(\"brown\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "lemmer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# initialize dictionary\n",
        "global_dictionary  = set(words.words()) | set(brown.words())\n",
        "global_dictionary = {word.lower() for word in global_dictionary}\n",
        "remove_words = list(stop_words) # might need to use word_tokenize\n",
        "remove_words.extend(['Play', 'video', 'starting', 'at', '::', 'follow', 'transcript', 'natural', 'language', 'lecture', 'processing']) # remove the common words that are included in transcript\n",
        "\n",
        "# Now start actually cleaning the text\n",
        "def clean_text(text):\n",
        "    text = text.lower() # lowercase\n",
        "    text = text.replace('\\n', ' ') # remove newline indicator\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # case\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text) # website\n",
        "    text = re.sub(r'(\\b\\w+\\b)(?: \\1)+', r'\\1', text) # remove duplicate next word after space\n",
        "    text = re.sub(r'\\b(?![aI]\\b)\\w\\b', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Remove stopwords and only keep words in dictionary\n",
        "def remove_terms(text):\n",
        "    text = clean_text(text)\n",
        "    words = text.split()\n",
        "    # filtered_words = [word for word in words if word not in remove_words] # remove stopwords\n",
        "    filtered_words = [word for word in words if word in global_dictionary] # remove if not in global dictionary\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "df['Transcript_Cleaned'] = df['Transcript'].apply(remove_terms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "TnCCvV8bWOco",
        "outputId": "7247ccbf-321d-45ff-efee-d79e4dc2bc63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this lecture is about natural language of content analysis as you see from this picture this is really the first step to process any text data text data are in natural languages so computers have to understand natural languages to some extent in order to make use of the data so thats the topic of this lecture were going to cover three things first what is natural language processing which is the main technique for processing natural language to obtain understanding the second is the state of the art of which stands for natural language processing finally were going to cover the relation between natural language processing and text retrieval first what is well the best way to explain it is to think about if you see a text in a foreign language that you can understand now what do you have to do in order to understand that text this is basically what computers are facing so looking at the simple sentence like a dog is chasing a boy on the playground we dont have any problems understanding this sentence but imagine what the computer would have to do in order to understand it well in general it would have to do the following first it would have to know dog is a noun a verb so this is called lexical analysis or tagging and we need to figure out the syntactic categories of those words so thats the first step after that were going to figure out the structure of the sentence so for example here it shows that and the dog would go together to form a noun phrase and we wont have dog and is to go first and there are some structures that are not just right but this structure shows what we might get if we look at the sentence and try to interpret the sentence some words would go together first and then they will go together with other words so here we show we have noun phrases as intermediate components and then verbal phrases finally we have a sentence and you get this structure we need to do something called a semantic analysis or and we may have a parser accompanying the program and that would automatically created this structure at this point you would know the structure of this sentence but still you dont know the meaning of the sentence so we have to go further to semantic analysis in our mind we usually can map such a sentence to what we already know in our knowledge base for example you might imagine a dog that looks like that theres a boy and theres some activity here but for a computer would have to use symbols to denote that wed use a symbol to denote a dog and can denote a boy and then can denote a playground now there is also a chasing activity thats happening here so we have a relationship chasing that connects all these symbols so this is how a computer would obtain some understanding of this sentence now from this representation we could also further infer some other things and we might indeed naturally think of something else when we read a text and this is called inference so for example if you believe that if being chased and this person might be scared but with this rule you can see computers could also infer that this boy maybe scared so this is some extra knowledge that youd infer based on some understanding of the text you can even go further to understand why the person say at this sentence so this has to do as a use of language this is called pragmatic analysis in order to understand the speak actor of a sentence right we say something to basically achieve some goal theres some purpose there and this has to do with the use of language in this case the person who said this sentence might be reminding another person to bring back the dog that could be one possible intent to reach this level of understanding would require all of these steps and a computer would have to go through all these steps in order to completely understand this sentence yet we humans have no trouble with understanding that we instantly would get everything there is a reason for thats because we have a large knowledge base in our brain and we can use common sense knowledge to help interpret the sentence computers unfortunately are hard to obtain such understanding they dont have such a knowledge base they are still incapable of doing reasoning and uncertainties so that makes natural language processing difficult for computers but the fundamental reason why natural language processing is difficult for computers is simply because natural language has not been designed for computers natural languages are designed for us to communicate there are other languages designed for computers for example programming languages those are harder for us right so natural languages is designed to make our communication efficient as a result we omit a lot of common sense knowledge because we assume everyone knows about that we also keep a lot of ambiguities because we assume the receiver or the hearer could know how to decipher an ambiguous word based on the knowledge or the context theres no need to demand different words for different meanings we could overload the same word with different meanings without the problem because of these reasons this makes every step in natural language of processing difficult for computers ambiguity is the main difficulty and common sense and reasoning is often required thats also hard so let me give you some examples of challenges here consider the word level ambiguity the same word can have different syntactic categories for example design can be a noun or a verb the word of root may have multiple meanings so square root in math sense or the root of a plant you might be able to think about its meanings there are also syntactical ambiguities for example the main topic of this lecture natural language processing can actually be interpreted in two ways in terms of the structure think for a moment and see if you can figure that out we usually think of this as processing of natural language but you could also think of this as do say language processing is natural so this is an example of synaptic ambiguity what we have different is structures that can be applied to the same sequence of words another common example of an ambiguous sentence is the following a man saw a boy with a telescope now in this case the question is who had a telescope this is called a prepositional phrase attachment ambiguity or attachment ambiguity now we generally dont have a problem with these ambiguities because we have a lot of background knowledge to help us the ambiguity another example of difficulty is anaphora resolution so think about the sentence john persuaded bill to buy a tv for himself the question here is does himself refer to john or bill so again this is something that you have to use some background or the context to figure out finally presupposition is another problem consider the sentence he has quit smoking now this obviously implies that he smoked before so imagine a computer wants to understand all these subtle differences and meanings it would have to use a lot of knowledge to figure that out it also would have to maintain a large knowledge base of all the meanings of words and how they are connected to our common sense knowledge of the world so this is why its very difficult so as a result we are steep not perfect in fact far from perfect in understanding natural language using computers so this slide sort of gains a simplified view of state of the art we can do part of speech tagging pretty well so showed accuracy here now this number is obviously based on a certain so dont take this literally this just shows that we can do it pretty well but its still not perfect in terms of we can do partial pretty well that means we can get noun phrase structures or verb phrase structure or some segment of the sentence and this dude correct them in terms of the structure and in some evaluation results we have seen above accuracy in terms of partial of sentences again have to say these numbers are relative to the in some other the numbers might be lower most of the existing work has been evaluated using news and so a lot of these numbers are more or less toward news data think about social media data the accuracy likely is lower in terms of a semantical analysis we are far from being able to do a complete understanding of a sentence but we have some techniques that would allow us to do partial understanding of the sentence so could mention some of them for example we have techniques that can allow us to extract the entities and relations mentioned in text articles for example recognizing dimensions of people locations organizations in text so this is called entity extraction we may be able to recognize the relations for example this person visited that place or this person met that person or this company acquired another company such relations can be extracted by using the computer current natural language processing techniques theyre not perfect but they can do well for some entities some entities are harder than others we can also do word sense disintegration to some extend we have to figure out whether this word in this sentence would have certain meaning in another context the computer could figure out it has a different meaning again its not perfect but you can do something in that direction we can also do sentiment analysis meaning to figure out whether a sentence is positive or negative this is especially useful for review analysis for example so these are examples of semantic analysis and they help us to obtain partial understanding of the sentences its not giving us a complete understanding as showed it before for this sentence but it would still help us gain understanding of the content and these can be useful in terms of inference we are not there yet probably because of the general difficulty of inference and uncertainties this is a general challenge in artificial intelligence now thats probably also because we dont have complete semantical representation for natural inaudible text so this is hard yet in some domains perhaps in limited domains when you have a lot of restrictions on the word uses you may be able to perform inference to some extent but in general we can not really do that reliably speech act analysis is also far from being done and we can only do that analysis for very special cases so this roughly gives you some idea about the state of the art and then we also talk a little bit about what we cant do and so we cant even do part of speech tagging now this looks like a simple task but think about the example here the two uses off may have different syntactic categories if you try to make a fine grained distinctions its not that easy to figure out such differences its also hard to do general complete and again the same sentence that you saw before is example this ambiguity can be very hard to and you can imagine example where you have to use a lot of knowledge in the context of the sentence or from the background in order to figure out who actually had the telescope so although the sentence looks very simple it actually is pretty hard and in cases when the sentence is very long imagine it has four or five prepositional phrases and there are even more possibilities to figure out its also harder to do precise deep semantic analysis so an example in the sentence john owns a restaurant how do we define owns exactly the word own it is something that we can understand but its very hard to precisely describe the meaning of own for computers so as a result we have a robust and a general natural language processing techniques that can process a lot of text data in a shallow way meaning we only do superficial analysis for example parts of speech tagging or a partial or recognizing sentiment and those are not deep understanding because were not really understanding the exact meaning of the sentence on the other hand of the deep understanding techniques tend not to scale up well meaning that they would fill only some restricted text and if you dont restrict the text domain or the use of words then these techniques tend not to work well they may work well based on machine learning techniques on the data that are similar to the training data that the program has been trained on but they generally wouldnt work well on the data that are very different from the training data so this pretty much summarizes the state of the art of natural language processing of course within such a short amount of time we cant really give you a complete view of which is a big field and id expect to see multiple courses on natural language processing topic itself but because of its relevance to the topic that we talk about its useful for you to know the background in case you happen to be exposed to that so what does that mean for text retrieval well in text retrieval we are dealing with all kinds of text its very hard to restrict text to a certain domain and we also are often dealing with a lot of text data so that means the techniques must be general robust and efficient and that just implies today we can only use fairly shallow techniques for text retrieval in fact most search engines today use something called a bag of words representation now this is probably the simplest representation you can possibly think of that is to turn text data into simply a bag of words meaning well keep individual words but well ignore all the orders of words and well keep duplicated occurrences of words so this is called a bag of words representation when you represent text in this way you ignore a lot of valid information that just makes it harder to understand the exact meaning of a sentence because weve lost the order but yet this representation tends to actually work pretty well for most search tasks and this was partly because the search task is not all that difficult if you see matching of some of the query words in a text document chances are that document is about the topic although there are exceptions so in comparison of some other tasks for example machine translation would require you to understand the language accurately otherwise the translation would be wrong so in comparison such tasks are all relatively easy such a representation is often sufficient and thats also the representation that the major search engines today like a or bing are using of course put in parentheses but not all of course there are many queries that are not answered well by the current search engines and they do require the replantation that would go beyond bag of words replantation that would require more natural language processing to be done there was another reason why we have not used the sophisticated techniques in modern search engines and thats because some retrieval techniques actually naturally solved the problem of so one example is word sense disintegration think about a word like java it could mean coffee or it could mean program language if you look at the word it would be ambiguous but when the user uses the word in the query usually there are other words for example looking for usage of java when have there that implies java means program language and that contest can help us naturally prefer documents which java is referring to program languages because those documents would probably match as well if java occurs in that documents where it means coffee then you would never match or with very small probability so this is the case when some retrieval techniques naturally achieve the goal of word another example is some technique called feedback which we will talk about later in some of the lectures this technique would allow us to additional words to the query and those additional words could be related to the query words and these words can help matching documents where the original query words have not occurred so this achieves to some extent semantic matching of terms so those techniques also helped us bypass some of the difficulties in natural language processing however in the long run we still need a deeper natural language processing techniques in order to improve the accuracy of the current search engines and its particularly needed for complex search tasks or for question and answering has recently launched a knowledge graph and this is one step toward that goal because knowledge graph would contain entities and their relations and this goes beyond the simple bag of words replantation and such technique should help us improve the search engine utility significantly although this is the open topic for research and exploration in sum in this lecture we talked about what is and weve talked about the state of that techniques what we can do what we cannot do and finally we also explain why the bag of words replantation remains the dominant replantation used in modern search engines even though deeper would be needed for future search engines if you want to know more you can take a look at some additional readings only cited one here and thats a good starting point thanks'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df['Transcript_Cleaned'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "syXKS19ZWOcp"
      },
      "outputs": [],
      "source": [
        "# # Create bigrams and trigrams from data\n",
        "\n",
        "# # Function to filter bigrams or trigrams\n",
        "# def ngram_filter(ngram):\n",
        "#     tags = nltk.pos_tag(ngram)\n",
        "#     if not all(tag[1] in ['JJ', 'NN'] for tag in tags):\n",
        "#         return False\n",
        "#     if any(word in stop_words for word in ngram):\n",
        "#         return False\n",
        "#     if 'n' in ngram or 't' in ngram:\n",
        "#         return False\n",
        "#     if 'PRON' in ngram:\n",
        "#         return False\n",
        "#     return True\n",
        "\n",
        "# # Function to find top ngrams\n",
        "# def find_top_ngrams(texts, ngram_measures, min_freq=50, min_pmi=5, top_k=100):\n",
        "#     finder = nltk.collocations.BigramCollocationFinder.from_documents(texts)\n",
        "#     finder.apply_freq_filter(min_freq)\n",
        "#     ngram_scores = finder.score_ngrams(ngram_measures.pmi)\n",
        "#     filtered_ngrams = [ngram for ngram, pmi in ngram_scores if ngram_filter(ngram) and pmi > min_pmi]\n",
        "#     return [' '.join(ngram) for ngram in filtered_ngrams][:top_k]\n",
        "\n",
        "# bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "# bigrams = find_top_ngrams([text.split() for text in df['Transcript_Cleaned']], bigram_measures)\n",
        "# trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
        "# trigrams = find_top_ngrams([text.split() for text in df['Transcript_Cleaned']], trigram_measures)\n",
        "\n",
        "# # Function to replace ngrams in text\n",
        "# def replace_ngrams(text):\n",
        "#     for gram in trigrams:\n",
        "#         text = text.replace(gram, '_'.join(gram.split()))\n",
        "#     for gram in bigrams:\n",
        "#         text = text.replace(gram, '_'.join(gram.split()))\n",
        "#     return text\n",
        "\n",
        "# # Apply ngram replacements to the text\n",
        "# df['Grams'] = df['Transcript_Cleaned'].map(replace_ngrams)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Tkc8EztMWOcs"
      },
      "outputs": [],
      "source": [
        "# Tokenize reviews + remove stop words + filter only nouns\n",
        "def tokenize_and_filter(text):\n",
        "    words = nltk.word_tokenize(text)\n",
        "    words = [word.lower() for word in words] # if word.lower() not in stop_words and len(word) > 2]\n",
        "    # print(words)\n",
        "    # pos_comment = nltk.pos_tag(words)\n",
        "    # filtered = [word[0] for word in pos_comment if word[1] in ['NN']]\n",
        "    return words #filtered\n",
        "\n",
        "# If using transcript instead of grams\n",
        "df['Transcript_Cleaned'] = df['Transcript_Cleaned'].map(tokenize_and_filter)\n",
        "\n",
        "# If using Grams instead of transcript\n",
        "# df['Grams'] = df['Grams'].map(tokenize_and_filter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHNVhuRrWOcx",
        "outputId": "5d599257-128a-4b60-8556-d7c02ca144cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this',\n",
              " 'lecture',\n",
              " 'is',\n",
              " 'about',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'of',\n",
              " 'content',\n",
              " 'analysis',\n",
              " 'as',\n",
              " 'you',\n",
              " 'see',\n",
              " 'from',\n",
              " 'this',\n",
              " 'picture',\n",
              " 'this',\n",
              " 'is',\n",
              " 'really',\n",
              " 'the',\n",
              " 'first',\n",
              " 'step',\n",
              " 'to',\n",
              " 'process',\n",
              " 'any',\n",
              " 'text',\n",
              " 'data',\n",
              " 'text',\n",
              " 'data',\n",
              " 'are',\n",
              " 'in',\n",
              " 'natural',\n",
              " 'languages',\n",
              " 'so',\n",
              " 'computers',\n",
              " 'have',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'natural',\n",
              " 'languages',\n",
              " 'to',\n",
              " 'some',\n",
              " 'extent',\n",
              " 'in',\n",
              " 'order',\n",
              " 'to',\n",
              " 'make',\n",
              " 'use',\n",
              " 'of',\n",
              " 'the',\n",
              " 'data',\n",
              " 'so',\n",
              " 'thats',\n",
              " 'the',\n",
              " 'topic',\n",
              " 'of',\n",
              " 'this',\n",
              " 'lecture',\n",
              " 'were',\n",
              " 'going',\n",
              " 'to',\n",
              " 'cover',\n",
              " 'three',\n",
              " 'things',\n",
              " 'first',\n",
              " 'what',\n",
              " 'is',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'which',\n",
              " 'is',\n",
              " 'the',\n",
              " 'main',\n",
              " 'technique',\n",
              " 'for',\n",
              " 'processing',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'to',\n",
              " 'obtain',\n",
              " 'understanding',\n",
              " 'the',\n",
              " 'second',\n",
              " 'is',\n",
              " 'the',\n",
              " 'state',\n",
              " 'of',\n",
              " 'the',\n",
              " 'art',\n",
              " 'of',\n",
              " 'which',\n",
              " 'stands',\n",
              " 'for',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'finally',\n",
              " 'were',\n",
              " 'going',\n",
              " 'to',\n",
              " 'cover',\n",
              " 'the',\n",
              " 'relation',\n",
              " 'between',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'and',\n",
              " 'text',\n",
              " 'retrieval',\n",
              " 'first',\n",
              " 'what',\n",
              " 'is',\n",
              " 'well',\n",
              " 'the',\n",
              " 'best',\n",
              " 'way',\n",
              " 'to',\n",
              " 'explain',\n",
              " 'it',\n",
              " 'is',\n",
              " 'to',\n",
              " 'think',\n",
              " 'about',\n",
              " 'if',\n",
              " 'you',\n",
              " 'see',\n",
              " 'a',\n",
              " 'text',\n",
              " 'in',\n",
              " 'a',\n",
              " 'foreign',\n",
              " 'language',\n",
              " 'that',\n",
              " 'you',\n",
              " 'can',\n",
              " 'understand',\n",
              " 'now',\n",
              " 'what',\n",
              " 'do',\n",
              " 'you',\n",
              " 'have',\n",
              " 'to',\n",
              " 'do',\n",
              " 'in',\n",
              " 'order',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'that',\n",
              " 'text',\n",
              " 'this',\n",
              " 'is',\n",
              " 'basically',\n",
              " 'what',\n",
              " 'computers',\n",
              " 'are',\n",
              " 'facing',\n",
              " 'so',\n",
              " 'looking',\n",
              " 'at',\n",
              " 'the',\n",
              " 'simple',\n",
              " 'sentence',\n",
              " 'like',\n",
              " 'a',\n",
              " 'dog',\n",
              " 'is',\n",
              " 'chasing',\n",
              " 'a',\n",
              " 'boy',\n",
              " 'on',\n",
              " 'the',\n",
              " 'playground',\n",
              " 'we',\n",
              " 'dont',\n",
              " 'have',\n",
              " 'any',\n",
              " 'problems',\n",
              " 'understanding',\n",
              " 'this',\n",
              " 'sentence',\n",
              " 'but',\n",
              " 'imagine',\n",
              " 'what',\n",
              " 'the',\n",
              " 'computer',\n",
              " 'would',\n",
              " 'have',\n",
              " 'to',\n",
              " 'do',\n",
              " 'in',\n",
              " 'order',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'it',\n",
              " 'well',\n",
              " 'in',\n",
              " 'general',\n",
              " 'it',\n",
              " 'would',\n",
              " 'have',\n",
              " 'to',\n",
              " 'do',\n",
              " 'the',\n",
              " 'following',\n",
              " 'first',\n",
              " 'it',\n",
              " 'would',\n",
              " 'have',\n",
              " 'to',\n",
              " 'know',\n",
              " 'dog',\n",
              " 'is',\n",
              " 'a',\n",
              " 'noun',\n",
              " 'a',\n",
              " 'verb',\n",
              " 'so',\n",
              " 'this',\n",
              " 'is',\n",
              " 'called',\n",
              " 'lexical',\n",
              " 'analysis',\n",
              " 'or',\n",
              " 'tagging',\n",
              " 'and',\n",
              " 'we',\n",
              " 'need',\n",
              " 'to',\n",
              " 'figure',\n",
              " 'out',\n",
              " 'the',\n",
              " 'syntactic',\n",
              " 'categories',\n",
              " 'of',\n",
              " 'those',\n",
              " 'words',\n",
              " 'so',\n",
              " 'thats',\n",
              " 'the',\n",
              " 'first',\n",
              " 'step',\n",
              " 'after',\n",
              " 'that',\n",
              " 'were',\n",
              " 'going',\n",
              " 'to',\n",
              " 'figure',\n",
              " 'out',\n",
              " 'the',\n",
              " 'structure',\n",
              " 'of',\n",
              " 'the',\n",
              " 'sentence',\n",
              " 'so',\n",
              " 'for',\n",
              " 'example',\n",
              " 'here',\n",
              " 'it',\n",
              " 'shows',\n",
              " 'that',\n",
              " 'and',\n",
              " 'the',\n",
              " 'dog',\n",
              " 'would',\n",
              " 'go',\n",
              " 'together',\n",
              " 'to',\n",
              " 'form',\n",
              " 'a',\n",
              " 'noun',\n",
              " 'phrase',\n",
              " 'and',\n",
              " 'we',\n",
              " 'wont',\n",
              " 'have',\n",
              " 'dog',\n",
              " 'and',\n",
              " 'is',\n",
              " 'to',\n",
              " 'go',\n",
              " 'first',\n",
              " 'and',\n",
              " 'there',\n",
              " 'are',\n",
              " 'some',\n",
              " 'structures',\n",
              " 'that',\n",
              " 'are',\n",
              " 'not',\n",
              " 'just',\n",
              " 'right',\n",
              " 'but',\n",
              " 'this',\n",
              " 'structure',\n",
              " 'shows',\n",
              " 'what',\n",
              " 'we',\n",
              " 'might',\n",
              " 'get',\n",
              " 'if',\n",
              " 'we',\n",
              " 'look',\n",
              " 'at',\n",
              " 'the',\n",
              " 'sentence',\n",
              " 'and',\n",
              " 'try',\n",
              " 'to',\n",
              " 'interpret',\n",
              " 'the',\n",
              " 'sentence',\n",
              " 'some',\n",
              " 'words',\n",
              " 'would',\n",
              " 'go',\n",
              " 'together',\n",
              " 'first',\n",
              " 'and',\n",
              " 'then',\n",
              " 'they',\n",
              " 'will',\n",
              " 'go',\n",
              " 'together',\n",
              " 'with',\n",
              " 'other',\n",
              " 'words',\n",
              " 'so',\n",
              " 'here',\n",
              " 'we',\n",
              " 'show',\n",
              " 'we',\n",
              " 'have',\n",
              " 'noun',\n",
              " 'phrases',\n",
              " 'as',\n",
              " 'intermediate',\n",
              " 'components',\n",
              " 'and',\n",
              " 'then',\n",
              " 'verbal',\n",
              " 'phrases',\n",
              " 'finally',\n",
              " 'we',\n",
              " 'have',\n",
              " 'a',\n",
              " 'sentence',\n",
              " 'and',\n",
              " 'you',\n",
              " 'get',\n",
              " 'this',\n",
              " 'structure',\n",
              " 'we',\n",
              " 'need',\n",
              " 'to',\n",
              " 'do',\n",
              " 'something',\n",
              " 'called',\n",
              " 'a',\n",
              " 'semantic',\n",
              " 'analysis',\n",
              " 'or',\n",
              " 'and',\n",
              " 'we',\n",
              " 'may',\n",
              " 'have',\n",
              " 'a',\n",
              " 'parser',\n",
              " 'accompanying',\n",
              " 'the',\n",
              " 'program',\n",
              " 'and',\n",
              " 'that',\n",
              " 'would',\n",
              " 'automatically',\n",
              " 'created',\n",
              " 'this',\n",
              " 'structure',\n",
              " 'at',\n",
              " 'this',\n",
              " 'point',\n",
              " 'you',\n",
              " 'would',\n",
              " 'know',\n",
              " 'the',\n",
              " 'structure',\n",
              " 'of',\n",
              " 'this',\n",
              " 'sentence',\n",
              " 'but',\n",
              " 'still',\n",
              " 'you',\n",
              " 'dont',\n",
              " 'know',\n",
              " 'the',\n",
              " 'meaning',\n",
              " 'of',\n",
              " 'the',\n",
              " 'sentence',\n",
              " 'so',\n",
              " 'we',\n",
              " 'have',\n",
              " 'to',\n",
              " 'go',\n",
              " 'further',\n",
              " 'to',\n",
              " 'semantic',\n",
              " 'analysis',\n",
              " 'in',\n",
              " 'our',\n",
              " 'mind',\n",
              " 'we',\n",
              " 'usually',\n",
              " 'can',\n",
              " 'map',\n",
              " 'such',\n",
              " 'a',\n",
              " 'sentence',\n",
              " 'to',\n",
              " 'what',\n",
              " 'we',\n",
              " 'already',\n",
              " 'know',\n",
              " 'in',\n",
              " 'our',\n",
              " 'knowledge',\n",
              " 'base',\n",
              " 'for',\n",
              " 'example',\n",
              " 'you',\n",
              " 'might',\n",
              " 'imagine',\n",
              " 'a',\n",
              " 'dog',\n",
              " 'that',\n",
              " 'looks',\n",
              " 'like',\n",
              " 'that',\n",
              " 'theres',\n",
              " 'a',\n",
              " 'boy',\n",
              " 'and',\n",
              " 'theres',\n",
              " 'some',\n",
              " 'activity',\n",
              " 'here',\n",
              " 'but',\n",
              " 'for',\n",
              " 'a',\n",
              " 'computer',\n",
              " 'would',\n",
              " 'have',\n",
              " 'to',\n",
              " 'use',\n",
              " 'symbols',\n",
              " 'to',\n",
              " 'denote',\n",
              " 'that',\n",
              " 'wed',\n",
              " 'use',\n",
              " 'a',\n",
              " 'symbol',\n",
              " 'to',\n",
              " 'denote',\n",
              " 'a',\n",
              " 'dog',\n",
              " 'and',\n",
              " 'can',\n",
              " 'denote',\n",
              " 'a',\n",
              " 'boy',\n",
              " 'and',\n",
              " 'then',\n",
              " 'can',\n",
              " 'denote',\n",
              " 'a',\n",
              " 'playground',\n",
              " 'now',\n",
              " 'there',\n",
              " 'is',\n",
              " 'also',\n",
              " 'a',\n",
              " 'chasing',\n",
              " 'activity',\n",
              " 'thats',\n",
              " 'happening',\n",
              " 'here',\n",
              " 'so',\n",
              " 'we',\n",
              " 'have',\n",
              " 'a',\n",
              " 'relationship',\n",
              " 'chasing',\n",
              " 'that',\n",
              " 'connects',\n",
              " 'all',\n",
              " 'these',\n",
              " 'symbols',\n",
              " 'so',\n",
              " 'this',\n",
              " 'is',\n",
              " 'how',\n",
              " 'a',\n",
              " 'computer',\n",
              " 'would',\n",
              " 'obtain',\n",
              " 'some',\n",
              " 'understanding',\n",
              " 'of',\n",
              " 'this',\n",
              " 'sentence',\n",
              " 'now',\n",
              " 'from',\n",
              " 'this',\n",
              " 'representation',\n",
              " 'we',\n",
              " 'could',\n",
              " 'also',\n",
              " 'further',\n",
              " 'infer',\n",
              " 'some',\n",
              " 'other',\n",
              " 'things',\n",
              " 'and',\n",
              " 'we',\n",
              " 'might',\n",
              " 'indeed',\n",
              " 'naturally',\n",
              " 'think',\n",
              " 'of',\n",
              " 'something',\n",
              " 'else',\n",
              " 'when',\n",
              " 'we',\n",
              " 'read',\n",
              " 'a',\n",
              " 'text',\n",
              " 'and',\n",
              " 'this',\n",
              " 'is',\n",
              " 'called',\n",
              " 'inference',\n",
              " 'so',\n",
              " 'for',\n",
              " 'example',\n",
              " 'if',\n",
              " 'you',\n",
              " 'believe',\n",
              " 'that',\n",
              " 'if',\n",
              " 'being',\n",
              " 'chased',\n",
              " 'and',\n",
              " 'this',\n",
              " 'person',\n",
              " 'might',\n",
              " 'be',\n",
              " 'scared',\n",
              " 'but',\n",
              " 'with',\n",
              " 'this',\n",
              " 'rule',\n",
              " 'you',\n",
              " 'can',\n",
              " 'see',\n",
              " 'computers',\n",
              " 'could',\n",
              " 'also',\n",
              " 'infer',\n",
              " 'that',\n",
              " 'this',\n",
              " 'boy',\n",
              " 'maybe',\n",
              " 'scared',\n",
              " 'so',\n",
              " 'this',\n",
              " 'is',\n",
              " 'some',\n",
              " 'extra',\n",
              " 'knowledge',\n",
              " 'that',\n",
              " 'youd',\n",
              " 'infer',\n",
              " 'based',\n",
              " 'on',\n",
              " 'some',\n",
              " 'understanding',\n",
              " 'of',\n",
              " 'the',\n",
              " 'text',\n",
              " 'you',\n",
              " 'can',\n",
              " 'even',\n",
              " 'go',\n",
              " 'further',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'why',\n",
              " 'the',\n",
              " 'person',\n",
              " 'say',\n",
              " 'at',\n",
              " 'this',\n",
              " 'sentence',\n",
              " 'so',\n",
              " 'this',\n",
              " 'has',\n",
              " 'to',\n",
              " 'do',\n",
              " 'as',\n",
              " 'a',\n",
              " 'use',\n",
              " 'of',\n",
              " 'language',\n",
              " 'this',\n",
              " 'is',\n",
              " 'called',\n",
              " 'pragmatic',\n",
              " 'analysis',\n",
              " 'in',\n",
              " 'order',\n",
              " 'to',\n",
              " 'understand',\n",
              " 'the',\n",
              " 'speak',\n",
              " 'actor',\n",
              " 'of',\n",
              " 'a',\n",
              " 'sentence',\n",
              " 'right',\n",
              " 'we',\n",
              " 'say',\n",
              " 'something',\n",
              " 'to',\n",
              " 'basically',\n",
              " 'achieve',\n",
              " 'some',\n",
              " 'goal',\n",
              " 'theres',\n",
              " 'some',\n",
              " 'purpose',\n",
              " 'there',\n",
              " 'and',\n",
              " 'this',\n",
              " 'has',\n",
              " 'to',\n",
              " 'do',\n",
              " 'with',\n",
              " 'the',\n",
              " 'use',\n",
              " 'of',\n",
              " 'language',\n",
              " 'in',\n",
              " 'this',\n",
              " 'case',\n",
              " 'the',\n",
              " 'person',\n",
              " 'who',\n",
              " 'said',\n",
              " 'this',\n",
              " 'sentence',\n",
              " 'might',\n",
              " 'be',\n",
              " 'reminding',\n",
              " 'another',\n",
              " 'person',\n",
              " 'to',\n",
              " 'bring',\n",
              " 'back',\n",
              " 'the',\n",
              " 'dog',\n",
              " 'that',\n",
              " 'could',\n",
              " 'be',\n",
              " 'one',\n",
              " 'possible',\n",
              " 'intent',\n",
              " 'to',\n",
              " 'reach',\n",
              " 'this',\n",
              " 'level',\n",
              " 'of',\n",
              " 'understanding',\n",
              " 'would',\n",
              " 'require',\n",
              " 'all',\n",
              " 'of',\n",
              " 'these',\n",
              " 'steps',\n",
              " 'and',\n",
              " 'a',\n",
              " 'computer',\n",
              " 'would',\n",
              " 'have',\n",
              " 'to',\n",
              " 'go',\n",
              " 'through',\n",
              " 'all',\n",
              " 'these',\n",
              " 'steps',\n",
              " 'in',\n",
              " 'order',\n",
              " 'to',\n",
              " 'completely',\n",
              " 'understand',\n",
              " 'this',\n",
              " 'sentence',\n",
              " 'yet',\n",
              " 'we',\n",
              " 'humans',\n",
              " 'have',\n",
              " 'no',\n",
              " 'trouble',\n",
              " 'with',\n",
              " 'understanding',\n",
              " 'that',\n",
              " 'we',\n",
              " 'instantly',\n",
              " 'would',\n",
              " 'get',\n",
              " 'everything',\n",
              " 'there',\n",
              " 'is',\n",
              " 'a',\n",
              " 'reason',\n",
              " 'for',\n",
              " 'thats',\n",
              " 'because',\n",
              " 'we',\n",
              " 'have',\n",
              " 'a',\n",
              " 'large',\n",
              " 'knowledge',\n",
              " 'base',\n",
              " 'in',\n",
              " 'our',\n",
              " 'brain',\n",
              " 'and',\n",
              " 'we',\n",
              " 'can',\n",
              " 'use',\n",
              " 'common',\n",
              " 'sense',\n",
              " 'knowledge',\n",
              " 'to',\n",
              " 'help',\n",
              " 'interpret',\n",
              " 'the',\n",
              " 'sentence',\n",
              " 'computers',\n",
              " 'unfortunately',\n",
              " 'are',\n",
              " 'hard',\n",
              " 'to',\n",
              " 'obtain',\n",
              " 'such',\n",
              " 'understanding',\n",
              " 'they',\n",
              " 'dont',\n",
              " 'have',\n",
              " 'such',\n",
              " 'a',\n",
              " 'knowledge',\n",
              " 'base',\n",
              " 'they',\n",
              " 'are',\n",
              " 'still',\n",
              " 'incapable',\n",
              " 'of',\n",
              " 'doing',\n",
              " 'reasoning',\n",
              " 'and',\n",
              " 'uncertainties',\n",
              " 'so',\n",
              " 'that',\n",
              " 'makes',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'difficult',\n",
              " 'for',\n",
              " 'computers',\n",
              " 'but',\n",
              " 'the',\n",
              " 'fundamental',\n",
              " 'reason',\n",
              " 'why',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'is',\n",
              " 'difficult',\n",
              " 'for',\n",
              " 'computers',\n",
              " 'is',\n",
              " 'simply',\n",
              " 'because',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'has',\n",
              " 'not',\n",
              " 'been',\n",
              " 'designed',\n",
              " 'for',\n",
              " 'computers',\n",
              " 'natural',\n",
              " 'languages',\n",
              " 'are',\n",
              " 'designed',\n",
              " 'for',\n",
              " 'us',\n",
              " 'to',\n",
              " 'communicate',\n",
              " 'there',\n",
              " 'are',\n",
              " 'other',\n",
              " 'languages',\n",
              " 'designed',\n",
              " 'for',\n",
              " 'computers',\n",
              " 'for',\n",
              " 'example',\n",
              " 'programming',\n",
              " 'languages',\n",
              " 'those',\n",
              " 'are',\n",
              " 'harder',\n",
              " 'for',\n",
              " 'us',\n",
              " 'right',\n",
              " 'so',\n",
              " 'natural',\n",
              " 'languages',\n",
              " 'is',\n",
              " 'designed',\n",
              " 'to',\n",
              " 'make',\n",
              " 'our',\n",
              " 'communication',\n",
              " 'efficient',\n",
              " 'as',\n",
              " 'a',\n",
              " 'result',\n",
              " 'we',\n",
              " 'omit',\n",
              " 'a',\n",
              " 'lot',\n",
              " 'of',\n",
              " 'common',\n",
              " 'sense',\n",
              " 'knowledge',\n",
              " 'because',\n",
              " 'we',\n",
              " 'assume',\n",
              " 'everyone',\n",
              " 'knows',\n",
              " 'about',\n",
              " 'that',\n",
              " 'we',\n",
              " 'also',\n",
              " 'keep',\n",
              " 'a',\n",
              " 'lot',\n",
              " 'of',\n",
              " 'ambiguities',\n",
              " 'because',\n",
              " 'we',\n",
              " 'assume',\n",
              " 'the',\n",
              " 'receiver',\n",
              " 'or',\n",
              " 'the',\n",
              " 'hearer',\n",
              " 'could',\n",
              " 'know',\n",
              " 'how',\n",
              " 'to',\n",
              " 'decipher',\n",
              " 'an',\n",
              " 'ambiguous',\n",
              " 'word',\n",
              " 'based',\n",
              " 'on',\n",
              " 'the',\n",
              " 'knowledge',\n",
              " 'or',\n",
              " 'the',\n",
              " 'context',\n",
              " 'theres',\n",
              " 'no',\n",
              " 'need',\n",
              " 'to',\n",
              " 'demand',\n",
              " 'different',\n",
              " 'words',\n",
              " 'for',\n",
              " 'different',\n",
              " 'meanings',\n",
              " 'we',\n",
              " 'could',\n",
              " 'overload',\n",
              " 'the',\n",
              " 'same',\n",
              " 'word',\n",
              " 'with',\n",
              " 'different',\n",
              " 'meanings',\n",
              " 'without',\n",
              " 'the',\n",
              " 'problem',\n",
              " 'because',\n",
              " 'of',\n",
              " 'these',\n",
              " 'reasons',\n",
              " 'this',\n",
              " 'makes',\n",
              " 'every',\n",
              " 'step',\n",
              " 'in',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'of',\n",
              " 'processing',\n",
              " 'difficult',\n",
              " 'for',\n",
              " 'computers',\n",
              " 'ambiguity',\n",
              " 'is',\n",
              " 'the',\n",
              " 'main',\n",
              " 'difficulty',\n",
              " 'and',\n",
              " 'common',\n",
              " 'sense',\n",
              " 'and',\n",
              " 'reasoning',\n",
              " 'is',\n",
              " 'often',\n",
              " 'required',\n",
              " 'thats',\n",
              " 'also',\n",
              " 'hard',\n",
              " 'so',\n",
              " 'let',\n",
              " 'me',\n",
              " 'give',\n",
              " 'you',\n",
              " 'some',\n",
              " 'examples',\n",
              " 'of',\n",
              " 'challenges',\n",
              " 'here',\n",
              " 'consider',\n",
              " 'the',\n",
              " 'word',\n",
              " 'level',\n",
              " 'ambiguity',\n",
              " 'the',\n",
              " 'same',\n",
              " 'word',\n",
              " 'can',\n",
              " 'have',\n",
              " 'different',\n",
              " 'syntactic',\n",
              " 'categories',\n",
              " 'for',\n",
              " 'example',\n",
              " 'design',\n",
              " 'can',\n",
              " 'be',\n",
              " 'a',\n",
              " 'noun',\n",
              " 'or',\n",
              " 'a',\n",
              " 'verb',\n",
              " 'the',\n",
              " 'word',\n",
              " 'of',\n",
              " 'root',\n",
              " 'may',\n",
              " 'have',\n",
              " 'multiple',\n",
              " 'meanings',\n",
              " 'so',\n",
              " 'square',\n",
              " 'root',\n",
              " 'in',\n",
              " 'math',\n",
              " 'sense',\n",
              " 'or',\n",
              " 'the',\n",
              " 'root',\n",
              " 'of',\n",
              " 'a',\n",
              " 'plant',\n",
              " 'you',\n",
              " 'might',\n",
              " 'be',\n",
              " 'able',\n",
              " 'to',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# df['Transcript_Cleaned'][0] #['the']\n",
        "df['Transcript_Cleaned'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "g7-szEIjWOc2"
      },
      "outputs": [],
      "source": [
        "# now I will make embeddings for my words, let's see if it works\n",
        "# Replace Transcript_Cleaned with grams if using that method\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import  torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "results = set()\n",
        "df['Transcript_Cleaned'].apply(results.update)\n",
        "vocab_size = len(results)\n",
        "\n",
        "# Create a vocabulary dictionary\n",
        "word_to_index = {word: idx for idx, word in enumerate(results)}\n",
        "\n",
        "# Convert words to indices in your DataFrame\n",
        "# AKA Encode these\n",
        "# df['Words_indices'] = df['Transcript_Cleaned'].apply(lambda x: [word_to_index[word] for word in x])\n",
        "def words_to_indices(words):\n",
        "    return [word_to_index[word] for word in words]\n",
        "df['Words_indices'] = df['Transcript_Cleaned'].apply(words_to_indices)\n",
        "\n",
        "# Create a reverse dictionary\n",
        "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
        "\n",
        "# Function to convert indices back to words\n",
        "def indices_to_words(indices):\n",
        "    return [index_to_word[idx] for idx in indices]\n",
        "\n",
        "# Aka Decode this column\n",
        "# df['Decoded_Words'] = df['Words_indices'].apply(indices_to_words)\n",
        "\n",
        "# Pad sequences to a specified length (e.g., maxlen)\n",
        "maxlen = 200  # You can adjust this based on your data\n",
        "padded_indices = pad_sequence([torch.LongTensor(seq) for seq in df['Words_indices']], batch_first=True, padding_value=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEqXuUrsWOc_",
        "outputId": "a870be9f-f820-4bc8-fa70-dff2569bbf83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "tensor([[   0,    0,    0,  ...,    0,    0,    0],\n",
            "        [   0,    0,    0,  ...,    0,    0,    0],\n",
            "        [   0,    0,    0,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [2369, 3362, 1539,  ..., 1874,  785,  943],\n",
            "        [   0,    0,    0,  ...,    0,    0,    0],\n",
            "        [   0,    0,    0,  ...,    0,    0,    0]], device='cuda:0')\n",
            "tensor([[   0,    0,    0,  ...,    0,    0,    0],\n",
            "        [   0,    0,    0,  ...,    0,    0,    0],\n",
            "        [   0,    0,    0,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [3362, 1539, 3238,  ...,  785,  943, 1523],\n",
            "        [   0,    0,    0,  ...,    0,    0,    0],\n",
            "        [   0,    0,    0,  ...,    0,    0,    0]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# make a batch and set up parameters\n",
        "block_size = 256\n",
        "batch_size = 128\n",
        "max_iters = 10000\n",
        "learning_rate = 1e-4\n",
        "eval_iters = 250\n",
        "# new\n",
        "n_embd = 128\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "n_head = 6\n",
        "# reduce gpu usage\n",
        "accumulation_steps = 6  # Accumulate gradients over 4 batches before performing optimization step\n",
        "\n",
        "# change to gpu\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "# Flatten the padded indices used to identify each word\n",
        "data = flattened_indices = padded_indices.view(-1)\n",
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "# print(len(data))\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "x, y = get_batch('train')\n",
        "print(x)\n",
        "print(y)\n",
        "\n",
        "# The result has many zeros, which is normal for a padded dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K28XGU2jWOdB",
        "outputId": "f36c3ae0-bf97-4493-dae5-5575999a0876"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0008)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "sum(data == 0) / sum(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pNM5HczWOdF"
      },
      "source": [
        "# LLM Start Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "77_3MzZVWOda"
      },
      "outputs": [],
      "source": [
        "# Estimating losses function\n",
        "@torch.no_grad()\n",
        "\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# This function is able to estimate the losses for the training iterations of the data\n",
        "# uing model.eval in order to prevent some aspects of the model to run during that time\n",
        "# It monitors the performance of loss and if it decreases per every iteration\n",
        "# These losses print out when model is running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYchx6UhWOdc",
        "outputId": "e37710a8-5477-4a51-c503-f9278a6027e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
              "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
              "        [1., 1., 1.,  ..., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "torch.tril(torch.ones(block_size, block_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "wPptlyWgWOdo"
      },
      "outputs": [],
      "source": [
        "# Scaled dot product attention\n",
        "class Head(nn.Module):\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        # These 3 variables, key, query, and value, are all very important in calculating attention\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        # This \"buffer\" is used as a mask for future tokens in attention\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        # B = batch size, T = sequence length, C = original embedding dimension\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        # Even though k and q are initialized in the same way, nn.Linear randomizes the initial weights to create these tensors\n",
        "\n",
        "        # create attention scores\n",
        "        weights = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
        "        # this is used to caclulate attention scores, a standard formula of getting dot product of q and k\n",
        "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        # this sets attention scores for future tokens to -inf (negative infinity), so that the model only pays attention to the old scores and not the new ones\n",
        "        #  it overwrites the attention scores calculated in the previous step for future tokens. In each iteration, the model is exposed to a partially\n",
        "        # revealed sequence, allowing it to attend only to past tokens. This is a form of autoregressive training.\n",
        "        weights = F.softmax(weights, dim=-1)\n",
        "        # normalized step\n",
        "        drop = self.dropout(weights)\n",
        "        # step to randomly ignore random nodes in order to prevent overfitting and codependence\n",
        "\n",
        "        # weighted aggregation of values\n",
        "        v = self.value(x)\n",
        "        out = drop @ v\n",
        "        # the attention scores in drop are dot product with the value tesnor to get the final outputs\n",
        "        return out\n",
        "\n",
        "# Note:\n",
        "# In transformers, the key and query vectors are typically designed to be similar to capture relevant information in both directions. They are used to\n",
        "# calculate the attention scores, indicating how much each element in the sequence should attend to every other element. The similarity between key and\n",
        "# query helps the model learn dependencies in both directions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Mby0-kk4WOdr"
      },
      "outputs": [],
      "source": [
        "# Multi-head attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        # this is a linear layer that concats all the heads created and mushes them togetherinto shape n_embd\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        # for every head, x is passed into self.heads aka the Head class\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "J6b2bNzpWOdv"
      },
      "outputs": [],
      "source": [
        "# Creating a feedforward class\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4*n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_embd*4, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Not super complicated, just a simple part of the transformer structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PfxehI2jWOdy"
      },
      "outputs": [],
      "source": [
        "# Creating a transformer block\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.attention = MultiHeadAttention(n_head, head_size)\n",
        "        self.feedforward = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.attention(x)\n",
        "        x = self.ln1(x + y)\n",
        "        y = self.feedforward(x)\n",
        "        x = self.ln2(x + y)\n",
        "        return x\n",
        "\n",
        "# As we can see, the initialization of this class basically includes the previous structures we made\n",
        "# So for the forward fucntion, everytime we pass our input through either multiheadattention, or feedforward\n",
        "# we need to linearize it, so that iw what we do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3Jk66UQpWOdz"
      },
      "outputs": [],
      "source": [
        "# Now to make a GPT model\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # Create a self vocab_size variable to save it into the class\n",
        "        self.vocab_size = vocab_size\n",
        "        # Make an embedding table\n",
        "        self.token_embedding_table = nn.Embedding(self.vocab_size, n_embd).to(device)\n",
        "        # Adding a positional embedding table as well\n",
        "        self.positional_embedding_table = nn.Embedding(block_size, n_embd).to(device)  # added new parameter, n_embd\n",
        "        # Adding 4 decoder layers\n",
        "        self.blocks = nn.Sequential(*(Block(n_embd, n_head=n_head).to(device) for _ in range(n_layer)))\n",
        "        # final layer normalization\n",
        "        self.lm_f = nn.LayerNorm(n_embd).to(device)\n",
        "        # unsure what this is below\n",
        "        self.lm_head = nn.Linear(n_embd, self.vocab_size).to(device)\n",
        "\n",
        "        # std variables to help training converge better\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "    # Linear layers are initialized with normal distribution, and embedding layers are initialized with normal distribution as well.\n",
        "\n",
        "    def forward(self, index, targets=None):\n",
        "        B, T = index.shape\n",
        "        # index represents the sequence of tokens\n",
        "\n",
        "        # Add in token and positional embeddings\n",
        "        token_embd = self.token_embedding_table(index)  # (B, T, C)\n",
        "        # This layer is an embedding table for token embeddings. Given an input index (representing a token), it retrieves the corresponding embedding vector from the table.\n",
        "        pos_embd = self.positional_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
        "        # Embedding table for positional embeddings. It helps the model take into account the order or position of tokens in the sequence\n",
        "        x = token_embd + pos_embd  # (B, T, C)\n",
        "        x = self.blocks(x)  # (B, T, C)\n",
        "        x = self.lm_f(x)  # (B, T, C)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, index, max_new_tokens):\n",
        "        # model is iteratively called to predict the next token, and the predicted token is concatenated to the input sequence\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self.forward(index)\n",
        "            logits = logits[:, -1, :]\n",
        "            # print(f\"Logits shape: {logits.shape}\")\n",
        "            # These are the raw scores produced by the model before applying the softmax function. Each entry in the logits tensor represents\n",
        "            # the model's prediction for the likelihood of a particular token in the vocabulary. The dimensions of logits are (B, T, vocab_size),\n",
        "            #  where B is the batch size, T is the sequence length, and vocab_size is the size of the vocabulary.\n",
        "            probabilities = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Ensure generated index is within the vocabulary size\n",
        "            valid_indices = torch.arange(self.vocab_size).to(device)\n",
        "            # print('vocab size', vocab_size)\n",
        "            index_next = torch.multinomial(probabilities[:, valid_indices], num_samples=1)\n",
        "            index_next = valid_indices[index_next]  # Map back to the original indices\n",
        "\n",
        "            index = torch.cat((index, index_next), dim=1)\n",
        "\n",
        "        return index\n",
        "\n",
        "# model = GPTLanguageModel(vocab_size)\n",
        "\n",
        "# To explain the positional and token embeddings:\n",
        "# In a Transformer, each position in the input sequence has a unique positional embedding associated with it. This positional embedding is added\n",
        "# to the token embedding of the corresponding word. If you didn't have positional embeddings and used a single embedding table for both tokens and positions,\n",
        "# the model might struggle to distinguish between words based on their positions in the sequence.\n",
        "\n",
        "# Having separate tables allows the model to learn distinct embeddings for tokens and positions. The positional embeddings can then be added to the\n",
        "# token embeddings during processing, ensuring that the model can effectively capture both the semantic content of words and their positions in the sequence.\n",
        "\n",
        "# So, even if you're working with a single sequence (no batches), having separate token and positional embeddings is still beneficial for the\n",
        "# Transformer model's ability to understand and leverage both semantic and positional information.\n",
        "\n",
        "# in order to deal with a prompt, make the GPT model encounter a prompt size of around 50.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zU8Uj0fFWOd1"
      },
      "outputs": [],
      "source": [
        "# Do not run this and next 2 cells, if loading in model\n",
        "\n",
        "# create the model\n",
        "max_prompt_size = 50\n",
        "model = GPTLanguageModel(vocab_size + max_prompt_size)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atcHZOpFWOeJ",
        "outputId": "e5f1f360-02bc-4e7d-9ebf-cb39110d9efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0, train loss: 8.27614974975586, val loss: 8.285806655883789\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 250, train loss: 5.994791507720947, val loss: 6.3692307472229\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 500, train loss: 5.131834030151367, val loss: 5.5337629318237305\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 750, train loss: 4.331768035888672, val loss: 4.832136154174805\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 1000, train loss: 3.6526691913604736, val loss: 4.251694202423096\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 1250, train loss: 3.103361129760742, val loss: 3.7700154781341553\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 1500, train loss: 2.6870217323303223, val loss: 3.4354522228240967\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 1750, train loss: 2.436267852783203, val loss: 3.1909255981445312\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 2000, train loss: 2.2853615283966064, val loss: 3.06366229057312\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 2250, train loss: 2.154566764831543, val loss: 2.9565837383270264\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 2500, train loss: 2.04891037940979, val loss: 2.875092029571533\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 2750, train loss: 1.9892644882202148, val loss: 2.8121814727783203\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 3000, train loss: 1.9429574012756348, val loss: 2.7765252590179443\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 3250, train loss: 1.8807419538497925, val loss: 2.7176287174224854\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 3500, train loss: 1.8150420188903809, val loss: 2.6834867000579834\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 3750, train loss: 1.8051820993423462, val loss: 2.6420514583587646\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 4000, train loss: 1.7580853700637817, val loss: 2.602583885192871\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 4250, train loss: 1.76382315158844, val loss: 2.606754779815674\n",
            "step 4500, train loss: 1.7258522510528564, val loss: 2.611492156982422\n",
            "step 4750, train loss: 1.719667673110962, val loss: 2.5584616661071777\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 5000, train loss: 1.7164883613586426, val loss: 2.5622007846832275\n",
            "step 5250, train loss: 1.6852039098739624, val loss: 2.544309139251709\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 5500, train loss: 1.6642731428146362, val loss: 2.5475361347198486\n",
            "step 5750, train loss: 1.6694903373718262, val loss: 2.5055348873138428\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 6000, train loss: 1.6476613283157349, val loss: 2.5113866329193115\n",
            "step 6250, train loss: 1.6128937005996704, val loss: 2.4972164630889893\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 6500, train loss: 1.6244548559188843, val loss: 2.5042433738708496\n",
            "step 6750, train loss: 1.5908643007278442, val loss: 2.5048768520355225\n",
            "step 7000, train loss: 1.6124247312545776, val loss: 2.477302312850952\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 7250, train loss: 1.592207908630371, val loss: 2.515044927597046\n",
            "step 7500, train loss: 1.5720336437225342, val loss: 2.4774250984191895\n",
            "step 7750, train loss: 1.5582797527313232, val loss: 2.45809006690979\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 8000, train loss: 1.5626447200775146, val loss: 2.4529364109039307\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 8250, train loss: 1.538844347000122, val loss: 2.486576795578003\n",
            "step 8500, train loss: 1.5502394437789917, val loss: 2.4653685092926025\n",
            "step 8750, train loss: 1.5230436325073242, val loss: 2.4487407207489014\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 9000, train loss: 1.5260834693908691, val loss: 2.4558777809143066\n",
            "step 9250, train loss: 1.509389877319336, val loss: 2.46225905418396\n",
            "step 9500, train loss: 1.5107207298278809, val loss: 2.4473910331726074\n",
            "Updated parameters  odict_keys(['token_embedding_table.weight', 'positional_embedding_table.weight', 'blocks.0.attention.heads.0.tril', 'blocks.0.attention.heads.0.key.weight', 'blocks.0.attention.heads.0.query.weight', 'blocks.0.attention.heads.0.value.weight', 'blocks.0.attention.heads.1.tril', 'blocks.0.attention.heads.1.key.weight', 'blocks.0.attention.heads.1.query.weight', 'blocks.0.attention.heads.1.value.weight', 'blocks.0.attention.heads.2.tril', 'blocks.0.attention.heads.2.key.weight', 'blocks.0.attention.heads.2.query.weight', 'blocks.0.attention.heads.2.value.weight', 'blocks.0.attention.heads.3.tril', 'blocks.0.attention.heads.3.key.weight', 'blocks.0.attention.heads.3.query.weight', 'blocks.0.attention.heads.3.value.weight', 'blocks.0.attention.heads.4.tril', 'blocks.0.attention.heads.4.key.weight', 'blocks.0.attention.heads.4.query.weight', 'blocks.0.attention.heads.4.value.weight', 'blocks.0.attention.heads.5.tril', 'blocks.0.attention.heads.5.key.weight', 'blocks.0.attention.heads.5.query.weight', 'blocks.0.attention.heads.5.value.weight', 'blocks.0.attention.proj.weight', 'blocks.0.attention.proj.bias', 'blocks.0.feedforward.net.0.weight', 'blocks.0.feedforward.net.0.bias', 'blocks.0.feedforward.net.2.weight', 'blocks.0.feedforward.net.2.bias', 'blocks.0.ln1.weight', 'blocks.0.ln1.bias', 'blocks.0.ln2.weight', 'blocks.0.ln2.bias', 'blocks.1.attention.heads.0.tril', 'blocks.1.attention.heads.0.key.weight', 'blocks.1.attention.heads.0.query.weight', 'blocks.1.attention.heads.0.value.weight', 'blocks.1.attention.heads.1.tril', 'blocks.1.attention.heads.1.key.weight', 'blocks.1.attention.heads.1.query.weight', 'blocks.1.attention.heads.1.value.weight', 'blocks.1.attention.heads.2.tril', 'blocks.1.attention.heads.2.key.weight', 'blocks.1.attention.heads.2.query.weight', 'blocks.1.attention.heads.2.value.weight', 'blocks.1.attention.heads.3.tril', 'blocks.1.attention.heads.3.key.weight', 'blocks.1.attention.heads.3.query.weight', 'blocks.1.attention.heads.3.value.weight', 'blocks.1.attention.heads.4.tril', 'blocks.1.attention.heads.4.key.weight', 'blocks.1.attention.heads.4.query.weight', 'blocks.1.attention.heads.4.value.weight', 'blocks.1.attention.heads.5.tril', 'blocks.1.attention.heads.5.key.weight', 'blocks.1.attention.heads.5.query.weight', 'blocks.1.attention.heads.5.value.weight', 'blocks.1.attention.proj.weight', 'blocks.1.attention.proj.bias', 'blocks.1.feedforward.net.0.weight', 'blocks.1.feedforward.net.0.bias', 'blocks.1.feedforward.net.2.weight', 'blocks.1.feedforward.net.2.bias', 'blocks.1.ln1.weight', 'blocks.1.ln1.bias', 'blocks.1.ln2.weight', 'blocks.1.ln2.bias', 'blocks.2.attention.heads.0.tril', 'blocks.2.attention.heads.0.key.weight', 'blocks.2.attention.heads.0.query.weight', 'blocks.2.attention.heads.0.value.weight', 'blocks.2.attention.heads.1.tril', 'blocks.2.attention.heads.1.key.weight', 'blocks.2.attention.heads.1.query.weight', 'blocks.2.attention.heads.1.value.weight', 'blocks.2.attention.heads.2.tril', 'blocks.2.attention.heads.2.key.weight', 'blocks.2.attention.heads.2.query.weight', 'blocks.2.attention.heads.2.value.weight', 'blocks.2.attention.heads.3.tril', 'blocks.2.attention.heads.3.key.weight', 'blocks.2.attention.heads.3.query.weight', 'blocks.2.attention.heads.3.value.weight', 'blocks.2.attention.heads.4.tril', 'blocks.2.attention.heads.4.key.weight', 'blocks.2.attention.heads.4.query.weight', 'blocks.2.attention.heads.4.value.weight', 'blocks.2.attention.heads.5.tril', 'blocks.2.attention.heads.5.key.weight', 'blocks.2.attention.heads.5.query.weight', 'blocks.2.attention.heads.5.value.weight', 'blocks.2.attention.proj.weight', 'blocks.2.attention.proj.bias', 'blocks.2.feedforward.net.0.weight', 'blocks.2.feedforward.net.0.bias', 'blocks.2.feedforward.net.2.weight', 'blocks.2.feedforward.net.2.bias', 'blocks.2.ln1.weight', 'blocks.2.ln1.bias', 'blocks.2.ln2.weight', 'blocks.2.ln2.bias', 'blocks.3.attention.heads.0.tril', 'blocks.3.attention.heads.0.key.weight', 'blocks.3.attention.heads.0.query.weight', 'blocks.3.attention.heads.0.value.weight', 'blocks.3.attention.heads.1.tril', 'blocks.3.attention.heads.1.key.weight', 'blocks.3.attention.heads.1.query.weight', 'blocks.3.attention.heads.1.value.weight', 'blocks.3.attention.heads.2.tril', 'blocks.3.attention.heads.2.key.weight', 'blocks.3.attention.heads.2.query.weight', 'blocks.3.attention.heads.2.value.weight', 'blocks.3.attention.heads.3.tril', 'blocks.3.attention.heads.3.key.weight', 'blocks.3.attention.heads.3.query.weight', 'blocks.3.attention.heads.3.value.weight', 'blocks.3.attention.heads.4.tril', 'blocks.3.attention.heads.4.key.weight', 'blocks.3.attention.heads.4.query.weight', 'blocks.3.attention.heads.4.value.weight', 'blocks.3.attention.heads.5.tril', 'blocks.3.attention.heads.5.key.weight', 'blocks.3.attention.heads.5.query.weight', 'blocks.3.attention.heads.5.value.weight', 'blocks.3.attention.proj.weight', 'blocks.3.attention.proj.bias', 'blocks.3.feedforward.net.0.weight', 'blocks.3.feedforward.net.0.bias', 'blocks.3.feedforward.net.2.weight', 'blocks.3.feedforward.net.2.bias', 'blocks.3.ln1.weight', 'blocks.3.ln1.bias', 'blocks.3.ln2.weight', 'blocks.3.ln2.bias', 'blocks.4.attention.heads.0.tril', 'blocks.4.attention.heads.0.key.weight', 'blocks.4.attention.heads.0.query.weight', 'blocks.4.attention.heads.0.value.weight', 'blocks.4.attention.heads.1.tril', 'blocks.4.attention.heads.1.key.weight', 'blocks.4.attention.heads.1.query.weight', 'blocks.4.attention.heads.1.value.weight', 'blocks.4.attention.heads.2.tril', 'blocks.4.attention.heads.2.key.weight', 'blocks.4.attention.heads.2.query.weight', 'blocks.4.attention.heads.2.value.weight', 'blocks.4.attention.heads.3.tril', 'blocks.4.attention.heads.3.key.weight', 'blocks.4.attention.heads.3.query.weight', 'blocks.4.attention.heads.3.value.weight', 'blocks.4.attention.heads.4.tril', 'blocks.4.attention.heads.4.key.weight', 'blocks.4.attention.heads.4.query.weight', 'blocks.4.attention.heads.4.value.weight', 'blocks.4.attention.heads.5.tril', 'blocks.4.attention.heads.5.key.weight', 'blocks.4.attention.heads.5.query.weight', 'blocks.4.attention.heads.5.value.weight', 'blocks.4.attention.proj.weight', 'blocks.4.attention.proj.bias', 'blocks.4.feedforward.net.0.weight', 'blocks.4.feedforward.net.0.bias', 'blocks.4.feedforward.net.2.weight', 'blocks.4.feedforward.net.2.bias', 'blocks.4.ln1.weight', 'blocks.4.ln1.bias', 'blocks.4.ln2.weight', 'blocks.4.ln2.bias', 'blocks.5.attention.heads.0.tril', 'blocks.5.attention.heads.0.key.weight', 'blocks.5.attention.heads.0.query.weight', 'blocks.5.attention.heads.0.value.weight', 'blocks.5.attention.heads.1.tril', 'blocks.5.attention.heads.1.key.weight', 'blocks.5.attention.heads.1.query.weight', 'blocks.5.attention.heads.1.value.weight', 'blocks.5.attention.heads.2.tril', 'blocks.5.attention.heads.2.key.weight', 'blocks.5.attention.heads.2.query.weight', 'blocks.5.attention.heads.2.value.weight', 'blocks.5.attention.heads.3.tril', 'blocks.5.attention.heads.3.key.weight', 'blocks.5.attention.heads.3.query.weight', 'blocks.5.attention.heads.3.value.weight', 'blocks.5.attention.heads.4.tril', 'blocks.5.attention.heads.4.key.weight', 'blocks.5.attention.heads.4.query.weight', 'blocks.5.attention.heads.4.value.weight', 'blocks.5.attention.heads.5.tril', 'blocks.5.attention.heads.5.key.weight', 'blocks.5.attention.heads.5.query.weight', 'blocks.5.attention.heads.5.value.weight', 'blocks.5.attention.proj.weight', 'blocks.5.attention.proj.bias', 'blocks.5.feedforward.net.0.weight', 'blocks.5.feedforward.net.0.bias', 'blocks.5.feedforward.net.2.weight', 'blocks.5.feedforward.net.2.bias', 'blocks.5.ln1.weight', 'blocks.5.ln1.bias', 'blocks.5.ln2.weight', 'blocks.5.ln2.bias', 'lm_f.weight', 'lm_f.bias', 'lm_head.weight', 'lm_head.bias'])\n",
            "step 9750, train loss: 1.498350739479065, val loss: 2.451021432876587\n",
            "1.3035334348678589\n"
          ]
        }
      ],
      "source": [
        "# Creating an Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# save the best params\n",
        "best_val_loss = float('inf')  # Initialize with a large value\n",
        "best_params = None\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_iters == 0:\n",
        "        losses = estimate_loss()\n",
        "        train_loss = losses['train']\n",
        "        val_loss = losses['val']\n",
        "        print(f\"step {iter}, train loss: {train_loss}, val loss: {val_loss}\")\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_params = model.state_dict()  # Save the current model parameters\n",
        "        print(\"Updated parameters \", best_params.keys())\n",
        "\n",
        "\n",
        "    logits, loss = model.forward(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    # clears the gradients of all optimized parameters\n",
        "    loss.backward()\n",
        "    # implement accumulation stepse\n",
        "    if (iter + 1) % accumulation_steps == 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "print(loss.item())\n",
        "\n",
        "# What we are doing here is the same thing as training the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "dYgjNOFTWOeO"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save the model into a pickle file\n",
        "with open('/content/drive/MyDrive/model-06.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Convert tensors to NumPy arrays in the model's state dictionary\n",
        "model_state_dict_np = {key: value.cpu().numpy().tolist() if isinstance(value, torch.Tensor) else value for key, value in model_state_dict.items()}\n",
        "\n",
        "# Save the model's state dictionary into a JSON file\n",
        "json_path = '/content/drive/MyDrive/model-06.json'\n",
        "with open(json_path, 'w') as json_file:\n",
        "    json.dump(model_state_dict_np, json_file)\n"
      ],
      "metadata": {
        "id": "u8Km20rF37Jc"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Saved Model"
      ],
      "metadata": {
        "id": "mZ6C7GD4robq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "VlE8l3AFWOeS"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Load the model, if necessary\n",
        "with open('/content/drive/MyDrive/model-06.pkl', 'rb') as f:\n",
        "    model = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "-H7604xrWOeT"
      },
      "outputs": [],
      "source": [
        "# # adjust the dictionaries (ONLY RUN ONCE)\n",
        "# index_to_word.update({0: ''})\n",
        "# word_to_index[''] = word_to_index.pop('block')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "zPFMqG0DWOeT"
      },
      "outputs": [],
      "source": [
        "# Model 3\n",
        "\n",
        "# So what I have done is I have removed the stopwords from being in df['Transcript_Cleaned']\n",
        "# Not entirely sure if this is a good idea but it is worth a shot\n",
        "# I am doing this in order to generate a good response from GPT form the prompt, and to do\n",
        "# that it must include words to form an actual sentence\n",
        "\n",
        "# Model 4\n",
        "\n",
        "# like Model 3, but I trained it on a GPU so should work much better\n",
        "\n",
        "# Model 5\n",
        "\n",
        "# Batch size 64, number of heads = 4\n",
        "\n",
        "# Model 6\n",
        "\n",
        "# different parameters\n",
        "# batch_size = 128 # learning_rate = 1e-4 # n_layer = 6 # n_head = 6 # accumulation_steps = 6\n",
        "# Rand this one all the way, 1000 iters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "cUDGOzZsWOeV"
      },
      "outputs": [],
      "source": [
        "# Add the prompt into the dictionary used for the training dataset\n",
        "# prompt = 'Can you give me an overview on Probabilistic Latent Semantic Analysis'.split()\n",
        "prompt = 'what is text retreival'\n",
        "\n",
        "# Find the maximum key in the existing dictionaries\n",
        "max_key = max(word_to_index.values()) if word_to_index else -1\n",
        "\n",
        "# Enumerate through the new words and add them to the dictionaries\n",
        "for word in prompt:\n",
        "    if word not in word_to_index:\n",
        "        max_key += 1\n",
        "        word_to_index[word] = max_key\n",
        "        index_to_word[max_key] = word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "98nabemcWOeW"
      },
      "outputs": [],
      "source": [
        "# Create result from prompt, as a chatbot would\n",
        "context = torch.tensor(words_to_indices(prompt), dtype=torch.long, device=device)\n",
        "# context = torch.zeros((1, 1), dtype=torch.long)\n",
        "generated_terms = indices_to_words(model.generate(context.unsqueeze(0), max_new_tokens=50)[0].tolist())\n",
        "# make sure max_new_tokens is less than block_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "5yQhG4pbWOeX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eea5b73-5013-4499-b686-f780adad3e28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "be copy character in crawling is better so that were going to knowing an index for a sentence or occasion and it actually mean this level is basically what do with the area models for example is very small grammar approximation of order to also a average and this score\n"
          ]
        }
      ],
      "source": [
        "print(' '.join(generated_terms[len(prompt):]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "LQXT4TxIWOeX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c8a879c-9c9b-4420-944b-701fcf7456a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual Vocabulary Size: 4035\n",
            "Model Vocabulary Size: 4011\n",
            "Embedding Dimension: 128\n",
            "Max Index in padded_indices: 4010\n",
            "Min Index in padded_indices: 0\n",
            "Context vector tensor([4011, 4012, 3737, 4013, 4014, 4022, 4019, 4014, 4013, 4016, 4033, 4013,\n",
            "        4014, 4015, 4016, 4013, 4015, 4016, 4022, 4034, 3737, 4027],\n",
            "       device='cuda:0')\n",
            "Model [4011, 4012, 3737, 4013, 4014, 4022, 4019, 4014, 4013, 4016, 4033, 4013, 4014, 4015, 4016, 4013, 4015, 4016, 4022, 4034, 3737, 4027, 3033, 3726, 3787, 3855, 2599, 3737, 1904, 2822, 2845, 3198, 3788, 3737, 2061, 2763, 785, 3272, 2860, 2557, 1714, 2753, 1272, 971, 2594, 3078, 1033, 2753, 785, 2218, 1187, 3122, 3693, 502, 785, 632, 716, 785, 2772, 2616, 2941, 2763, 312, 2881, 785, 2722, 662, 47, 1286, 1288, 165, 3431]\n",
            "Prompt Indices: [4011, 4012, 3737, 4013, 4014, 4022, 4019, 4014, 4013, 4016, 4033, 4013, 4014, 4015, 4016, 4013, 4015, 4016, 4022, 4034, 3737, 4027]\n"
          ]
        }
      ],
      "source": [
        "# debugging issues with sizes and lengths\n",
        "# Check vocabulary size\n",
        "print(\"Actual Vocabulary Size:\", len(word_to_index))\n",
        "print(\"Model Vocabulary Size:\", vocab_size)\n",
        "\n",
        "# Check embedding dimension\n",
        "print(\"Embedding Dimension:\", n_embd)\n",
        "\n",
        "# Check index values\n",
        "print(\"Max Index in padded_indices:\", torch.max(padded_indices).item())\n",
        "print(\"Min Index in padded_indices:\", torch.min(padded_indices).item())\n",
        "\n",
        "print(\"Context vector\", context)\n",
        "print(\"Model\", model.generate(context.unsqueeze(0), max_new_tokens=50)[0].tolist())\n",
        "\n",
        "# Check indices added from prompt\n",
        "prompt_indices = words_to_indices(prompt)\n",
        "print(\"Prompt Indices:\", prompt_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "46Cm_UduWOea"
      },
      "outputs": [],
      "source": [
        "### Notes on model\n",
        "# model 1 is on just the grams\n",
        "# model 2 is on the full transcript training\n",
        "# model 3, I stopped removing stopwords, in order to generate a better response from the context vector\n",
        "\n",
        "# model 1 prompt: Can you give me an overview on Probabilistic Latent Semantic Analysis\n",
        "# result: word_distribution promise profile allocation justification minimize edge briefly mix environment sky\n",
        "# interest root research present tilde york engine domain light popularity likelihood bye bridge summary understood\n",
        "# separating pick quantitate sub polarity encounter observation right setting future order Can distinguish accurate\n",
        "# stick aggregate doesnt apply party photo message scientist transpose categorize\n",
        "\n",
        "# model 2 prompt: Can you give me an overview on Probabilistic Latent Semantic Analysis\n",
        "# result: assign goal clutch discovering simplification meal bomb discovery generality recalibration confidence front\n",
        "# request implement anticipate percent suppose perspective choice attribute state development sit uncertainty choose\n",
        "# viewer play regime baring doubt hash table tolerance exploitation bit guess causal moreover letter area web profile\n",
        "# rating algorithm incomplete incomplete well imbalance event motion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDsZ1dgrWOeb"
      },
      "source": [
        "# Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import GPTNeoForCausalLM, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "\n",
        "# # Check if CUDA (GPU) is available\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# torch.cuda.set_per_process_memory_fraction(0.85)  # or any smaller fraction\n",
        "\n",
        "# model_name = 'EleutherAI/gpt-neo-1.3B'\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "# model = GPTNeoForCausalLM.from_pretrained(model_name).to(device)  # Move the model to CUDA if available\n",
        "\n",
        "# train_dataset = TextDataset(\n",
        "#     tokenizer=tokenizer,\n",
        "#     file_path='/content/drive/MyDrive/all_lectures.csv',  # Replace with your fine-tuning dataset\n",
        "#     block_size=64,\n",
        "# )\n",
        "\n",
        "# data_collator = DataCollatorForLanguageModeling(\n",
        "#     tokenizer=tokenizer,\n",
        "#     mlm=False\n",
        "# )\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=TrainingArguments(\n",
        "#         output_dir='/content/drive/MyDrive/fine-tuned-model',\n",
        "#         overwrite_output_dir=True,\n",
        "#         num_train_epochs=1,\n",
        "#         per_device_train_batch_size=2,  # Adjust batch size based on GPU memory\n",
        "#         save_steps=10,  # Adjust the frequency of saving checkpoints\n",
        "#         gradient_accumulation_steps=8  # or any larger value\n",
        "#     ),\n",
        "#     data_collator=data_collator,\n",
        "#     train_dataset=train_dataset\n",
        "# )\n",
        "\n",
        "# trainer.train()\n"
      ],
      "metadata": {
        "id": "aOVCt1zD8dA0"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CWtWMv0Yb9ei"
      },
      "execution_count": 32,
      "outputs": []
=======
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week Number</th>\n",
       "      <th>Lesson Number</th>\n",
       "      <th>Lesson Title</th>\n",
       "      <th>Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Natural Language Content Analysis</td>\n",
       "      <td>This lecture is about Natural Language of Cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Access</td>\n",
       "      <td>In this lecture,\\r\\nwe're going to talk about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Retrieval Problem</td>\n",
       "      <td>This lecture is about\\r\\nthe text retrieval pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overview of Text Retrieval Methods</td>\n",
       "      <td>This lecture is a overview of\\r\\ntext retrieva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Vector Space Model - Basic Idea</td>\n",
       "      <td>This lecture is about the\\r\\nvector space retr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Week Number  Lesson Number                        Lesson Title  \\\n",
       "0            1              1   Natural Language Content Analysis   \n",
       "1            1              2                         Text Access   \n",
       "2            1              3              Text Retrieval Problem   \n",
       "3            1              4  Overview of Text Retrieval Methods   \n",
       "4            1              5     Vector Space Model - Basic Idea   \n",
       "\n",
       "                                          Transcript  \n",
       "0  This lecture is about Natural Language of Cont...  \n",
       "1  In this lecture,\\r\\nwe're going to talk about ...  \n",
       "2  This lecture is about\\r\\nthe text retrieval pr...  \n",
       "3  This lecture is a overview of\\r\\ntext retrieva...  \n",
       "4  This lecture is about the\\r\\nvector space retr...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Directory path\n",
    "#directory_path = 'C:\\\\Users\\\\azaan\\\\OneDrive\\\\Documents\\\\GitHub\\\\cs410_LLM_project\\\\data\\\\all_lectures.csv'\n",
    "#Chris's Directory\n",
    "directory_path = 'C:\\\\Users\\\\chris\\\\cs410_LLM_project\\\\data\\\\all_lectures.csv'\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "df = pd.DataFrame(columns=['Week Number', 'Lesson Number', 'Lesson Title', 'Transcript'])\n",
    "\n",
    "# Read in csv to dataframe\n",
    "df = pd.read_csv(directory_path)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# clean up words in dataset -- this includes removing stopwords\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, words, brown\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"brown\")\n",
    "#Chris - added following downloads\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# initialize dictionary\n",
    "global_dictionary  = set(words.words()) | set(brown.words())\n",
    "global_dictionary = {word.lower() for word in global_dictionary}\n",
    "remove_words = list(stop_words) # might need to use word_tokenize\n",
    "remove_words.extend(['Play', 'video', 'starting', 'at', '::', 'follow', 'transcript', 'natural', 'language', 'lecture', 'processing']) # add the common words that's include d in transcript\n",
    "\n",
    "# Now start actually cleaning the text\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase\n",
    "    text = text.replace('\\n', ' ') # remove newline indicator\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # case\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text) # website\n",
    "    text = re.sub(r'(\\b\\w+\\b)(?: \\1)+', r'\\1', text) # remove duplicate next word after space\n",
    "    text = re.sub(r'\\b(?![aI]\\b)\\w\\b', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Remove stopwords and only keep words in dictionary\n",
    "def remove_terms(text):\n",
    "    text = clean_text(text)\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in remove_words] # remove stopwords\n",
    "    filtered_words = [word for word in filtered_words if word in global_dictionary] # remove if not in global dictionary\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "df['Transcript_Cleaned'] = df['Transcript'].apply(remove_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'content analysis see picture really first step process text data text data languages computers understand languages extent order make use data thats topic going cover three things first main technique obtain understanding second state art stands finally going cover relation text retrieval first well best way explain think see text foreign understand order understand text basically computers facing looking simple sentence like dog chasing boy playground dont problems understanding sentence imagine computer would order understand well general would following first would know dog noun verb called lexical analysis tagging need figure syntactic categories words thats first step going figure structure sentence example shows dog would go together form noun phrase wont dog go first structures right structure shows might get look sentence try interpret sentence words would go together first go together words show noun phrases intermediate components verbal phrases finally sentence get structure need something called semantic analysis may parser accompanying program would automatically created structure point would know structure sentence still dont know meaning sentence go semantic analysis mind usually map sentence already know knowledge base example might imagine dog looks like theres boy theres activity computer would use symbols denote wed use symbol denote dog denote boy denote playground also chasing activity thats happening relationship chasing connects symbols computer would obtain understanding sentence representation could also infer things might indeed naturally think something else read text called inference example believe chased person might scared rule see computers could also infer boy maybe scared extra knowledge youd infer based understanding text even go understand person say sentence use called pragmatic analysis order understand speak actor sentence right say something basically achieve goal theres purpose use case person said sentence might reminding another person bring back dog could one possible intent reach level understanding would require steps computer would go steps order completely understand sentence yet humans trouble understanding instantly would get everything reason thats large knowledge base brain use common sense knowledge help interpret sentence computers unfortunately hard obtain understanding dont knowledge base still incapable reasoning uncertainties makes difficult computers fundamental reason difficult computers simply designed computers languages designed us communicate languages designed computers example programming languages harder us right languages designed make communication efficient result omit lot common sense knowledge assume everyone knows also keep lot ambiguities assume receiver hearer could know decipher ambiguous word based knowledge context theres need demand different words different meanings could overload word different meanings without problem reasons makes every step difficult computers ambiguity main difficulty common sense reasoning often required thats also hard let give examples challenges consider word level ambiguity word different syntactic categories example design noun verb word root may multiple meanings square root math sense root plant might able think meanings also syntactical ambiguities example main topic actually interpreted two ways terms structure think moment see figure usually think could also think say example synaptic ambiguity different structures applied sequence words another common example ambiguous sentence following man saw boy telescope case question telescope called prepositional phrase attachment ambiguity attachment ambiguity generally dont problem ambiguities lot background knowledge help us ambiguity another example difficulty anaphora resolution think sentence john persuaded bill buy tv question refer john bill something use background context figure finally presupposition another problem consider sentence quit smoking obviously implies smoked imagine computer wants understand subtle differences meanings would use lot knowledge figure also would maintain large knowledge base meanings words connected common sense knowledge world difficult result steep perfect fact far perfect understanding using computers slide sort gains simplified view state art part speech tagging pretty well showed accuracy number obviously based certain dont take literally shows pretty well still perfect terms partial pretty well means get noun phrase structures verb phrase structure segment sentence dude correct terms structure evaluation results seen accuracy terms partial sentences say numbers relative numbers might lower existing work evaluated using news lot numbers less toward news data think social media data accuracy likely lower terms semantical analysis far able complete understanding sentence techniques would allow us partial understanding sentence could mention example techniques allow us extract entities relations mentioned text articles example recognizing dimensions people locations organizations text called entity extraction may able recognize relations example person visited place person met person company acquired another company relations extracted using computer current techniques theyre perfect well entities entities harder others also word sense disintegration extend figure whether word sentence would certain meaning another context computer could figure different meaning perfect something direction also sentiment analysis meaning figure whether sentence positive negative especially useful review analysis example examples semantic analysis help us obtain partial understanding sentences giving us complete understanding showed sentence would still help us gain understanding content useful terms inference yet probably general difficulty inference uncertainties general challenge artificial intelligence thats probably also dont complete semantical representation inaudible text hard yet domains perhaps limited domains lot restrictions word uses may able perform inference extent general really reliably speech act analysis also far done analysis special cases roughly gives idea state art also talk little bit cant cant even part speech tagging looks like simple task think example two uses may different syntactic categories try make fine grained distinctions easy figure differences also hard general complete sentence saw example ambiguity hard imagine example use lot knowledge context sentence background order figure actually telescope although sentence looks simple actually pretty hard cases sentence long imagine four five prepositional phrases even possibilities figure also harder precise deep semantic analysis example sentence john owns restaurant define owns exactly word something understand hard precisely describe meaning computers result robust general techniques process lot text data shallow way meaning superficial analysis example parts speech tagging partial recognizing sentiment deep understanding really understanding exact meaning sentence hand deep understanding techniques tend scale well meaning would fill restricted text dont restrict text domain use words techniques tend work well may work well based machine learning techniques data similar training data program trained generally wouldnt work well data different training data pretty much summarizes state art course within short amount time cant really give complete view big field id expect see multiple courses topic relevance topic talk useful know background case happen exposed mean text retrieval well text retrieval dealing kinds text hard restrict text certain domain also often dealing lot text data means techniques must general robust efficient implies today use fairly shallow techniques text retrieval fact search engines today use something called bag words representation probably simplest representation possibly think turn text data simply bag words meaning well keep individual words well ignore orders words well keep duplicated occurrences words called bag words representation represent text way ignore lot valid information makes harder understand exact meaning sentence weve lost order yet representation tends actually work pretty well search tasks partly search task difficult see matching query words text document chances document topic although exceptions comparison tasks example machine translation would require understand accurately otherwise translation would wrong comparison tasks relatively easy representation often sufficient thats also representation major search engines today like bing using course put parentheses course many queries answered well current search engines require replantation would go beyond bag words replantation would require done another reason used sophisticated techniques modern search engines thats retrieval techniques actually naturally solved problem one example word sense disintegration think word like java could mean coffee could mean program look word would ambiguous user uses word query usually words example looking usage java implies java means program contest help us naturally prefer documents java referring program languages documents would probably match well java occurs documents means coffee would never match small probability case retrieval techniques naturally achieve goal word another example technique called feedback talk later lectures technique would allow us additional words query additional words could related query words words help matching documents original query words occurred achieves extent semantic matching terms techniques also helped us bypass difficulties however long run still need deeper techniques order improve accuracy current search engines particularly needed complex search tasks question answering recently launched knowledge graph one step toward goal knowledge graph would contain entities relations goes beyond simple bag words replantation technique help us improve search engine utility significantly although open topic research exploration sum talked weve talked state techniques cannot finally also explain bag words replantation remains dominant replantation used modern search engines even though deeper would needed future search engines want know take look additional readings cited one thats good point thanks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Transcript_Cleaned'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bigrams and trigrams from data\n",
    "\n",
    "# Function to filter bigrams or trigrams\n",
    "def ngram_filter(ngram):\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if not all(tag[1] in ['JJ', 'NN'] for tag in tags):\n",
    "        return False\n",
    "    if any(word in stop_words for word in ngram):\n",
    "        return False\n",
    "    if 'n' in ngram or 't' in ngram:\n",
    "        return False\n",
    "    if 'PRON' in ngram:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Function to find top ngrams\n",
    "def find_top_ngrams(texts, ngram_measures, min_freq=50, min_pmi=5, top_k=100):\n",
    "    finder = nltk.collocations.BigramCollocationFinder.from_documents(texts)\n",
    "    finder.apply_freq_filter(min_freq)\n",
    "    ngram_scores = finder.score_ngrams(ngram_measures.pmi)\n",
    "    filtered_ngrams = [ngram for ngram, pmi in ngram_scores if ngram_filter(ngram) and pmi > min_pmi]\n",
    "    return [' '.join(ngram) for ngram in filtered_ngrams][:top_k]\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigrams = find_top_ngrams([text.split() for text in df['Transcript_Cleaned']], bigram_measures)\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "trigrams = find_top_ngrams([text.split() for text in df['Transcript_Cleaned']], trigram_measures)\n",
    "\n",
    "# Function to replace ngrams in text\n",
    "def replace_ngrams(text):\n",
    "    for gram in trigrams:\n",
    "        text = text.replace(gram, '_'.join(gram.split()))\n",
    "    for gram in bigrams:\n",
    "        text = text.replace(gram, '_'.join(gram.split()))\n",
    "    return text\n",
    "\n",
    "# Apply ngram replacements to the text\n",
    "df['Grams'] = df['Transcript_Cleaned'].map(replace_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize reviews + remove stop words + filter only nouns\n",
    "def tokenize_and_filter(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.lower() not in stop_words and len(word) > 2]\n",
    "    pos_comment = nltk.pos_tag(words)\n",
    "    filtered = [word[0] for word in pos_comment if word[1] in ['NN']]\n",
    "    return filtered\n",
    "\n",
    "df['Grams'] = df['Grams'].map(tokenize_and_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I will make embeddings for my words, let's see if it works\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import  torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "results = set()\n",
    "df['Grams'].apply(results.update)\n",
    "vocab_size = len(results)\n",
    "\n",
    "# Create a vocabulary dictionary\n",
    "word_to_index = {word: idx for idx, word in enumerate(results)}\n",
    "\n",
    "# Convert words to indices in your DataFrame\n",
    "# AKA Encode these\n",
    "# df['Grams_indices'] = df['Grams'].apply(lambda x: [word_to_index[word] for word in x])\n",
    "def words_to_indices(words):\n",
    "    return [word_to_index[word] for word in words]\n",
    "df['Grams_indices'] = df['Grams'].apply(words_to_indices)\n",
    "\n",
    "# Create a reverse dictionary\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "# Function to convert indices back to words\n",
    "def indices_to_words(indices):\n",
    "    return [index_to_word[idx] for idx in indices]\n",
    "\n",
    "# # Apply the function to the 'Grams_indices' column\n",
    "# Aka Decode these grams\n",
    "# df['Decoded_Grams'] = df['Grams_indices'].apply(indices_to_words)\n",
    "\n",
    "# Pad sequences to a specified length (e.g., maxlen)\n",
    "maxlen = 60  # You can adjust this based on your data\n",
    "padded_indices = pad_sequence([torch.LongTensor(seq) for seq in df['Grams_indices']], batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,  920,  918, 1519],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 663,  858,  215,  ...,  376,  933,  239],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [1402, 1327,  376,  ...,    0,    0,    0]])\n",
      "tensor([[   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,  918, 1519, 1218],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 858,  215, 1519,  ...,  933,  239, 1177],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [1327,  376,  724,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "# make a batch and set up parameters\n",
    "block_size = 256\n",
    "batch_size = 128\n",
    "max_iters = 5000\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250\n",
    "# new\n",
    "n_embd = 128\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "n_head = 4\n",
    "prompt_size = 50\n",
    "\n",
    "# Flatten the padded indices used to identify each word\n",
    "data = flattened_indices = padded_indices.view(-1)\n",
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "# print(len(data))\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Start Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating losses function\n",
    "@torch.no_grad()\n",
    "\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled dot product attention\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        # create attention scores\n",
    "        weights = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        drop = self.dropout(weights)\n",
    "        \n",
    "        # weighted aggregation of values\n",
    "        v = self.value(x)\n",
    "        out = drop @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a feedforward class\n",
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd*4, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a transformer block\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.attention = MultiHeadAttention(n_head, head_size)\n",
    "        self.feedforward = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.attention(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.feedforward(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to make a GPT model\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # Adding a positional embedding table as well\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_embd) # added new parameter, n_embd\n",
    "        # Adding 4 decoder layers\n",
    "        self.blocks = nn.Sequential(*(Block(n_embd, n_head=n_head) for _ in range(n_layer)))\n",
    "        # final layer normalization\n",
    "        self.lm_f = nn.LayerNorm(n_embd)\n",
    "        # unsure what this is below\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    # std variables to help training converge better\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "\n",
    "        # Add in token and posiional embeddings\n",
    "        token_embd = self.token_embedding_table(index) # (B, T, C)\n",
    "        pos_embd = self.positional_embedding_table(torch.arange(T)) # (T, C)\n",
    "        x = token_embd + pos_embd # (B, T, C)\n",
    "        x = self.blocks(x) # (B, T, C)\n",
    "        x = self.lm_f(x) # (B, T, C)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(index)\n",
    "            logits = logits[:, -1, :]\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            index_next = torch.multinomial(probabilities, num_samples=1)\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "        return index\n",
    "\n",
    "# model = GPTLanguageModel(vocab_size)\n",
    "\n",
    "# in order to deal with a prompt, make the GPT model encounter a prompt size of around 50.\n",
    "#model = GPTLangugeModel(vocab_size + prompt_size)\n",
    "#Chris - changed spelling\n",
    "model = GPTLanguageModel(vocab_size + prompt_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10244\\2330220691.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0meval_iters\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"step {iter}, train loss: {losses['train']}, val loss: {losses['val']}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10244\\3648292872.py\u001b[0m in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10244\\133303816.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, index, targets)\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3052\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3053\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3055\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Creating an Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}, train loss: {losses['train']}, val loss: {losses['val']}\")\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model into a pickle file\n",
    "with open('model-01.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model, if necessary\n",
    "with open('model-01.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary cell \n",
    "model2 = GPTLanguageModel(vocab_size + prompt_size)\n",
    "\n",
    "# index_to_word.update({0: ''})\n",
    "# word_to_index[''] = word_to_index.pop('block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the prompt into the dictionary used for the training dataset\n",
    "# index_to_word.update({0: ''})\n",
    "# word_to_index[''] = word_to_index.pop('block')\n",
    "\n",
    "# Issue a pompt, and we can try and generate an answer from it\n",
    "prompt = 'Can you give me an overview on Probabilistic Latent Semantic Analysis'.split()\n",
    "# k = len(prompt)\n",
    "\n",
    "# Find the maximum key in the existing dictionaries\n",
    "max_key = max(word_to_index.values()) if word_to_index else -1\n",
    "\n",
    "# Enumerate through the new words and add them to the dictionaries\n",
    "for word in prompt:\n",
    "    if word not in word_to_index:\n",
    "        max_key += 1\n",
    "        word_to_index[word] = max_key\n",
    "        index_to_word[max_key] = word\n",
    "\n",
    "# context = torch.zeros((1, 1), dtype=torch.long)\n",
    "context = torch.tensor(words_to_indices(prompt), dtype=torch.long)\n",
    "words_generated = model2.generate(context.unsqueeze(0), max_new_tokens=50)[0].tolist()\n",
    "words_generated = [x for x in words_generated if x <= len(index_to_word)]\n",
    "generated_terms = indices_to_words(words_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function depend illustrate lunch dynamic stage gamma series attach smoking foundation pay pay care syntax regression basis simplification time code assembly amount counter human kind sum stops validate collect inbound somehow diverse perturb attempt award run cleanness knowledge entry modeling detection methodology contingency master extent collaboration closest influence\n"
     ]
>>>>>>> b2fabcbe84ccfe82b2de108859b8e389a584f4d4
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
<<<<<<< HEAD
  "nbformat": 4,
  "nbformat_minor": 0
}
=======
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flask Frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will take in a question and return the response from our LLM\n",
    "def answer(question):\n",
    "    promt = question.split()\n",
    "    for word in prompt:\n",
    "        if word not in word_to_index:\n",
    "            max_key += 1\n",
    "            word_to_index[word] = max_key\n",
    "            index_to_word[max_key] = word\n",
    "    context = torch.tensor(words_to_indices(prompt), dtype=torch.long)\n",
    "    words_generated = model2.generate(context.unsqueeze(0), max_new_tokens=50)[0].tolist()\n",
    "    words_generated = [x for x in words_generated if x <= len(index_to_word)]\n",
    "    generated_terms = indices_to_words(words_generated)\n",
    "    return ' '.join(generated_terms[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Flask in c:\\users\\chris\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from Flask) (2.0.3)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from Flask) (2.11.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from Flask) (2.0.1)\n",
      "Requirement already satisfied: click>=5.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from Flask) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\chris\\anaconda3\\lib\\site-packages (from click>=5.1->Flask) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from Jinja2>=2.10.1->Flask) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "#This code creates the initial website\n",
    "@app.route(\"/\")\n",
    "def html():\n",
    "    return render_template(\"index.html\", info=\"Hello, welcome to out chatbot! Ask a question about course material above.\")\n",
    "\n",
    "#This code will update the website when the user submits a question\n",
    "@app.route(\"/update\", methods=[\"GET\",\"POST\"])\n",
    "def update():\n",
    "    question = request.form['input']\n",
    "    response = answer(question)\n",
    "    return render_template(\"index.html\", info=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
>>>>>>> b2fabcbe84ccfe82b2de108859b8e389a584f4d4
