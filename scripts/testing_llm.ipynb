{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week Number</th>\n",
       "      <th>Lesson Number</th>\n",
       "      <th>Lesson Title</th>\n",
       "      <th>Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Natural Language Content Analysis</td>\n",
       "      <td>This lecture is about Natural Language of Cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Access</td>\n",
       "      <td>In this lecture,\\r\\nwe're going to talk about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Retrieval Problem</td>\n",
       "      <td>This lecture is about\\r\\nthe text retrieval pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overview of Text Retrieval Methods</td>\n",
       "      <td>This lecture is a overview of\\r\\ntext retrieva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Vector Space Model - Basic Idea</td>\n",
       "      <td>This lecture is about the\\r\\nvector space retr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Week Number  Lesson Number                        Lesson Title  \\\n",
       "0            1              1   Natural Language Content Analysis   \n",
       "1            1              2                         Text Access   \n",
       "2            1              3              Text Retrieval Problem   \n",
       "3            1              4  Overview of Text Retrieval Methods   \n",
       "4            1              5     Vector Space Model - Basic Idea   \n",
       "\n",
       "                                          Transcript  \n",
       "0  This lecture is about Natural Language of Cont...  \n",
       "1  In this lecture,\\r\\nwe're going to talk about ...  \n",
       "2  This lecture is about\\r\\nthe text retrieval pr...  \n",
       "3  This lecture is a overview of\\r\\ntext retrieva...  \n",
       "4  This lecture is about the\\r\\nvector space retr...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Directory path\n",
    "#directory_path = 'C:\\\\Users\\\\azaan\\\\OneDrive\\\\Documents\\\\GitHub\\\\cs410_LLM_project\\\\data\\\\all_lectures.csv'\n",
    "#Chris's Directory\n",
    "directory_path = 'C:\\\\Users\\\\chris\\\\cs410_LLM_project\\\\data\\\\all_lectures.csv'\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "df = pd.DataFrame(columns=['Week Number', 'Lesson Number', 'Lesson Title', 'Transcript'])\n",
    "\n",
    "# Read in csv to dataframe\n",
    "df = pd.read_csv(directory_path)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# clean up words in dataset -- this includes removing stopwords\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, words, brown\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"brown\")\n",
    "#Chris - added following downloads\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# initialize dictionary\n",
    "global_dictionary  = set(words.words()) | set(brown.words())\n",
    "global_dictionary = {word.lower() for word in global_dictionary}\n",
    "remove_words = list(stop_words) # might need to use word_tokenize\n",
    "remove_words.extend(['Play', 'video', 'starting', 'at', '::', 'follow', 'transcript', 'natural', 'language', 'lecture', 'processing']) # add the common words that's include d in transcript\n",
    "\n",
    "# Now start actually cleaning the text\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase\n",
    "    text = text.replace('\\n', ' ') # remove newline indicator\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # case\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text) # website\n",
    "    text = re.sub(r'(\\b\\w+\\b)(?: \\1)+', r'\\1', text) # remove duplicate next word after space\n",
    "    text = re.sub(r'\\b(?![aI]\\b)\\w\\b', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Remove stopwords and only keep words in dictionary\n",
    "def remove_terms(text):\n",
    "    text = clean_text(text)\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in remove_words] # remove stopwords\n",
    "    filtered_words = [word for word in filtered_words if word in global_dictionary] # remove if not in global dictionary\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "df['Transcript_Cleaned'] = df['Transcript'].apply(remove_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'content analysis see picture really first step process text data text data languages computers understand languages extent order make use data thats topic going cover three things first main technique obtain understanding second state art stands finally going cover relation text retrieval first well best way explain think see text foreign understand order understand text basically computers facing looking simple sentence like dog chasing boy playground dont problems understanding sentence imagine computer would order understand well general would following first would know dog noun verb called lexical analysis tagging need figure syntactic categories words thats first step going figure structure sentence example shows dog would go together form noun phrase wont dog go first structures right structure shows might get look sentence try interpret sentence words would go together first go together words show noun phrases intermediate components verbal phrases finally sentence get structure need something called semantic analysis may parser accompanying program would automatically created structure point would know structure sentence still dont know meaning sentence go semantic analysis mind usually map sentence already know knowledge base example might imagine dog looks like theres boy theres activity computer would use symbols denote wed use symbol denote dog denote boy denote playground also chasing activity thats happening relationship chasing connects symbols computer would obtain understanding sentence representation could also infer things might indeed naturally think something else read text called inference example believe chased person might scared rule see computers could also infer boy maybe scared extra knowledge youd infer based understanding text even go understand person say sentence use called pragmatic analysis order understand speak actor sentence right say something basically achieve goal theres purpose use case person said sentence might reminding another person bring back dog could one possible intent reach level understanding would require steps computer would go steps order completely understand sentence yet humans trouble understanding instantly would get everything reason thats large knowledge base brain use common sense knowledge help interpret sentence computers unfortunately hard obtain understanding dont knowledge base still incapable reasoning uncertainties makes difficult computers fundamental reason difficult computers simply designed computers languages designed us communicate languages designed computers example programming languages harder us right languages designed make communication efficient result omit lot common sense knowledge assume everyone knows also keep lot ambiguities assume receiver hearer could know decipher ambiguous word based knowledge context theres need demand different words different meanings could overload word different meanings without problem reasons makes every step difficult computers ambiguity main difficulty common sense reasoning often required thats also hard let give examples challenges consider word level ambiguity word different syntactic categories example design noun verb word root may multiple meanings square root math sense root plant might able think meanings also syntactical ambiguities example main topic actually interpreted two ways terms structure think moment see figure usually think could also think say example synaptic ambiguity different structures applied sequence words another common example ambiguous sentence following man saw boy telescope case question telescope called prepositional phrase attachment ambiguity attachment ambiguity generally dont problem ambiguities lot background knowledge help us ambiguity another example difficulty anaphora resolution think sentence john persuaded bill buy tv question refer john bill something use background context figure finally presupposition another problem consider sentence quit smoking obviously implies smoked imagine computer wants understand subtle differences meanings would use lot knowledge figure also would maintain large knowledge base meanings words connected common sense knowledge world difficult result steep perfect fact far perfect understanding using computers slide sort gains simplified view state art part speech tagging pretty well showed accuracy number obviously based certain dont take literally shows pretty well still perfect terms partial pretty well means get noun phrase structures verb phrase structure segment sentence dude correct terms structure evaluation results seen accuracy terms partial sentences say numbers relative numbers might lower existing work evaluated using news lot numbers less toward news data think social media data accuracy likely lower terms semantical analysis far able complete understanding sentence techniques would allow us partial understanding sentence could mention example techniques allow us extract entities relations mentioned text articles example recognizing dimensions people locations organizations text called entity extraction may able recognize relations example person visited place person met person company acquired another company relations extracted using computer current techniques theyre perfect well entities entities harder others also word sense disintegration extend figure whether word sentence would certain meaning another context computer could figure different meaning perfect something direction also sentiment analysis meaning figure whether sentence positive negative especially useful review analysis example examples semantic analysis help us obtain partial understanding sentences giving us complete understanding showed sentence would still help us gain understanding content useful terms inference yet probably general difficulty inference uncertainties general challenge artificial intelligence thats probably also dont complete semantical representation inaudible text hard yet domains perhaps limited domains lot restrictions word uses may able perform inference extent general really reliably speech act analysis also far done analysis special cases roughly gives idea state art also talk little bit cant cant even part speech tagging looks like simple task think example two uses may different syntactic categories try make fine grained distinctions easy figure differences also hard general complete sentence saw example ambiguity hard imagine example use lot knowledge context sentence background order figure actually telescope although sentence looks simple actually pretty hard cases sentence long imagine four five prepositional phrases even possibilities figure also harder precise deep semantic analysis example sentence john owns restaurant define owns exactly word something understand hard precisely describe meaning computers result robust general techniques process lot text data shallow way meaning superficial analysis example parts speech tagging partial recognizing sentiment deep understanding really understanding exact meaning sentence hand deep understanding techniques tend scale well meaning would fill restricted text dont restrict text domain use words techniques tend work well may work well based machine learning techniques data similar training data program trained generally wouldnt work well data different training data pretty much summarizes state art course within short amount time cant really give complete view big field id expect see multiple courses topic relevance topic talk useful know background case happen exposed mean text retrieval well text retrieval dealing kinds text hard restrict text certain domain also often dealing lot text data means techniques must general robust efficient implies today use fairly shallow techniques text retrieval fact search engines today use something called bag words representation probably simplest representation possibly think turn text data simply bag words meaning well keep individual words well ignore orders words well keep duplicated occurrences words called bag words representation represent text way ignore lot valid information makes harder understand exact meaning sentence weve lost order yet representation tends actually work pretty well search tasks partly search task difficult see matching query words text document chances document topic although exceptions comparison tasks example machine translation would require understand accurately otherwise translation would wrong comparison tasks relatively easy representation often sufficient thats also representation major search engines today like bing using course put parentheses course many queries answered well current search engines require replantation would go beyond bag words replantation would require done another reason used sophisticated techniques modern search engines thats retrieval techniques actually naturally solved problem one example word sense disintegration think word like java could mean coffee could mean program look word would ambiguous user uses word query usually words example looking usage java implies java means program contest help us naturally prefer documents java referring program languages documents would probably match well java occurs documents means coffee would never match small probability case retrieval techniques naturally achieve goal word another example technique called feedback talk later lectures technique would allow us additional words query additional words could related query words words help matching documents original query words occurred achieves extent semantic matching terms techniques also helped us bypass difficulties however long run still need deeper techniques order improve accuracy current search engines particularly needed complex search tasks question answering recently launched knowledge graph one step toward goal knowledge graph would contain entities relations goes beyond simple bag words replantation technique help us improve search engine utility significantly although open topic research exploration sum talked weve talked state techniques cannot finally also explain bag words replantation remains dominant replantation used modern search engines even though deeper would needed future search engines want know take look additional readings cited one thats good point thanks'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Transcript_Cleaned'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bigrams and trigrams from data\n",
    "\n",
    "# Function to filter bigrams or trigrams\n",
    "def ngram_filter(ngram):\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if not all(tag[1] in ['JJ', 'NN'] for tag in tags):\n",
    "        return False\n",
    "    if any(word in stop_words for word in ngram):\n",
    "        return False\n",
    "    if 'n' in ngram or 't' in ngram:\n",
    "        return False\n",
    "    if 'PRON' in ngram:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Function to find top ngrams\n",
    "def find_top_ngrams(texts, ngram_measures, min_freq=50, min_pmi=5, top_k=100):\n",
    "    finder = nltk.collocations.BigramCollocationFinder.from_documents(texts)\n",
    "    finder.apply_freq_filter(min_freq)\n",
    "    ngram_scores = finder.score_ngrams(ngram_measures.pmi)\n",
    "    filtered_ngrams = [ngram for ngram, pmi in ngram_scores if ngram_filter(ngram) and pmi > min_pmi]\n",
    "    return [' '.join(ngram) for ngram in filtered_ngrams][:top_k]\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigrams = find_top_ngrams([text.split() for text in df['Transcript_Cleaned']], bigram_measures)\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "trigrams = find_top_ngrams([text.split() for text in df['Transcript_Cleaned']], trigram_measures)\n",
    "\n",
    "# Function to replace ngrams in text\n",
    "def replace_ngrams(text):\n",
    "    for gram in trigrams:\n",
    "        text = text.replace(gram, '_'.join(gram.split()))\n",
    "    for gram in bigrams:\n",
    "        text = text.replace(gram, '_'.join(gram.split()))\n",
    "    return text\n",
    "\n",
    "# Apply ngram replacements to the text\n",
    "df['Grams'] = df['Transcript_Cleaned'].map(replace_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize reviews + remove stop words + filter only nouns\n",
    "def tokenize_and_filter(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.lower() not in stop_words and len(word) > 2]\n",
    "    pos_comment = nltk.pos_tag(words)\n",
    "    filtered = [word[0] for word in pos_comment if word[1] in ['NN']]\n",
    "    return filtered\n",
    "\n",
    "df['Grams'] = df['Grams'].map(tokenize_and_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I will make embeddings for my words, let's see if it works\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import  torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "results = set()\n",
    "df['Grams'].apply(results.update)\n",
    "vocab_size = len(results)\n",
    "\n",
    "# Create a vocabulary dictionary\n",
    "word_to_index = {word: idx for idx, word in enumerate(results)}\n",
    "\n",
    "# Convert words to indices in your DataFrame\n",
    "# AKA Encode these\n",
    "# df['Grams_indices'] = df['Grams'].apply(lambda x: [word_to_index[word] for word in x])\n",
    "def words_to_indices(words):\n",
    "    return [word_to_index[word] for word in words]\n",
    "df['Grams_indices'] = df['Grams'].apply(words_to_indices)\n",
    "\n",
    "# Create a reverse dictionary\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "# Function to convert indices back to words\n",
    "def indices_to_words(indices):\n",
    "    return [index_to_word[idx] for idx in indices]\n",
    "\n",
    "# # Apply the function to the 'Grams_indices' column\n",
    "# Aka Decode these grams\n",
    "# df['Decoded_Grams'] = df['Grams_indices'].apply(indices_to_words)\n",
    "\n",
    "# Pad sequences to a specified length (e.g., maxlen)\n",
    "maxlen = 60  # You can adjust this based on your data\n",
    "padded_indices = pad_sequence([torch.LongTensor(seq) for seq in df['Grams_indices']], batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,  920,  918, 1519],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 663,  858,  215,  ...,  376,  933,  239],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [1402, 1327,  376,  ...,    0,    0,    0]])\n",
      "tensor([[   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [   0,    0,    0,  ...,  918, 1519, 1218],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 858,  215, 1519,  ...,  933,  239, 1177],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [1327,  376,  724,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "# make a batch and set up parameters\n",
    "block_size = 256\n",
    "batch_size = 128\n",
    "max_iters = 5000\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250\n",
    "# new\n",
    "n_embd = 128\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "n_head = 4\n",
    "prompt_size = 50\n",
    "\n",
    "# Flatten the padded indices used to identify each word\n",
    "data = flattened_indices = padded_indices.view(-1)\n",
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "# print(len(data))\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Start Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating losses function\n",
    "@torch.no_grad()\n",
    "\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled dot product attention\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "\n",
    "        # create attention scores\n",
    "        weights = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        drop = self.dropout(weights)\n",
    "        \n",
    "        # weighted aggregation of values\n",
    "        v = self.value(x)\n",
    "        out = drop @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a feedforward class\n",
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd*4, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a transformer block\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.attention = MultiHeadAttention(n_head, head_size)\n",
    "        self.feedforward = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.attention(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.feedforward(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to make a GPT model\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        # Adding a positional embedding table as well\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_embd) # added new parameter, n_embd\n",
    "        # Adding 4 decoder layers\n",
    "        self.blocks = nn.Sequential(*(Block(n_embd, n_head=n_head) for _ in range(n_layer)))\n",
    "        # final layer normalization\n",
    "        self.lm_f = nn.LayerNorm(n_embd)\n",
    "        # unsure what this is below\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    # std variables to help training converge better\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "\n",
    "        # Add in token and posiional embeddings\n",
    "        token_embd = self.token_embedding_table(index) # (B, T, C)\n",
    "        pos_embd = self.positional_embedding_table(torch.arange(T)) # (T, C)\n",
    "        x = token_embd + pos_embd # (B, T, C)\n",
    "        x = self.blocks(x) # (B, T, C)\n",
    "        x = self.lm_f(x) # (B, T, C)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(index)\n",
    "            logits = logits[:, -1, :]\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            index_next = torch.multinomial(probabilities, num_samples=1)\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "        return index\n",
    "\n",
    "# model = GPTLanguageModel(vocab_size)\n",
    "\n",
    "# in order to deal with a prompt, make the GPT model encounter a prompt size of around 50.\n",
    "#model = GPTLangugeModel(vocab_size + prompt_size)\n",
    "#Chris - changed spelling\n",
    "model = GPTLanguageModel(vocab_size + prompt_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10244\\2330220691.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0meval_iters\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"step {iter}, train loss: {losses['train']}, val loss: {losses['val']}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10244\\3648292872.py\u001b[0m in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10244\\133303816.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, index, targets)\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3052\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3053\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3055\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Creating an Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}, train loss: {losses['train']}, val loss: {losses['val']}\")\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model into a pickle file\n",
    "with open('model-01.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model, if necessary\n",
    "with open('model-01.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary cell \n",
    "model2 = GPTLanguageModel(vocab_size + prompt_size)\n",
    "\n",
    "# index_to_word.update({0: ''})\n",
    "# word_to_index[''] = word_to_index.pop('block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the prompt into the dictionary used for the training dataset\n",
    "# index_to_word.update({0: ''})\n",
    "# word_to_index[''] = word_to_index.pop('block')\n",
    "\n",
    "# Issue a pompt, and we can try and generate an answer from it\n",
    "prompt = 'Can you give me an overview on Probabilistic Latent Semantic Analysis'.split()\n",
    "# k = len(prompt)\n",
    "\n",
    "# Find the maximum key in the existing dictionaries\n",
    "max_key = max(word_to_index.values()) if word_to_index else -1\n",
    "\n",
    "# Enumerate through the new words and add them to the dictionaries\n",
    "for word in prompt:\n",
    "    if word not in word_to_index:\n",
    "        max_key += 1\n",
    "        word_to_index[word] = max_key\n",
    "        index_to_word[max_key] = word\n",
    "\n",
    "# context = torch.zeros((1, 1), dtype=torch.long)\n",
    "context = torch.tensor(words_to_indices(prompt), dtype=torch.long)\n",
    "words_generated = model2.generate(context.unsqueeze(0), max_new_tokens=50)[0].tolist()\n",
    "words_generated = [x for x in words_generated if x <= len(index_to_word)]\n",
    "generated_terms = indices_to_words(words_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function depend illustrate lunch dynamic stage gamma series attach smoking foundation pay pay care syntax regression basis simplification time code assembly amount counter human kind sum stops validate collect inbound somehow diverse perturb attempt award run cleanness knowledge entry modeling detection methodology contingency master extent collaboration closest influence\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(generated_terms[len(prompt):]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flask Frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function will take in a question and return the response from our LLM\n",
    "def answer(question):\n",
    "    promt = question.split()\n",
    "    for word in prompt:\n",
    "        if word not in word_to_index:\n",
    "            max_key += 1\n",
    "            word_to_index[word] = max_key\n",
    "            index_to_word[max_key] = word\n",
    "    context = torch.tensor(words_to_indices(prompt), dtype=torch.long)\n",
    "    words_generated = model2.generate(context.unsqueeze(0), max_new_tokens=50)[0].tolist()\n",
    "    words_generated = [x for x in words_generated if x <= len(index_to_word)]\n",
    "    generated_terms = indices_to_words(words_generated)\n",
    "    return ' '.join(generated_terms[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Flask in c:\\users\\chris\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from Flask) (2.0.3)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from Flask) (2.11.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from Flask) (2.0.1)\n",
      "Requirement already satisfied: click>=5.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from Flask) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\chris\\anaconda3\\lib\\site-packages (from click>=5.1->Flask) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from Jinja2>=2.10.1->Flask) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "#This code creates the initial website\n",
    "@app.route(\"/\")\n",
    "def html():\n",
    "    return render_template(\"index.html\", info=\"Hello, welcome to out chatbot! Ask a question about course material above.\")\n",
    "\n",
    "#This code will update the website when the user submits a question\n",
    "@app.route(\"/update\", methods=[\"GET\",\"POST\"])\n",
    "def update():\n",
    "    question = request.form['input']\n",
    "    response = answer(question)\n",
    "    return render_template(\"index.html\", info=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
