{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read in the 3 txt files and combine into 1\n",
    "# import glob\n",
    "# import os\n",
    "\n",
    "# # os.chdir('sample_data')\n",
    "# read_files = glob.glob(\"sample_data/*.txt\")\n",
    "\n",
    "# with open(\"sample_data/combined.txt\", \"wb\") as outfile:\n",
    "#     for f in read_files:\n",
    "#         with open(f, \"rb\") as infile:\n",
    "#             outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Filename                                            Content\n",
      "0  7_1.txt  0:00\\n[SOUND] In this lecture we give an overv...\n",
      "1  7_2.txt  0:00\\n[SOUND] So, looking at the text mining p...\n",
      "2  7_3.txt  0:00\\n[SOUND]\\nPlay video starting at ::9 and ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azaan\\AppData\\Local\\Temp\\ipykernel_32988\\3844069920.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Filename': filename, 'Content': content}, ignore_index=True)\n",
      "C:\\Users\\azaan\\AppData\\Local\\Temp\\ipykernel_32988\\3844069920.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Filename': filename, 'Content': content}, ignore_index=True)\n",
      "C:\\Users\\azaan\\AppData\\Local\\Temp\\ipykernel_32988\\3844069920.py:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Filename': filename, 'Content': content}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# # import the dataset\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # Directory path\n",
    "# directory_path = 'C:\\\\Users\\\\azaan\\\\OneDrive\\\\Documents\\\\GitHub\\\\cs410_LLM_project\\\\sample_data'\n",
    "\n",
    "# # Initialize an empty DataFrame\n",
    "# df = pd.DataFrame(columns=['Filename', 'Content'])\n",
    "\n",
    "# # Iterate over each file in the directory\n",
    "# for filename in os.listdir(directory_path):\n",
    "#     if filename.endswith('.txt'):\n",
    "#         file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "#         # Read the content of the text file\n",
    "#         with open(file_path, 'r') as file:\n",
    "#             content = file.read()\n",
    "        \n",
    "#         # Append a row to the DataFrame\n",
    "#         df = df.append({'Filename': filename, 'Content': content}, ignore_index=True)\n",
    "\n",
    "# # Display the resulting DataFrame\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week Number</th>\n",
       "      <th>Lesson Number</th>\n",
       "      <th>Lesson Title</th>\n",
       "      <th>Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Natural Language Content Analysis</td>\n",
       "      <td>This lecture is about Natural Language of Cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Access</td>\n",
       "      <td>In this lecture,\\nwe're going to talk about th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Retrieval Problem</td>\n",
       "      <td>This lecture is about\\nthe text retrieval prob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overview of Text Retrieval Methods</td>\n",
       "      <td>This lecture is a overview of\\ntext retrieval ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Vector Space Model - Basic Idea</td>\n",
       "      <td>This lecture is about the\\nvector space retrie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Week Number  Lesson Number                        Lesson Title  \\\n",
       "0            1              1   Natural Language Content Analysis   \n",
       "1            1              2                         Text Access   \n",
       "2            1              3              Text Retrieval Problem   \n",
       "3            1              4  Overview of Text Retrieval Methods   \n",
       "4            1              5     Vector Space Model - Basic Idea   \n",
       "\n",
       "                                          Transcript  \n",
       "0  This lecture is about Natural Language of Cont...  \n",
       "1  In this lecture,\\nwe're going to talk about th...  \n",
       "2  This lecture is about\\nthe text retrieval prob...  \n",
       "3  This lecture is a overview of\\ntext retrieval ...  \n",
       "4  This lecture is about the\\nvector space retrie...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Directory path\n",
    "directory_path = 'C:\\\\Users\\\\azaan\\\\OneDrive\\\\Documents\\\\GitHub\\\\cs410_LLM_project\\\\data\\\\all_lectures.csv'\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "df = pd.DataFrame(columns=['Week Number', 'Lesson Number', 'Lesson Title', 'Transcript'])\n",
    "\n",
    "# Read in csv to dataframe\n",
    "df = pd.read_csv(directory_path)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\azaan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\azaan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\azaan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# clean up words in dataset -- this includes removing stopwords\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, words, brown\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"brown\")\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# initialize dictionary\n",
    "global_dictionary  = set(words.words()) | set(brown.words())\n",
    "global_dictionary = {word.lower() for word in global_dictionary}\n",
    "remove_words = list(stop_words) # might need to use word_tokenize\n",
    "remove_words.extend(['Play', 'video', 'starting', 'at', '::', 'follow', 'transcript', 'natural', 'language', 'lecture', 'processing']) # add the common words that's include d in transcript\n",
    "\n",
    "# Now start actually cleaning the text\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase\n",
    "    text = text.replace('\\n', ' ') # remove newline indicator\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # case\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text) # website\n",
    "    text = re.sub(r'(\\b\\w+\\b)(?: \\1)+', r'\\1', text) # remove duplicate next word after space\n",
    "    text = re.sub(r'\\b(?![aI]\\b)\\w\\b', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Remove stopwords and only keep words in dictionary\n",
    "def remove_terms(text):\n",
    "    text = clean_text(text)\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in remove_words] # remove stopwords\n",
    "    filtered_words = [word for word in filtered_words if word in global_dictionary] # remove if not in global dictionary\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "df['Transcript_Cleaned'] = df['Transcript'].apply(remove_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'going talk text access previous talked content analysis explained state techniques still good enough process lot unrestricted text data robust manner result bag words remains popular applications like search engine going talk help users get access text data also important step convert raw big text data small random data actually needed specific application main question well address text information system help users get access relevant text data going cover two complimentary push versus pull going talk two ways implement pull mode querying versus browsing first push versus pull two different ways connect users right information right time difference takes initiative party takes initiative pull mode users take initiative start information access process case user typically would use search engine fulfill goal example user may type query browse results find relevant information usually appropriate satisfying users ad hoc information need ad hoc information need temporary information need example want buy product suddenly need read reviews related product cracked information purchased product generally longer need information temporary information need case hard system predict need proper users take initiative thats search engines useful today many people many information needs time speaking probably many queries mostly adequate information needs pull mode contrast push mode system would take initiative push information user recommend information user case usually supported recommender system would appropriate user stable information example may research interest topic interest tends stay rather stable hobby another example stable information need case system interact learn interest monitor information stream system seen relevant items interest system could take initiative recommend information example news filter news recommended system could monitor news stream identify interesting news simply push news articles mode information access may also property system good knowledge users need happens search context example search information web search engine might infer might also interested something related formation would recommend information reminds example advertisement placed search page two high level two modes text access lets look pull mode detail pull mode distinguish two ways help users querying versus browsing querying user would enter query typical query search engine system would return relevant documents use works well user knows exactly used know exactly looking tend know right query works well time also know sometimes doesnt work well dont know right use query want browse information topic area use browsing would useful case case browsing users would simply navigate relevant information following paths supported structures documents system would maintain kind structures user could structures navigate really works well user wants explore information space user doesnt know using query simply user finds inconvenient type query even user knows query type user using search information still harder enter query case browsing tends convenient relationship browsing querying best understood making imagine site seeing imagine touring city know exact address attraction taking taxi perhaps fastest way go directly site dont know exact address may need walk around take taxi nearby place walk around turns exactly information studies know exactly looking use right query find information thats usually fastest way find information dont know exact use well clearly probably wont well related pages need also walk around information space meaning following links browsing finally get relevant page want learn likely lot browsing like looking around area want see interesting attractions related inaudible analogy also tells us today good support query dont really good support browsing order browse effectively need map guide us like need map chicago city chicago need topical map tour information space construct topical map fact interesting research question might bring us interesting browsing experience web applications summarize weve talked two high level text access push pull push tends supported recommender system pull tends supported search engine course sophisticated inaudible information system combine two pull mode inaudible querying browsing generally want combine two ways help assist support querying browsing want know relationship pull push read article give excellent discussion relationship machine filtering information retrieval informational filtering similar information recommendation push mode information access'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Transcript_Cleaned'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bigrams and trigrams from data\n",
    "\n",
    "# Function to filter bigrams or trigrams\n",
    "def ngram_filter(ngram):\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if not all(tag[1] in ['JJ', 'NN'] for tag in tags):\n",
    "        return False\n",
    "    if any(word in stop_words for word in ngram):\n",
    "        return False\n",
    "    if 'n' in ngram or 't' in ngram:\n",
    "        return False\n",
    "    if 'PRON' in ngram:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Function to find top ngrams\n",
    "def find_top_ngrams(texts, ngram_measures, min_freq=50, min_pmi=5, top_k=100):\n",
    "    finder = nltk.collocations.BigramCollocationFinder.from_documents(texts)\n",
    "    finder.apply_freq_filter(min_freq)\n",
    "    ngram_scores = finder.score_ngrams(ngram_measures.pmi)\n",
    "    filtered_ngrams = [ngram for ngram, pmi in ngram_scores if ngram_filter(ngram) and pmi > min_pmi]\n",
    "    return [' '.join(ngram) for ngram in filtered_ngrams][:top_k]\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "bigrams = find_top_ngrams([text.split() for text in df['Transcript_Cleaned']], bigram_measures)\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "trigrams = find_top_ngrams([text.split() for text in df['Transcript_Cleaned']], trigram_measures)\n",
    "\n",
    "# Function to replace ngrams in text\n",
    "def replace_ngrams(text):\n",
    "    for gram in trigrams:\n",
    "        text = text.replace(gram, '_'.join(gram.split()))\n",
    "    for gram in bigrams:\n",
    "        text = text.replace(gram, '_'.join(gram.split()))\n",
    "    return text\n",
    "\n",
    "# Apply ngram replacements to the text\n",
    "df['Grams'] = df['Transcript_Cleaned'].map(replace_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize reviews + remove stop words + filter only nouns\n",
    "def tokenize_and_filter(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.lower() not in stop_words and len(word) > 2]\n",
    "    pos_comment = nltk.pos_tag(words)\n",
    "    filtered = [word[0] for word in pos_comment if word[1] in ['NN']]\n",
    "    return filtered\n",
    "\n",
    "df['Grams'] = df['Grams'].map(tokenize_and_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0102e-38, 5.9694e-39, 1.0286e-38, 8.9081e-39, 8.9082e-39, 6.9796e-39],\n",
       "        [9.0919e-39, 9.9184e-39, 7.3470e-39, 1.0194e-38, 1.0469e-38, 1.0010e-38],\n",
       "        [8.4490e-39, 1.1112e-38, 9.5511e-39, 1.0102e-38, 7.3470e-39, 1.0653e-38],\n",
       "        [1.0194e-38, 4.6838e-39, 4.4082e-39, 9.9184e-39, 9.0000e-39, 1.0561e-38],\n",
       "        [1.0653e-38, 4.1327e-39, 8.9082e-39, 9.8265e-39, 9.4592e-39, 1.0561e-38],\n",
       "        [1.0653e-38, 1.0469e-38, 9.5510e-39, 8.7245e-39, 8.9082e-39, 9.8266e-39]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some pytorch  training\n",
    "import torch\n",
    "\n",
    "randint = torch.randint(0, 10000, (6,1))\n",
    "tensor = torch.tensor([[5, 8, 7],  [4, 8, 6], [9, 8, 4]])\n",
    "zeros = torch.zeros([6,6])\n",
    "ones = torch.ones([6,6])\n",
    "arange = torch.arange(0, 10000)\n",
    "linspace = torch.linspace(0, 10000, steps=100)\n",
    "logspace = torch.logspace(-5000, 5000, steps=1000)\n",
    "eye = torch.eye(6)\n",
    "empty_like = torch.empty_like(eye)\n",
    "\n",
    "randint\n",
    "tensor\n",
    "zeros\n",
    "ones\n",
    "arange\n",
    "linspace\n",
    "logspace\n",
    "eye\n",
    "empty_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azaan\\AppData\\Local\\Temp\\ipykernel_45480\\388226070.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  sample_softmax = F.softmax(sample)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3333, 0.3333, 0.3333])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# print(device)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# more pytorch fucntions\n",
    "probabilities = torch.tensor([0.4, 0.6, 0.1])\n",
    "samples = torch.multinomial(probabilities, num_samples=7, replacement=True)\n",
    "ones = torch.ones([6,6])\n",
    "tril = torch.tril(ones)\n",
    "triu = torch.triu(ones)\n",
    "zeros = torch.zeros(6, 6)\n",
    "masked_fill = zeros.masked_fill(tril == 1, float(8))\n",
    "exp = torch.exp(masked_fill)\n",
    "transposed = tril.transpose(0, 1)\n",
    "stack = torch.stack([tril, triu, zeros])\n",
    "sample = torch.tensor([10., 10., 10.])\n",
    "linear = nn.Linear(3, 3, bias=False)\n",
    "sample_linear = linear(sample)\n",
    "sample_softmax = F.softmax(sample)\n",
    "\n",
    "samples\n",
    "ones\n",
    "tril\n",
    "triu\n",
    "zeros\n",
    "masked_fill\n",
    "exp\n",
    "transposed\n",
    "stack\n",
    "sample\n",
    "linear\n",
    "sample_linear\n",
    "sample_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I will make embeddings for my words, let's see if it works\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "results = set()\n",
    "df['Grams'].apply(results.update)\n",
    "# print(len(results))\n",
    "\n",
    "# Create a vocabulary dictionary\n",
    "word_to_index = {word: idx for idx, word in enumerate(results)}\n",
    "\n",
    "# Convert words to indices in your DataFrame\n",
    "# AKA Encode these\n",
    "df['Grams_indices'] = df['Grams'].apply(lambda x: [word_to_index[word] for word in x])\n",
    "\n",
    "# Create a reverse dictionary\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "# Function to convert indices back to words\n",
    "def indices_to_words(indices):\n",
    "    return [index_to_word[idx] for idx in indices]\n",
    "\n",
    "# # Apply the function to the 'Grams_indices' column\n",
    "# Aka Decode these grams\n",
    "# df['Decoded_Grams'] = df['Grams_indices'].apply(indices_to_words)\n",
    "\n",
    "# Pad sequences to a specified length (e.g., maxlen)\n",
    "maxlen = 50  # You can adjust this based on your data\n",
    "padded_indices = pad_sequence([torch.LongTensor(seq) for seq in df['Grams_indices']], batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[160, 106,  83],\n",
       "        [312, 186, 161],\n",
       "        [166, 229,  91]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix multiplication examples with pytorch\n",
    "a = torch.tensor([[3, 4], [5, 8], [8, 3]])\n",
    "b = torch.tensor([[8, 26, 5], [34, 7, 17]])\n",
    "\n",
    "# form matmul, can do either below\n",
    "a @ b\n",
    "matmul = torch.matmul(a, b)\n",
    "\n",
    "matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   0,  812,  691, 1457,  528, 1342,  528,  812],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1005, 1528,  513,  592, 1005,  191, 1437,  872]])\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 812,  691, 1457,  528, 1342,  528,  812,  691],\n",
      "        [   0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1528,  513,  592, 1005,  191, 1437,  872,  191]])\n"
     ]
    }
   ],
   "source": [
    "# make a batch\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "# Flatten the padded indices used to identify each word\n",
    "data = flattened_indices = padded_indices.view(-1)\n",
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "# print(len(data))\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print(x)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to make a simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, index, targets=None):\n",
    "        logits = self.token_embedding_table(index)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(index)\n",
    "            logits = logits[:, -1, :]\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "            index_next = torch.multinomial(probabilities, num_samples=1)\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "        return index\n",
    "\n",
    "# embedding_dim = 11\n",
    "# embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# embedded_indices = torch.LongTensor(df['Grams_indices'].tolist())\n",
    "# embedded_grams = embedding(padded_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['examine', 'extent', 'unique', 'notion', 'spending', 'personalization', 'september', 'second', 'deviate', 'respond', 'era', 'randomness', 'oil', 'frame', 'library', 'know', 'param', 'hear', 'quit', 'paradigm', 'motion', 'pattern', 'background_model', 'somehow', 'retrieve', 'house', 'eigenvalue', 'week', 'symbolist', 'distribution', 'setting', 'resolve_problem', 'speed', 'table', 'simplicity', 'article', 'duplicate', 'recalibration', 'balance', 'review', 'diplomacy', 'non', 'signal', 'worth', 'technology', 'quit', 'war', 'connection', 'record', 'illustrate', 'discovering', 'efficient', 'suggestion', 'sociologist', 'robot', 'pseudo', 'advertising', 'schema', 'hit', 'ser', 'statement', 'think', 'invoke', 'airport', 'technology', 'threshold', 'animal', 'comparing', 'accurate', 'note', 'show', 'consumer', 'surprising', 'conclude', 'argument', 'index', 'proper', 'retrieval', 'inventory', 'recreation', 'guide', 'frame', 'module', 'backbone', 'symmetrical', 'dislike', 'angle', 'saturday', 'dog', 'mapping', 'cetera', 'discounting', 'amount', 'accumulator', 'chance', 'synonyms', 'paragraph', 'combine', 'non', 'twitter', 'calculation', 'erase', 'collaboration', 'therefore', 'perform', 'lifetime', 'cluster', 'capture', 'dude', 'odd', 'shallow', 'lock', 'imagine', 'croft', 'stay', 'flip', 'lose', 'roll', 'reveal', 'estimate', 'reproduction', 'reasoning', 'freedom', 'libation', 'narrow', 'indicator', 'coherence', 'talk', 'coverage', 'plant', 'conversation', 'numerator', 'desire', 'lose', 'recommend', 'toy', 'advertisement', 'goal', 'sub', 'suggest', 'fast', 'tend', 'measure', 'alpha', 'reason', 'ranking', 'scoring', 'activity', 'impose', 'picture', 'extent', 'compensate', 'annotation', 'premise', 'sociologist', 'meal', 'road', 'greedy', 'seminar', 'area', 'aggregation', 'prize', 'want', 'position', 'meta', 'tilde', 'place', 'raise', 'significance', 'estate', 'bound', 'laid', 'search_engine', 'theta_sub', 'attention', 'latitude', 'underflow', 'object', 'chapter', 'switch', 'vocabulary', 'john', 'estate', 'plug', 'role', 'sky', 'wall', 'step', 'chris', 'removal', 'convenient', 'premise', 'politician', 'multiple', 'proto', 'thats', 'attachment', 'ranking', 'synthesis', 'football', 'precision', 'antithesis', 'factorization', 'desirable', 'permission', 'capitalization', 'misspell', 'mess', 'pause', 'mistake', 'create', 'depend', 'comment', 'count', 'city', 'pull', 'signal', 'deficiency', 'estep', 'recreation', 'contest', 'pierre', 'thats', 'surface', 'separating', 'trouble', 'modification', 'adjust', 'dispose', 'criticism', 'change', 'introduce', 'sorry', 'waiting', 'habit', 'address', 'posterior', 'specificity', 'tasker', 'impact', 'sample', 'exploit', 'case', 'mapper', 'heuristic', 'particular', 'parameter', 'data', 'component', 'place', 'shorter', 'reviewer', 'downside', 'prediction', 'encourage', 'improvement', 'see', 'encourage', 'rate', 'convey', 'field', 'essence', 'ama', 'divide', 'moreover', 'resolve_problem', 'versa', 'formation', 'master', 'categorize', 'abortion', 'restaurant', 'influence', 'figure', 'document', 'circle', 'bring', 'visualization', 'building', 'estep', 'imbalance', 'bar', 'jumping', 'comparing', 'multiplication', 'interpret', 'odd', 'sine', 'seminar', 'form', 'robot', 'ate', 'viewer', 'discourse', 'perform', 'rise', 'conference', 'verb', 'vision', 'modify', 'posterior', 'gun', 'guidance', 'promise', 'user', 'portion', 'phenomena', 'passage', 'company', 'speech', 'tolerance', 'freshness', 'classification', 'denote', 'slide', 'que', 'upper', 'list', 'client', 'need', 'company', 'valuable', 'discourse', 'counting', 'byproduct', 'performance', 'predict', 'quotation', 'lea', 'feature', 'science', 'partition', 'dude', 'community', 'requirement', 'significance', 'situation', 'zoom', 'infusion', 'adjacency', 'influence', 'news', 'add', 'imbalance', 'ser', 'alleviate', 'vector_space_model', 'seek', 'adjust', 'chapter', 'unique', 'expand', 'efficiency', 'judged', 'total', 'overview', 'interacting', 'crawling', 'proportion', 'causality', 'handle', 'mixture_model', 'clue', 'control', 'tool', 'thesaurus', 'verb', 'encode', 'knowing', 'recreation', 'take', 'everyone', 'graphs', 'chance', 'inventory', 'bomb', 'fetch', 'watch', 'compression', 'capture', 'basic', 'flip', 'flip', 'param', 'explore', 'whats', 'method', 'apology', 'sub', 'flow', 'accumulator', 'premise', 'suggestion', 'fashion', 'content', 'normalization', 'sentiment', 'cutoff', 'manage', 'chain', 'sparsity', 'imbalance', 'etcetera', 'guy', 'gap', 'agglomerative', 'field', 'habit', 'macro', 'trigger', 'indicating', 'filter', 'bye', 'non', 'satisfy', 'relevant', 'color', 'popularity', 'concern', 'consumer', 'generate', 'modification', 'dispose', 'denominator', 'format', 'promise', 'gene', 'random', 'vocabulary', 'name', 'floor', 'lack', 'deal', 'password', 'com', 'want', 'mechanism', 'cough', 'proto', 'condition', 'face', 'article', 'drive', 'inject', 'minimum', 'group', 'paper', 'flood', 'audition', 'intuition', 'portion', 'doubt', 'hidden', 'mean', 'regime', 'complication', 'transform', 'spent', 'root', 'subtract', 'party', 'programmer', 'visit', 'texas', 'board', 'judging', 'god', 'therefore', 'setup', 'managing', 'start', 'app', 'pure', 'corresponding', 'data', 'assumption', 'outline', 'distinguish', 'iteration', 'hold', 'allowance', 'business', 'construction', 'industry', 'conditioning', 'feed', 'council', 'joint', 'reason', 'judgment', 'animal', 'causal', 'companion', 'research', 'construct', 'meaning', 'delta', 'playground', 'margin', 'hoc', 'campaign']\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(results)\n",
    "\n",
    "\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated_terms = indices_to_words(model.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 10000\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250\n",
    "\n",
    "# Estimating losses function\n",
    "@torch.no_grad()\n",
    "\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train loss: 7.715757369995117, val loss: 7.75339412689209\n",
      "step 250, train loss: 7.627959728240967, val loss: 7.680568218231201\n",
      "step 500, train loss: 7.561256408691406, val loss: 7.5812764167785645\n",
      "step 750, train loss: 7.469468593597412, val loss: 7.532351970672607\n",
      "step 1000, train loss: 7.353198528289795, val loss: 7.462738990783691\n",
      "step 1250, train loss: 7.239411354064941, val loss: 7.38288688659668\n",
      "step 1500, train loss: 7.17946195602417, val loss: 7.340051651000977\n",
      "step 1750, train loss: 7.086147785186768, val loss: 7.255041122436523\n",
      "step 2000, train loss: 6.9901580810546875, val loss: 7.194165229797363\n",
      "step 2250, train loss: 6.93467378616333, val loss: 7.119497299194336\n",
      "step 2500, train loss: 6.812793254852295, val loss: 7.031903266906738\n",
      "step 2750, train loss: 6.6689863204956055, val loss: 6.9859395027160645\n",
      "step 3000, train loss: 6.622453689575195, val loss: 6.903146743774414\n",
      "step 3250, train loss: 6.540916442871094, val loss: 6.815154075622559\n",
      "step 3500, train loss: 6.479652404785156, val loss: 6.76450777053833\n",
      "step 3750, train loss: 6.393984317779541, val loss: 6.659424304962158\n",
      "step 4000, train loss: 6.298311233520508, val loss: 6.615868091583252\n",
      "step 4250, train loss: 6.181439399719238, val loss: 6.602420806884766\n",
      "step 4500, train loss: 6.0472846031188965, val loss: 6.479955196380615\n",
      "step 4750, train loss: 6.001980304718018, val loss: 6.390115737915039\n",
      "step 5000, train loss: 5.9344987869262695, val loss: 6.387157440185547\n",
      "step 5250, train loss: 5.754324913024902, val loss: 6.291153907775879\n",
      "step 5500, train loss: 5.773649215698242, val loss: 6.212615013122559\n",
      "step 5750, train loss: 5.624664783477783, val loss: 6.131336688995361\n",
      "step 6000, train loss: 5.532083034515381, val loss: 6.080965042114258\n",
      "step 6250, train loss: 5.363506317138672, val loss: 6.001960277557373\n",
      "step 6500, train loss: 5.247128486633301, val loss: 5.994673728942871\n",
      "step 6750, train loss: 5.207979202270508, val loss: 5.870771884918213\n",
      "step 7000, train loss: 5.096258163452148, val loss: 5.785749912261963\n",
      "step 7250, train loss: 5.035673141479492, val loss: 5.817104339599609\n",
      "step 7500, train loss: 4.974546909332275, val loss: 5.789953708648682\n",
      "step 7750, train loss: 4.801209926605225, val loss: 5.677224636077881\n",
      "step 8000, train loss: 4.877026081085205, val loss: 5.463468551635742\n",
      "step 8250, train loss: 4.698168754577637, val loss: 5.381472587585449\n",
      "step 8500, train loss: 4.74367618560791, val loss: 5.311861991882324\n",
      "step 8750, train loss: 4.633225440979004, val loss: 5.304746150970459\n",
      "step 9000, train loss: 4.573948860168457, val loss: 5.3495635986328125\n",
      "step 9250, train loss: 4.541970252990723, val loss: 5.1302385330200195\n",
      "step 9500, train loss: 4.3024773597717285, val loss: 5.204629898071289\n",
      "step 9750, train loss: 4.239582061767578, val loss: 5.07002592086792\n",
      "2.301208019256592\n"
     ]
    }
   ],
   "source": [
    "# Creating an Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}, train loss: {losses['train']}, val loss: {losses['val']}\")\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['examine', 'discovering', 'indri', 'element', 'wont', 'closeness', 'delta', 'none', 'max', 'vector_space_model', 'write', 'criterion', 'accumulate', 'methodology', 'incorporate', 'robustness', 'copy', 'meaning', 'libation', 'get', 'mechanism', 'color', 'incorrect', 'data', 'complex', 'precise', 'tag', 'hit', 'order', 'attraction', 'compensate', 'width', 'era', 'filter', 'infinity', 'relevant', 'want', 'lag', 'proto', 'climb', 'stare', 'regime', 'pierre', 'regeneration', 'couple', 'table', 'training', 'format', 'commend', 'tell', 'reason', 'partial', 'attribute', 'disadvantage', 'independence', 'war', 'taxi', 'collector', 'independence', 'total', 'confidence', 'trade', 'definition', 'word', 'relation', 'didnt', 'incentive', 'recalculation', 'thing', 'state', 'accurate', 'usability', 'domain', 'climb', 'output', 'scientist', 'energy', 'classifier', 'complexity', 'word', 'detection', 'math', 'predictor', 'holder', 'margin', 'randomly', 'kind', 'chicago', 'notation', 'optimization', 'non', 'well', 'develop', 'benefit', 'photo', 'chicago', 'discovering', 'identity', 'graph', 'recompute', 'effect', 'separator', 'log', 'buy', 'talk', 'solution', 'situation', 'lag', 'purpose', 'emphasize', 'tilde', 'optimize', 'phone', 'stop', 'mission', 'subtract', 'triangle', 'transpire', 'symbol', 'year', 'guy', 'recalculation', 'incorrect', 'constraint', 'god', 'manager', 'week', 'oil', 'duration', 'principle', 'everyone', 'form', 'manager', 'depend', 'casuality', 'incorrect', 'weather', 'engine', 'decrease', 'data', 'partition', 'product', 'regime', 'usefulness', 'estate', 'delta', 'recalibration', 'number', 'backbone', 'trouble', 'reward', 'encounter', 'field', 'bomb', 'impact', 'estimator', 'person', 'provide', 'anything', 'extension', 'construct', 'keep', 'vary', 'exploration', 'overload', 'thank', 'robust', 'dislike', 'complexity', 'type', 'convenience', 'table', 'slide', 'actor', 'july', 'manage', 'reveal', 'lag', 'making', 'test', 'negative', 'robustness', 'car', 'fed', 'sparsity', 'aspect', 'allocation', 'presentation', 'fit', 'annotate', 'study', 'byproduct', 'wont', 'sign', 'scientist', 'coherent', 'restrict', 'dimension', 'setting', 'degenerate', 'personalization', 'encode', 'adaptation', 'emphasis', 'specialization', 'discuss', 'indicator', 'disintegration', 'hypothesize', 'introduction', 'yeah', 'prominence', 'dog', 'want', 'brief', 'lunch', 'decrease', 'tag', 'lack', 'review', 'gun', 'crawl', 'bye', 'casuality', 'odd', 'book', 'treatment', 'front', 'method', 'chance', 'see', 'difference', 'structure', 'exploitation', 'hierarchy', 'approach', 'price', 'game', 'popularity', 'sorry', 'market', 'hypothesize', 'raise', 'efficiency', 'competition', 'present', 'trunk', 'arent', 'speak', 'attribution', 'twitter', 'occurrence', 'reinforcement', 'sky', 'depth', 'lose', 'backbone', 'airport', 'consumer', 'bypass', 'direction', 'face', 'hotel', 'overview', 'aggregation', 'causality', 'determine', 'ontology', 'opportunity', 'area', 'data', 'paragraph', 'copy', 'vote', 'traffic', 'angle', 'meaning', 'copy', 'air', 'difficulty', 'smoothing', 'chunk', 'england', 'star', 'delivery', 'cleanliness', 'traffic', 'web', 'view', 'business', 'pretend', 'forth', 'zeta', 'simulate', 'denote', 'passing', 'property', 'shape', 'capability', 'user', 'extent', 'law', 'donation', 'notice', 'apply', 'compensate', 'simplest', 'click', 'help', 'exploitation', 'grammar', 'partition', 'topic_modeling', 'context', 'war', 'road', 'placement', 'size', 'probability_word_distribution', 'highway', 'reduction', 'micro', 'encode', 'advance', 'aggregation', 'complaint', 'doesnt', 'simulate', 'factorization', 'chicago', 'formulas', 'labor', 'recompute', 'significance', 'deviation', 'deviation', 'understanding', 'managing', 'treatment', 'council', 'advertisement', 'chasing', 'passage', 'study', 'abstract', 'robust', 'command', 'columns', 'experience', 'band', 'advertising', 'contest', 'pierre', 'index', 'generational', 'invert', 'viewpoint', 'location', 'cetera', 'correspond', 'intelligence', 'backbone', 'whats', 'start', 'rita', 'recalculation', 'punching', 'access', 'attention', 'decision', 'question', 'image', 'efficient', 'scoring', 'quarter', 'modification', 'characterization', 'space', 'contexts', 'scenario', 'okapi', 'contest', 'ideal', 'one', 'art', 'content', 'support', 'channel', 'likelihood_function', 'section', 'absence', 'kit', 'scroll', 'matrix', 'action', 'explain', 'scaling', 'room', 'item', 'observation', 'surfer', 'horse', 'mean', 'multiple', 'tolerance', 'separating', 'cuss', 'gain', 'eats', 'minus', 'activity', 'finding', 'lister', 'simulate', 'coffee', 'corresponding', 'curve', 'pretend', 'accumulator', 'document', 'reproduction', 'ignore', 'anticipate', 'text', 'nice', 'promotion', 'pure', 'relevant', 'analyst', 'proper', 'plot', 'help', 'overview', 'ideal', 'england', 'croft', 'right', 'optimal', 'find', 'check', 'air', 'committee', 'floor', 'quit', 'situation', 'air', 'week', 'build', 'jumping', 'mean', 'trick', 'guarantee', 'english', 'examine', 'reference', 'correct', 'functioning', 'block', 'realist', 'cleanness', 'england', 'cache', 'let', 'family', 'essence', 'browse', 'feel', 'author', 'checking', 'faction', 'actor', 'trade', 'zoom', 'chapter', 'hold', 'recall', 'lea', 'rock', 'maximum', 'algorithm', 'index', 'fast', 'label', 'zoom', 'talk', 'generational', 'price', 'analyst', 'express', 'exclusion', 'put', 'making', 'highway', 'compensate', 'shallow', 'spending', 'robustness', 'impose', 'impact', 'concern', 'ring']\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated_terms = indices_to_words(model.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6414,  1.5462, -0.1138, -0.1755,  0.1906,  0.4271,  0.3570,  2.2193,\n",
      "         0.1779, -1.0558])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn((4, 8, 10))\n",
    "B, T, C = input.shape\n",
    "output = input.view(B*T, C)\n",
    "# print(output)\n",
    "print(output[:][-1][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU(inplace=True)\n",
      "tensor([0.4875])\n",
      "tensor([-0.0500])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-0.05], dtype=torch.float32)\n",
    "y = nn.ReLU(x)\n",
    "print(y)\n",
    "\n",
    "x = torch.tensor([-0.05], dtype=torch.float32)\n",
    "y = torch.sigmoid(x)\n",
    "print(y)\n",
    "\n",
    "x = torch.tensor([-0.05], dtype=torch.float32)\n",
    "y = torch.tanh(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
