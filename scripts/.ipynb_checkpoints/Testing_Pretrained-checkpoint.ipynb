{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_0pT6I1ywYww",
    "outputId": "4e8bc5d7-b2a8-4e4a-87e5-d1ef14806843"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hZyz2edKwa_a",
    "outputId": "d24fa1b3-a761-495c-c822-13f9cc8b35ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\chris\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\chris\\anaconda3\\lib\\site-packages (4.36.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.9.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\chris\\anaconda3\\lib\\site-packages (4.36.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers[torch]) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers[torch]) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers[torch]) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers[torch]) (4.64.1)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.10 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.1.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\chris\\anaconda3\\lib\\site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers[torch]) (3.0.9)\n",
      "Requirement already satisfied: sympy in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.10.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.11.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\chris\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2022.9.14)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Cwj0Ro1UwchD",
    "outputId": "e09ab92d-46a6-426a-9bf2-15276e27157f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week Number</th>\n",
       "      <th>Lesson Number</th>\n",
       "      <th>Lesson Title</th>\n",
       "      <th>Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Natural Language Content Analysis</td>\n",
       "      <td>This lecture is about Natural Language of Cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Access</td>\n",
       "      <td>In this lecture,\\r\\nwe're going to talk about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Retrieval Problem</td>\n",
       "      <td>This lecture is about\\r\\nthe text retrieval pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overview of Text Retrieval Methods</td>\n",
       "      <td>This lecture is a overview of\\r\\ntext retrieva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Vector Space Model - Basic Idea</td>\n",
       "      <td>This lecture is about the\\r\\nvector space retr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Week Number  Lesson Number                        Lesson Title  \\\n",
       "0            1              1   Natural Language Content Analysis   \n",
       "1            1              2                         Text Access   \n",
       "2            1              3              Text Retrieval Problem   \n",
       "3            1              4  Overview of Text Retrieval Methods   \n",
       "4            1              5     Vector Space Model - Basic Idea   \n",
       "\n",
       "                                          Transcript  \n",
       "0  This lecture is about Natural Language of Cont...  \n",
       "1  In this lecture,\\r\\nwe're going to talk about ...  \n",
       "2  This lecture is about\\r\\nthe text retrieval pr...  \n",
       "3  This lecture is a overview of\\r\\ntext retrieva...  \n",
       "4  This lecture is about the\\r\\nvector space retr...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Directory path\n",
    "#CHRIS directory_path = '/content/drive/MyDrive/all_lectures.csv'\n",
    "directory_path = '../data/all_lectures.csv'\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "df = pd.DataFrame(columns=['Week Number', 'Lesson Number', 'Lesson Title', 'Transcript'])\n",
    "\n",
    "# Read in csv to dataframe\n",
    "df = pd.read_csv(directory_path)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1LsPljrweuU",
    "outputId": "aeecbef7-cea4-4402-9de5-b1103308bdbc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# clean up words in dataset -- this includes removing stopwords\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, words, brown\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# initialize dictionary\n",
    "global_dictionary  = set(words.words()) | set(brown.words())\n",
    "global_dictionary = {word.lower() for word in global_dictionary}\n",
    "remove_words = list(stop_words) # might need to use word_tokenize\n",
    "remove_words.extend(['Play', 'video', 'starting', 'at', '::', 'follow', 'transcript', 'natural', 'language', 'lecture', 'processing']) # remove the common words that are included in transcript\n",
    "\n",
    "# Now start actually cleaning the text\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase\n",
    "    text = text.replace('\\n', ' ') # remove newline indicator\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # case\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text) # website\n",
    "    text = re.sub(r'(\\b\\w+\\b)(?: \\1)+', r'\\1', text) # remove duplicate next word after space\n",
    "    text = re.sub(r'\\b(?![aI]\\b)\\w\\b', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Remove stopwords and only keep words in dictionary\n",
    "def remove_terms(text):\n",
    "    text = clean_text(text)\n",
    "    words = text.split()\n",
    "    # filtered_words = [word for word in words if word not in remove_words] # remove stopwords\n",
    "    filtered_words = [word for word in words if word in global_dictionary] # remove if not in global dictionary\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Tokenize reviews + remove stop words + filter only nouns\n",
    "def tokenize_and_filter(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word.lower() for word in words] # if word.lower() not in stop_words and len(word) > 2]\n",
    "    # print(words)\n",
    "    # pos_comment = nltk.pos_tag(words)\n",
    "    # filtered = [word[0] for word in pos_comment if word[1] in ['NN']]\n",
    "    return words #filtered\n",
    "\n",
    "def lower_text(text):\n",
    "    words = text.lower()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yl1ADwEW6BNg"
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 500\n",
    "\n",
    "df['Transcript_Cleaned'] = df['Transcript'].apply(remove_terms)\n",
    "df['Transcript_Cleaned'] = df['Transcript_Cleaned'].apply(lower_text)\n",
    "# Skipping this in order to tokenize later\n",
    "# df['Transcript_Cleaned'] = df['Transcript_Cleaned'].map(tokenize_and_filter)\n",
    "# df['Transcript_Cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g09bRJPg9gqI"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "model_choice = \"gpt2\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_choice)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # added padding token\n",
    "\n",
    "class CustomGPT2Model(GPT2LMHeadModel):\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = super().forward(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        # Extract the loss from the outputs if labels are provided\n",
    "        loss = outputs.loss if labels is not None else None\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": outputs.logits}\n",
    "\n",
    "# Replace \"gpt2\" with your desired model configuration\n",
    "config = GPT2Config.from_pretrained(model_choice)\n",
    "\n",
    "# Instantiate your custom GPT-2 model\n",
    "model = CustomGPT2Model(config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# modify data to be used as a list\n",
    "data = list(df['Transcript_Cleaned'])\n",
    "\n",
    "# create tokenizer function to allow arguemnts in apply method to df columns\n",
    "def tokenize_text(tokens):\n",
    "    return tokenizer(tokens, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# df['Tokenized_Text'] = df['Transcript_Cleaned'].apply(lambda x: tokenizer.encode(\" \".join(x), return_tensors=\"pt\"))\n",
    "train, val = train_test_split(data, test_size=0.2)\n",
    "train_tokenized = tokenizer(train, padding=True, truncation=True, max_length=512)\n",
    "val_tokenized = tokenizer(val, padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7HhoDjg8vv8B"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "        self.labels = [ids[:-1] for ids in encodings[\"input_ids\"]]  # Shift labels by one position\n",
    "        self.labels = [torch.tensor(ids + [0]) for ids in self.labels]  # Set the last token to 0 or another appropriate value\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ct2V0qmRv0Er"
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_tokenized)\n",
    "val_dataset = Dataset(val_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "hM7GM9ekvz8Y"
   },
   "outputs": [],
   "source": [
    "# Define Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10,\n",
    "    gradient_accumulation_steps=6,\n",
    "    learning_rate=1e-5,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,  # Set an appropriate evaluation frequency\n",
    "    save_total_limit=2,  # Adjust as needed\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"perplexity\",\n",
    "    greater_is_better=False,  # Lower perplexity is better\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "uAmonF0Avz53",
    "outputId": "8870fc41-91a3-4118-dfcc-378fb6fe41ba"
   },
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-63a5f79fedf1>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item[\"labels\"] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/30 50:03 < 13:39, 0.01 it/s, Epoch 3.63/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "metadata": {
      "tags": null
     },
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-63a5f79fedf1>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item[\"labels\"] = torch.tensor(self.labels[idx])\n",
      "<ipython-input-17-63a5f79fedf1>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item[\"labels\"] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 1:05:23, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=9.493714396158854, metrics={'train_runtime': 4095.8194, 'train_samples_per_second': 0.093, 'train_steps_per_second': 0.007, 'total_flos': 94065131520000.0, 'train_loss': 9.493714396158854, 'epoch': 4.74})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "f3yiLlSDxzgx",
    "outputId": "1fece71e-f3ed-433a-8c94-7c6f791aeec5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-63a5f79fedf1>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item[\"labels\"] = torch.tensor(self.labels[idx])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 9.17955493927002,\n",
       " 'eval_runtime': 70.4454,\n",
       " 'eval_samples_per_second': 0.284,\n",
       " 'eval_steps_per_second': 0.043,\n",
       " 'epoch': 4.74}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmjD6fbEx0uj"
   },
   "outputs": [],
   "source": [
    "trainer.save_model('/content/drive/MyDrive/Finetuned_Model_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ix-84VnIHY4H",
    "outputId": "b8bb176c-4456-4c43-8a66-14b67143e38d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\"'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjKvI7l8x0xK"
   },
   "outputs": [],
   "source": [
    "text = \"The best ways to retrieve text are\"\n",
    "inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "outputs = model(**inputs)\n",
    "# print(outputs)\n",
    "\n",
    "# Access the logits from the dictionary\n",
    "logits = outputs[\"logits\"]\n",
    "# print(logits)\n",
    "\n",
    "# Apply softmax to the logits\n",
    "# predictions = torch.nn.functional.softmax(logits, dim=-1)\n",
    "temperature = 0.85\n",
    "predictions = torch.nn.functional.softmax(logits / temperature, dim=-1)\n",
    "# print(predictions)\n",
    "\n",
    "# Convert predictions to NumPy array\n",
    "predictions = predictions.cpu().detach().numpy()\n",
    "print(predictions)\n",
    "\n",
    "# Decode predictions\n",
    "predicted_token_ids = torch.argmax(torch.from_numpy(predictions), dim=-1)\n",
    "print(predicted_token_ids)\n",
    "\n",
    "# Convert tensor to a Python list\n",
    "predicted_token_ids = predicted_token_ids.tolist()\n",
    "\n",
    "# Decode token IDs to words\n",
    "predicted_tokens = [tokenizer.decode(ids) for ids in predicted_token_ids[0]]\n",
    "print(predicted_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1JnIPPw-_sd9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
