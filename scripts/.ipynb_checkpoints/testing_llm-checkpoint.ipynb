{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "obo0zjkwWjiN",
    "outputId": "ed3a5a43-065f-42a3-a528-ee9a8fb45a15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N1o9Mbd-0eHb",
    "outputId": "28ba2591-0116-4172-ef2e-bb3bf5bc04ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
      "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
      "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.25.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "p6bE_XZvWOeM"
   },
   "outputs": [],
   "source": [
    "# # PyTorch clear gpu cache\n",
    "# device = torch.cuda\n",
    "# del device\n",
    "# torch.cuda.empty_cache() # unsure if it really works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "aCdyYEAtWOcL",
    "outputId": "bd4bb135-b719-4327-a397-11b176e4c8fb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week Number</th>\n",
       "      <th>Lesson Number</th>\n",
       "      <th>Lesson Title</th>\n",
       "      <th>Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Natural Language Content Analysis</td>\n",
       "      <td>This lecture is about Natural Language of Cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Text Access</td>\n",
       "      <td>In this lecture,\\r\\nwe're going to talk about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Text Retrieval Problem</td>\n",
       "      <td>This lecture is about\\r\\nthe text retrieval pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Overview of Text Retrieval Methods</td>\n",
       "      <td>This lecture is a overview of\\r\\ntext retrieva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Vector Space Model - Basic Idea</td>\n",
       "      <td>This lecture is about the\\r\\nvector space retr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Week Number  Lesson Number                        Lesson Title  \\\n",
       "0            1              1   Natural Language Content Analysis   \n",
       "1            1              2                         Text Access   \n",
       "2            1              3              Text Retrieval Problem   \n",
       "3            1              4  Overview of Text Retrieval Methods   \n",
       "4            1              5     Vector Space Model - Basic Idea   \n",
       "\n",
       "                                          Transcript  \n",
       "0  This lecture is about Natural Language of Cont...  \n",
       "1  In this lecture,\\r\\nwe're going to talk about ...  \n",
       "2  This lecture is about\\r\\nthe text retrieval pr...  \n",
       "3  This lecture is a overview of\\r\\ntext retrieva...  \n",
       "4  This lecture is about the\\r\\nvector space retr...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Directory path\n",
    "directory_path = '/content/drive/MyDrive/all_lectures.csv'\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "df = pd.DataFrame(columns=['Week Number', 'Lesson Number', 'Lesson Title', 'Transcript'])\n",
    "\n",
    "# Read in csv to dataframe\n",
    "df = pd.read_csv(directory_path)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "76EyM65xWOce"
   },
   "outputs": [],
   "source": [
    "# # for testing a sample dataframe\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# # Testing directory path\n",
    "# directory_path = 'C:\\\\Users\\\\azaan\\\\OneDrive\\\\Documents\\\\GitHub\\\\cs410_LLM_project\\\\sample_data\\\\module_7_sample.csv'\n",
    "\n",
    "# # Initialize an empty DataFrame\n",
    "# df = pd.DataFrame(columns=['Week Number', 'Lesson Number', 'Lesson Title', 'Transcript'])\n",
    "\n",
    "# # Read in csv to dataframe\n",
    "# df = pd.read_csv(directory_path)\n",
    "\n",
    "# # Display the resulting DataFrame\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sNoeF8zWOcg",
    "outputId": "27492b70-667c-4bec-ad6b-169b977eeefc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# clean up words in dataset -- this includes removing stopwords\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, words, brown\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "lemmer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# initialize dictionary\n",
    "global_dictionary  = set(words.words()) | set(brown.words())\n",
    "global_dictionary = {word.lower() for word in global_dictionary}\n",
    "remove_words = list(stop_words) # might need to use word_tokenize\n",
    "remove_words.extend(['Play', 'video', 'starting', 'at', '::', 'follow', 'transcript', 'natural', 'language', 'lecture', 'processing']) # remove the common words that are included in transcript\n",
    "\n",
    "# Now start actually cleaning the text\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase\n",
    "    text = text.replace('\\n', ' ') # remove newline indicator\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # case\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text) # website\n",
    "    text = re.sub(r'(\\b\\w+\\b)(?: \\1)+', r'\\1', text) # remove duplicate next word after space\n",
    "    text = re.sub(r'\\b(?![aI]\\b)\\w\\b', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Remove stopwords and only keep words in dictionary\n",
    "def remove_terms(text):\n",
    "    text = clean_text(text)\n",
    "    words = text.split()\n",
    "    # filtered_words = [word for word in words if word not in remove_words] # remove stopwords\n",
    "    filtered_words = [word for word in words if word in global_dictionary] # remove if not in global dictionary\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "df['Transcript_Cleaned'] = df['Transcript'].apply(remove_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "TnCCvV8bWOco",
    "outputId": "a0fc1395-9b95-4e69-a7c5-352687645eae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this lecture is about natural language of content analysis as you see from this picture this is really the first step to process any text data text data are in natural languages so computers have to understand natural languages to some extent in order to make use of the data so thats the topic of this lecture were going to cover three things first what is natural language processing which is the main technique for processing natural language to obtain understanding the second is the state of the art of which stands for natural language processing finally were going to cover the relation between natural language processing and text retrieval first what is well the best way to explain it is to think about if you see a text in a foreign language that you can understand now what do you have to do in order to understand that text this is basically what computers are facing so looking at the simple sentence like a dog is chasing a boy on the playground we dont have any problems understanding this sentence but imagine what the computer would have to do in order to understand it well in general it would have to do the following first it would have to know dog is a noun a verb so this is called lexical analysis or tagging and we need to figure out the syntactic categories of those words so thats the first step after that were going to figure out the structure of the sentence so for example here it shows that and the dog would go together to form a noun phrase and we wont have dog and is to go first and there are some structures that are not just right but this structure shows what we might get if we look at the sentence and try to interpret the sentence some words would go together first and then they will go together with other words so here we show we have noun phrases as intermediate components and then verbal phrases finally we have a sentence and you get this structure we need to do something called a semantic analysis or and we may have a parser accompanying the program and that would automatically created this structure at this point you would know the structure of this sentence but still you dont know the meaning of the sentence so we have to go further to semantic analysis in our mind we usually can map such a sentence to what we already know in our knowledge base for example you might imagine a dog that looks like that theres a boy and theres some activity here but for a computer would have to use symbols to denote that wed use a symbol to denote a dog and can denote a boy and then can denote a playground now there is also a chasing activity thats happening here so we have a relationship chasing that connects all these symbols so this is how a computer would obtain some understanding of this sentence now from this representation we could also further infer some other things and we might indeed naturally think of something else when we read a text and this is called inference so for example if you believe that if being chased and this person might be scared but with this rule you can see computers could also infer that this boy maybe scared so this is some extra knowledge that youd infer based on some understanding of the text you can even go further to understand why the person say at this sentence so this has to do as a use of language this is called pragmatic analysis in order to understand the speak actor of a sentence right we say something to basically achieve some goal theres some purpose there and this has to do with the use of language in this case the person who said this sentence might be reminding another person to bring back the dog that could be one possible intent to reach this level of understanding would require all of these steps and a computer would have to go through all these steps in order to completely understand this sentence yet we humans have no trouble with understanding that we instantly would get everything there is a reason for thats because we have a large knowledge base in our brain and we can use common sense knowledge to help interpret the sentence computers unfortunately are hard to obtain such understanding they dont have such a knowledge base they are still incapable of doing reasoning and uncertainties so that makes natural language processing difficult for computers but the fundamental reason why natural language processing is difficult for computers is simply because natural language has not been designed for computers natural languages are designed for us to communicate there are other languages designed for computers for example programming languages those are harder for us right so natural languages is designed to make our communication efficient as a result we omit a lot of common sense knowledge because we assume everyone knows about that we also keep a lot of ambiguities because we assume the receiver or the hearer could know how to decipher an ambiguous word based on the knowledge or the context theres no need to demand different words for different meanings we could overload the same word with different meanings without the problem because of these reasons this makes every step in natural language of processing difficult for computers ambiguity is the main difficulty and common sense and reasoning is often required thats also hard so let me give you some examples of challenges here consider the word level ambiguity the same word can have different syntactic categories for example design can be a noun or a verb the word of root may have multiple meanings so square root in math sense or the root of a plant you might be able to think about its meanings there are also syntactical ambiguities for example the main topic of this lecture natural language processing can actually be interpreted in two ways in terms of the structure think for a moment and see if you can figure that out we usually think of this as processing of natural language but you could also think of this as do say language processing is natural so this is an example of synaptic ambiguity what we have different is structures that can be applied to the same sequence of words another common example of an ambiguous sentence is the following a man saw a boy with a telescope now in this case the question is who had a telescope this is called a prepositional phrase attachment ambiguity or attachment ambiguity now we generally dont have a problem with these ambiguities because we have a lot of background knowledge to help us the ambiguity another example of difficulty is anaphora resolution so think about the sentence john persuaded bill to buy a tv for himself the question here is does himself refer to john or bill so again this is something that you have to use some background or the context to figure out finally presupposition is another problem consider the sentence he has quit smoking now this obviously implies that he smoked before so imagine a computer wants to understand all these subtle differences and meanings it would have to use a lot of knowledge to figure that out it also would have to maintain a large knowledge base of all the meanings of words and how they are connected to our common sense knowledge of the world so this is why its very difficult so as a result we are steep not perfect in fact far from perfect in understanding natural language using computers so this slide sort of gains a simplified view of state of the art we can do part of speech tagging pretty well so showed accuracy here now this number is obviously based on a certain so dont take this literally this just shows that we can do it pretty well but its still not perfect in terms of we can do partial pretty well that means we can get noun phrase structures or verb phrase structure or some segment of the sentence and this dude correct them in terms of the structure and in some evaluation results we have seen above accuracy in terms of partial of sentences again have to say these numbers are relative to the in some other the numbers might be lower most of the existing work has been evaluated using news and so a lot of these numbers are more or less toward news data think about social media data the accuracy likely is lower in terms of a semantical analysis we are far from being able to do a complete understanding of a sentence but we have some techniques that would allow us to do partial understanding of the sentence so could mention some of them for example we have techniques that can allow us to extract the entities and relations mentioned in text articles for example recognizing dimensions of people locations organizations in text so this is called entity extraction we may be able to recognize the relations for example this person visited that place or this person met that person or this company acquired another company such relations can be extracted by using the computer current natural language processing techniques theyre not perfect but they can do well for some entities some entities are harder than others we can also do word sense disintegration to some extend we have to figure out whether this word in this sentence would have certain meaning in another context the computer could figure out it has a different meaning again its not perfect but you can do something in that direction we can also do sentiment analysis meaning to figure out whether a sentence is positive or negative this is especially useful for review analysis for example so these are examples of semantic analysis and they help us to obtain partial understanding of the sentences its not giving us a complete understanding as showed it before for this sentence but it would still help us gain understanding of the content and these can be useful in terms of inference we are not there yet probably because of the general difficulty of inference and uncertainties this is a general challenge in artificial intelligence now thats probably also because we dont have complete semantical representation for natural inaudible text so this is hard yet in some domains perhaps in limited domains when you have a lot of restrictions on the word uses you may be able to perform inference to some extent but in general we can not really do that reliably speech act analysis is also far from being done and we can only do that analysis for very special cases so this roughly gives you some idea about the state of the art and then we also talk a little bit about what we cant do and so we cant even do part of speech tagging now this looks like a simple task but think about the example here the two uses off may have different syntactic categories if you try to make a fine grained distinctions its not that easy to figure out such differences its also hard to do general complete and again the same sentence that you saw before is example this ambiguity can be very hard to and you can imagine example where you have to use a lot of knowledge in the context of the sentence or from the background in order to figure out who actually had the telescope so although the sentence looks very simple it actually is pretty hard and in cases when the sentence is very long imagine it has four or five prepositional phrases and there are even more possibilities to figure out its also harder to do precise deep semantic analysis so an example in the sentence john owns a restaurant how do we define owns exactly the word own it is something that we can understand but its very hard to precisely describe the meaning of own for computers so as a result we have a robust and a general natural language processing techniques that can process a lot of text data in a shallow way meaning we only do superficial analysis for example parts of speech tagging or a partial or recognizing sentiment and those are not deep understanding because were not really understanding the exact meaning of the sentence on the other hand of the deep understanding techniques tend not to scale up well meaning that they would fill only some restricted text and if you dont restrict the text domain or the use of words then these techniques tend not to work well they may work well based on machine learning techniques on the data that are similar to the training data that the program has been trained on but they generally wouldnt work well on the data that are very different from the training data so this pretty much summarizes the state of the art of natural language processing of course within such a short amount of time we cant really give you a complete view of which is a big field and id expect to see multiple courses on natural language processing topic itself but because of its relevance to the topic that we talk about its useful for you to know the background in case you happen to be exposed to that so what does that mean for text retrieval well in text retrieval we are dealing with all kinds of text its very hard to restrict text to a certain domain and we also are often dealing with a lot of text data so that means the techniques must be general robust and efficient and that just implies today we can only use fairly shallow techniques for text retrieval in fact most search engines today use something called a bag of words representation now this is probably the simplest representation you can possibly think of that is to turn text data into simply a bag of words meaning well keep individual words but well ignore all the orders of words and well keep duplicated occurrences of words so this is called a bag of words representation when you represent text in this way you ignore a lot of valid information that just makes it harder to understand the exact meaning of a sentence because weve lost the order but yet this representation tends to actually work pretty well for most search tasks and this was partly because the search task is not all that difficult if you see matching of some of the query words in a text document chances are that document is about the topic although there are exceptions so in comparison of some other tasks for example machine translation would require you to understand the language accurately otherwise the translation would be wrong so in comparison such tasks are all relatively easy such a representation is often sufficient and thats also the representation that the major search engines today like a or bing are using of course put in parentheses but not all of course there are many queries that are not answered well by the current search engines and they do require the replantation that would go beyond bag of words replantation that would require more natural language processing to be done there was another reason why we have not used the sophisticated techniques in modern search engines and thats because some retrieval techniques actually naturally solved the problem of so one example is word sense disintegration think about a word like java it could mean coffee or it could mean program language if you look at the word it would be ambiguous but when the user uses the word in the query usually there are other words for example looking for usage of java when have there that implies java means program language and that contest can help us naturally prefer documents which java is referring to program languages because those documents would probably match as well if java occurs in that documents where it means coffee then you would never match or with very small probability so this is the case when some retrieval techniques naturally achieve the goal of word another example is some technique called feedback which we will talk about later in some of the lectures this technique would allow us to additional words to the query and those additional words could be related to the query words and these words can help matching documents where the original query words have not occurred so this achieves to some extent semantic matching of terms so those techniques also helped us bypass some of the difficulties in natural language processing however in the long run we still need a deeper natural language processing techniques in order to improve the accuracy of the current search engines and its particularly needed for complex search tasks or for question and answering has recently launched a knowledge graph and this is one step toward that goal because knowledge graph would contain entities and their relations and this goes beyond the simple bag of words replantation and such technique should help us improve the search engine utility significantly although this is the open topic for research and exploration in sum in this lecture we talked about what is and weve talked about the state of that techniques what we can do what we cannot do and finally we also explain why the bag of words replantation remains the dominant replantation used in modern search engines even though deeper would be needed for future search engines if you want to know more you can take a look at some additional readings only cited one here and thats a good starting point thanks'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Transcript_Cleaned'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "syXKS19ZWOcp"
   },
   "outputs": [],
   "source": [
    "# # Create bigrams and trigrams from data\n",
    "\n",
    "# # Function to filter bigrams or trigrams\n",
    "# def ngram_filter(ngram):\n",
    "#     tags = nltk.pos_tag(ngram)\n",
    "#     if not all(tag[1] in ['JJ', 'NN'] for tag in tags):\n",
    "#         return False\n",
    "#     if any(word in stop_words for word in ngram):\n",
    "#         return False\n",
    "#     if 'n' in ngram or 't' in ngram:\n",
    "#         return False\n",
    "#     if 'PRON' in ngram:\n",
    "#         return False\n",
    "#     return True\n",
    "\n",
    "# # Function to find top ngrams\n",
    "# def find_top_ngrams(texts, ngram_measures, min_freq=50, min_pmi=5, top_k=100):\n",
    "#     finder = nltk.collocations.BigramCollocationFinder.from_documents(texts)\n",
    "#     finder.apply_freq_filter(min_freq)\n",
    "#     ngram_scores = finder.score_ngrams(ngram_measures.pmi)\n",
    "#     filtered_ngrams = [ngram for ngram, pmi in ngram_scores if ngram_filter(ngram) and pmi > min_pmi]\n",
    "#     return [' '.join(ngram) for ngram in filtered_ngrams][:top_k]\n",
    "\n",
    "# bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "# bigrams = find_top_ngrams([text.split() for text in df['Transcript_Cleaned']], bigram_measures)\n",
    "# trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "# trigrams = find_top_ngrams([text.split() for text in df['Transcript_Cleaned']], trigram_measures)\n",
    "\n",
    "# # Function to replace ngrams in text\n",
    "# def replace_ngrams(text):\n",
    "#     for gram in trigrams:\n",
    "#         text = text.replace(gram, '_'.join(gram.split()))\n",
    "#     for gram in bigrams:\n",
    "#         text = text.replace(gram, '_'.join(gram.split()))\n",
    "#     return text\n",
    "\n",
    "# # Apply ngram replacements to the text\n",
    "# df['Grams'] = df['Transcript_Cleaned'].map(replace_ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Tkc8EztMWOcs"
   },
   "outputs": [],
   "source": [
    "# Tokenize reviews + remove stop words + filter only nouns\n",
    "def tokenize_and_filter(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [word.lower() for word in words] # if word.lower() not in stop_words and len(word) > 2]\n",
    "    # print(words)\n",
    "    # pos_comment = nltk.pos_tag(words)\n",
    "    # filtered = [word[0] for word in pos_comment if word[1] in ['NN']]\n",
    "    return words #filtered\n",
    "\n",
    "# If using transcript instead of grams\n",
    "df['Transcript_Cleaned'] = df['Transcript_Cleaned'].map(tokenize_and_filter)\n",
    "\n",
    "# If using Grams instead of transcript\n",
    "# df['Grams'] = df['Grams'].map(tokenize_and_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JHNVhuRrWOcx",
    "outputId": "a20ef47b-9143-4d4a-c4ea-0aa9d9b763e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'lecture',\n",
       " 'is',\n",
       " 'about',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'of',\n",
       " 'content',\n",
       " 'analysis',\n",
       " 'as',\n",
       " 'you',\n",
       " 'see',\n",
       " 'from',\n",
       " 'this',\n",
       " 'picture',\n",
       " 'this',\n",
       " 'is',\n",
       " 'really',\n",
       " 'the',\n",
       " 'first',\n",
       " 'step',\n",
       " 'to',\n",
       " 'process',\n",
       " 'any',\n",
       " 'text',\n",
       " 'data',\n",
       " 'text',\n",
       " 'data',\n",
       " 'are',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'languages',\n",
       " 'so',\n",
       " 'computers',\n",
       " 'have',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'natural',\n",
       " 'languages',\n",
       " 'to',\n",
       " 'some',\n",
       " 'extent',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'make',\n",
       " 'use',\n",
       " 'of',\n",
       " 'the',\n",
       " 'data',\n",
       " 'so',\n",
       " 'thats',\n",
       " 'the',\n",
       " 'topic',\n",
       " 'of',\n",
       " 'this',\n",
       " 'lecture',\n",
       " 'were',\n",
       " 'going',\n",
       " 'to',\n",
       " 'cover',\n",
       " 'three',\n",
       " 'things',\n",
       " 'first',\n",
       " 'what',\n",
       " 'is',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'which',\n",
       " 'is',\n",
       " 'the',\n",
       " 'main',\n",
       " 'technique',\n",
       " 'for',\n",
       " 'processing',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'to',\n",
       " 'obtain',\n",
       " 'understanding',\n",
       " 'the',\n",
       " 'second',\n",
       " 'is',\n",
       " 'the',\n",
       " 'state',\n",
       " 'of',\n",
       " 'the',\n",
       " 'art',\n",
       " 'of',\n",
       " 'which',\n",
       " 'stands',\n",
       " 'for',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'finally',\n",
       " 'were',\n",
       " 'going',\n",
       " 'to',\n",
       " 'cover',\n",
       " 'the',\n",
       " 'relation',\n",
       " 'between',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'and',\n",
       " 'text',\n",
       " 'retrieval',\n",
       " 'first',\n",
       " 'what',\n",
       " 'is',\n",
       " 'well',\n",
       " 'the',\n",
       " 'best',\n",
       " 'way',\n",
       " 'to',\n",
       " 'explain',\n",
       " 'it',\n",
       " 'is',\n",
       " 'to',\n",
       " 'think',\n",
       " 'about',\n",
       " 'if',\n",
       " 'you',\n",
       " 'see',\n",
       " 'a',\n",
       " 'text',\n",
       " 'in',\n",
       " 'a',\n",
       " 'foreign',\n",
       " 'language',\n",
       " 'that',\n",
       " 'you',\n",
       " 'can',\n",
       " 'understand',\n",
       " 'now',\n",
       " 'what',\n",
       " 'do',\n",
       " 'you',\n",
       " 'have',\n",
       " 'to',\n",
       " 'do',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'that',\n",
       " 'text',\n",
       " 'this',\n",
       " 'is',\n",
       " 'basically',\n",
       " 'what',\n",
       " 'computers',\n",
       " 'are',\n",
       " 'facing',\n",
       " 'so',\n",
       " 'looking',\n",
       " 'at',\n",
       " 'the',\n",
       " 'simple',\n",
       " 'sentence',\n",
       " 'like',\n",
       " 'a',\n",
       " 'dog',\n",
       " 'is',\n",
       " 'chasing',\n",
       " 'a',\n",
       " 'boy',\n",
       " 'on',\n",
       " 'the',\n",
       " 'playground',\n",
       " 'we',\n",
       " 'dont',\n",
       " 'have',\n",
       " 'any',\n",
       " 'problems',\n",
       " 'understanding',\n",
       " 'this',\n",
       " 'sentence',\n",
       " 'but',\n",
       " 'imagine',\n",
       " 'what',\n",
       " 'the',\n",
       " 'computer',\n",
       " 'would',\n",
       " 'have',\n",
       " 'to',\n",
       " 'do',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'it',\n",
       " 'well',\n",
       " 'in',\n",
       " 'general',\n",
       " 'it',\n",
       " 'would',\n",
       " 'have',\n",
       " 'to',\n",
       " 'do',\n",
       " 'the',\n",
       " 'following',\n",
       " 'first',\n",
       " 'it',\n",
       " 'would',\n",
       " 'have',\n",
       " 'to',\n",
       " 'know',\n",
       " 'dog',\n",
       " 'is',\n",
       " 'a',\n",
       " 'noun',\n",
       " 'a',\n",
       " 'verb',\n",
       " 'so',\n",
       " 'this',\n",
       " 'is',\n",
       " 'called',\n",
       " 'lexical',\n",
       " 'analysis',\n",
       " 'or',\n",
       " 'tagging',\n",
       " 'and',\n",
       " 'we',\n",
       " 'need',\n",
       " 'to',\n",
       " 'figure',\n",
       " 'out',\n",
       " 'the',\n",
       " 'syntactic',\n",
       " 'categories',\n",
       " 'of',\n",
       " 'those',\n",
       " 'words',\n",
       " 'so',\n",
       " 'thats',\n",
       " 'the',\n",
       " 'first',\n",
       " 'step',\n",
       " 'after',\n",
       " 'that',\n",
       " 'were',\n",
       " 'going',\n",
       " 'to',\n",
       " 'figure',\n",
       " 'out',\n",
       " 'the',\n",
       " 'structure',\n",
       " 'of',\n",
       " 'the',\n",
       " 'sentence',\n",
       " 'so',\n",
       " 'for',\n",
       " 'example',\n",
       " 'here',\n",
       " 'it',\n",
       " 'shows',\n",
       " 'that',\n",
       " 'and',\n",
       " 'the',\n",
       " 'dog',\n",
       " 'would',\n",
       " 'go',\n",
       " 'together',\n",
       " 'to',\n",
       " 'form',\n",
       " 'a',\n",
       " 'noun',\n",
       " 'phrase',\n",
       " 'and',\n",
       " 'we',\n",
       " 'wont',\n",
       " 'have',\n",
       " 'dog',\n",
       " 'and',\n",
       " 'is',\n",
       " 'to',\n",
       " 'go',\n",
       " 'first',\n",
       " 'and',\n",
       " 'there',\n",
       " 'are',\n",
       " 'some',\n",
       " 'structures',\n",
       " 'that',\n",
       " 'are',\n",
       " 'not',\n",
       " 'just',\n",
       " 'right',\n",
       " 'but',\n",
       " 'this',\n",
       " 'structure',\n",
       " 'shows',\n",
       " 'what',\n",
       " 'we',\n",
       " 'might',\n",
       " 'get',\n",
       " 'if',\n",
       " 'we',\n",
       " 'look',\n",
       " 'at',\n",
       " 'the',\n",
       " 'sentence',\n",
       " 'and',\n",
       " 'try',\n",
       " 'to',\n",
       " 'interpret',\n",
       " 'the',\n",
       " 'sentence',\n",
       " 'some',\n",
       " 'words',\n",
       " 'would',\n",
       " 'go',\n",
       " 'together',\n",
       " 'first',\n",
       " 'and',\n",
       " 'then',\n",
       " 'they',\n",
       " 'will',\n",
       " 'go',\n",
       " 'together',\n",
       " 'with',\n",
       " 'other',\n",
       " 'words',\n",
       " 'so',\n",
       " 'here',\n",
       " 'we',\n",
       " 'show',\n",
       " 'we',\n",
       " 'have',\n",
       " 'noun',\n",
       " 'phrases',\n",
       " 'as',\n",
       " 'intermediate',\n",
       " 'components',\n",
       " 'and',\n",
       " 'then',\n",
       " 'verbal',\n",
       " 'phrases',\n",
       " 'finally',\n",
       " 'we',\n",
       " 'have',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'and',\n",
       " 'you',\n",
       " 'get',\n",
       " 'this',\n",
       " 'structure',\n",
       " 'we',\n",
       " 'need',\n",
       " 'to',\n",
       " 'do',\n",
       " 'something',\n",
       " 'called',\n",
       " 'a',\n",
       " 'semantic',\n",
       " 'analysis',\n",
       " 'or',\n",
       " 'and',\n",
       " 'we',\n",
       " 'may',\n",
       " 'have',\n",
       " 'a',\n",
       " 'parser',\n",
       " 'accompanying',\n",
       " 'the',\n",
       " 'program',\n",
       " 'and',\n",
       " 'that',\n",
       " 'would',\n",
       " 'automatically',\n",
       " 'created',\n",
       " 'this',\n",
       " 'structure',\n",
       " 'at',\n",
       " 'this',\n",
       " 'point',\n",
       " 'you',\n",
       " 'would',\n",
       " 'know',\n",
       " 'the',\n",
       " 'structure',\n",
       " 'of',\n",
       " 'this',\n",
       " 'sentence',\n",
       " 'but',\n",
       " 'still',\n",
       " 'you',\n",
       " 'dont',\n",
       " 'know',\n",
       " 'the',\n",
       " 'meaning',\n",
       " 'of',\n",
       " 'the',\n",
       " 'sentence',\n",
       " 'so',\n",
       " 'we',\n",
       " 'have',\n",
       " 'to',\n",
       " 'go',\n",
       " 'further',\n",
       " 'to',\n",
       " 'semantic',\n",
       " 'analysis',\n",
       " 'in',\n",
       " 'our',\n",
       " 'mind',\n",
       " 'we',\n",
       " 'usually',\n",
       " 'can',\n",
       " 'map',\n",
       " 'such',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'to',\n",
       " 'what',\n",
       " 'we',\n",
       " 'already',\n",
       " 'know',\n",
       " 'in',\n",
       " 'our',\n",
       " 'knowledge',\n",
       " 'base',\n",
       " 'for',\n",
       " 'example',\n",
       " 'you',\n",
       " 'might',\n",
       " 'imagine',\n",
       " 'a',\n",
       " 'dog',\n",
       " 'that',\n",
       " 'looks',\n",
       " 'like',\n",
       " 'that',\n",
       " 'theres',\n",
       " 'a',\n",
       " 'boy',\n",
       " 'and',\n",
       " 'theres',\n",
       " 'some',\n",
       " 'activity',\n",
       " 'here',\n",
       " 'but',\n",
       " 'for',\n",
       " 'a',\n",
       " 'computer',\n",
       " 'would',\n",
       " 'have',\n",
       " 'to',\n",
       " 'use',\n",
       " 'symbols',\n",
       " 'to',\n",
       " 'denote',\n",
       " 'that',\n",
       " 'wed',\n",
       " 'use',\n",
       " 'a',\n",
       " 'symbol',\n",
       " 'to',\n",
       " 'denote',\n",
       " 'a',\n",
       " 'dog',\n",
       " 'and',\n",
       " 'can',\n",
       " 'denote',\n",
       " 'a',\n",
       " 'boy',\n",
       " 'and',\n",
       " 'then',\n",
       " 'can',\n",
       " 'denote',\n",
       " 'a',\n",
       " 'playground',\n",
       " 'now',\n",
       " 'there',\n",
       " 'is',\n",
       " 'also',\n",
       " 'a',\n",
       " 'chasing',\n",
       " 'activity',\n",
       " 'thats',\n",
       " 'happening',\n",
       " 'here',\n",
       " 'so',\n",
       " 'we',\n",
       " 'have',\n",
       " 'a',\n",
       " 'relationship',\n",
       " 'chasing',\n",
       " 'that',\n",
       " 'connects',\n",
       " 'all',\n",
       " 'these',\n",
       " 'symbols',\n",
       " 'so',\n",
       " 'this',\n",
       " 'is',\n",
       " 'how',\n",
       " 'a',\n",
       " 'computer',\n",
       " 'would',\n",
       " 'obtain',\n",
       " 'some',\n",
       " 'understanding',\n",
       " 'of',\n",
       " 'this',\n",
       " 'sentence',\n",
       " 'now',\n",
       " 'from',\n",
       " 'this',\n",
       " 'representation',\n",
       " 'we',\n",
       " 'could',\n",
       " 'also',\n",
       " 'further',\n",
       " 'infer',\n",
       " 'some',\n",
       " 'other',\n",
       " 'things',\n",
       " 'and',\n",
       " 'we',\n",
       " 'might',\n",
       " 'indeed',\n",
       " 'naturally',\n",
       " 'think',\n",
       " 'of',\n",
       " 'something',\n",
       " 'else',\n",
       " 'when',\n",
       " 'we',\n",
       " 'read',\n",
       " 'a',\n",
       " 'text',\n",
       " 'and',\n",
       " 'this',\n",
       " 'is',\n",
       " 'called',\n",
       " 'inference',\n",
       " 'so',\n",
       " 'for',\n",
       " 'example',\n",
       " 'if',\n",
       " 'you',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'if',\n",
       " 'being',\n",
       " 'chased',\n",
       " 'and',\n",
       " 'this',\n",
       " 'person',\n",
       " 'might',\n",
       " 'be',\n",
       " 'scared',\n",
       " 'but',\n",
       " 'with',\n",
       " 'this',\n",
       " 'rule',\n",
       " 'you',\n",
       " 'can',\n",
       " 'see',\n",
       " 'computers',\n",
       " 'could',\n",
       " 'also',\n",
       " 'infer',\n",
       " 'that',\n",
       " 'this',\n",
       " 'boy',\n",
       " 'maybe',\n",
       " 'scared',\n",
       " 'so',\n",
       " 'this',\n",
       " 'is',\n",
       " 'some',\n",
       " 'extra',\n",
       " 'knowledge',\n",
       " 'that',\n",
       " 'youd',\n",
       " 'infer',\n",
       " 'based',\n",
       " 'on',\n",
       " 'some',\n",
       " 'understanding',\n",
       " 'of',\n",
       " 'the',\n",
       " 'text',\n",
       " 'you',\n",
       " 'can',\n",
       " 'even',\n",
       " 'go',\n",
       " 'further',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'why',\n",
       " 'the',\n",
       " 'person',\n",
       " 'say',\n",
       " 'at',\n",
       " 'this',\n",
       " 'sentence',\n",
       " 'so',\n",
       " 'this',\n",
       " 'has',\n",
       " 'to',\n",
       " 'do',\n",
       " 'as',\n",
       " 'a',\n",
       " 'use',\n",
       " 'of',\n",
       " 'language',\n",
       " 'this',\n",
       " 'is',\n",
       " 'called',\n",
       " 'pragmatic',\n",
       " 'analysis',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'the',\n",
       " 'speak',\n",
       " 'actor',\n",
       " 'of',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'right',\n",
       " 'we',\n",
       " 'say',\n",
       " 'something',\n",
       " 'to',\n",
       " 'basically',\n",
       " 'achieve',\n",
       " 'some',\n",
       " 'goal',\n",
       " 'theres',\n",
       " 'some',\n",
       " 'purpose',\n",
       " 'there',\n",
       " 'and',\n",
       " 'this',\n",
       " 'has',\n",
       " 'to',\n",
       " 'do',\n",
       " 'with',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'language',\n",
       " 'in',\n",
       " 'this',\n",
       " 'case',\n",
       " 'the',\n",
       " 'person',\n",
       " 'who',\n",
       " 'said',\n",
       " 'this',\n",
       " 'sentence',\n",
       " 'might',\n",
       " 'be',\n",
       " 'reminding',\n",
       " 'another',\n",
       " 'person',\n",
       " 'to',\n",
       " 'bring',\n",
       " 'back',\n",
       " 'the',\n",
       " 'dog',\n",
       " 'that',\n",
       " 'could',\n",
       " 'be',\n",
       " 'one',\n",
       " 'possible',\n",
       " 'intent',\n",
       " 'to',\n",
       " 'reach',\n",
       " 'this',\n",
       " 'level',\n",
       " 'of',\n",
       " 'understanding',\n",
       " 'would',\n",
       " 'require',\n",
       " 'all',\n",
       " 'of',\n",
       " 'these',\n",
       " 'steps',\n",
       " 'and',\n",
       " 'a',\n",
       " 'computer',\n",
       " 'would',\n",
       " 'have',\n",
       " 'to',\n",
       " 'go',\n",
       " 'through',\n",
       " 'all',\n",
       " 'these',\n",
       " 'steps',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'completely',\n",
       " 'understand',\n",
       " 'this',\n",
       " 'sentence',\n",
       " 'yet',\n",
       " 'we',\n",
       " 'humans',\n",
       " 'have',\n",
       " 'no',\n",
       " 'trouble',\n",
       " 'with',\n",
       " 'understanding',\n",
       " 'that',\n",
       " 'we',\n",
       " 'instantly',\n",
       " 'would',\n",
       " 'get',\n",
       " 'everything',\n",
       " 'there',\n",
       " 'is',\n",
       " 'a',\n",
       " 'reason',\n",
       " 'for',\n",
       " 'thats',\n",
       " 'because',\n",
       " 'we',\n",
       " 'have',\n",
       " 'a',\n",
       " 'large',\n",
       " 'knowledge',\n",
       " 'base',\n",
       " 'in',\n",
       " 'our',\n",
       " 'brain',\n",
       " 'and',\n",
       " 'we',\n",
       " 'can',\n",
       " 'use',\n",
       " 'common',\n",
       " 'sense',\n",
       " 'knowledge',\n",
       " 'to',\n",
       " 'help',\n",
       " 'interpret',\n",
       " 'the',\n",
       " 'sentence',\n",
       " 'computers',\n",
       " 'unfortunately',\n",
       " 'are',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'obtain',\n",
       " 'such',\n",
       " 'understanding',\n",
       " 'they',\n",
       " 'dont',\n",
       " 'have',\n",
       " 'such',\n",
       " 'a',\n",
       " 'knowledge',\n",
       " 'base',\n",
       " 'they',\n",
       " 'are',\n",
       " 'still',\n",
       " 'incapable',\n",
       " 'of',\n",
       " 'doing',\n",
       " 'reasoning',\n",
       " 'and',\n",
       " 'uncertainties',\n",
       " 'so',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'difficult',\n",
       " 'for',\n",
       " 'computers',\n",
       " 'but',\n",
       " 'the',\n",
       " 'fundamental',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'is',\n",
       " 'difficult',\n",
       " 'for',\n",
       " 'computers',\n",
       " 'is',\n",
       " 'simply',\n",
       " 'because',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'has',\n",
       " 'not',\n",
       " 'been',\n",
       " 'designed',\n",
       " 'for',\n",
       " 'computers',\n",
       " 'natural',\n",
       " 'languages',\n",
       " 'are',\n",
       " 'designed',\n",
       " 'for',\n",
       " 'us',\n",
       " 'to',\n",
       " 'communicate',\n",
       " 'there',\n",
       " 'are',\n",
       " 'other',\n",
       " 'languages',\n",
       " 'designed',\n",
       " 'for',\n",
       " 'computers',\n",
       " 'for',\n",
       " 'example',\n",
       " 'programming',\n",
       " 'languages',\n",
       " 'those',\n",
       " 'are',\n",
       " 'harder',\n",
       " 'for',\n",
       " 'us',\n",
       " 'right',\n",
       " 'so',\n",
       " 'natural',\n",
       " 'languages',\n",
       " 'is',\n",
       " 'designed',\n",
       " 'to',\n",
       " 'make',\n",
       " 'our',\n",
       " 'communication',\n",
       " 'efficient',\n",
       " 'as',\n",
       " 'a',\n",
       " 'result',\n",
       " 'we',\n",
       " 'omit',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'common',\n",
       " 'sense',\n",
       " 'knowledge',\n",
       " 'because',\n",
       " 'we',\n",
       " 'assume',\n",
       " 'everyone',\n",
       " 'knows',\n",
       " 'about',\n",
       " 'that',\n",
       " 'we',\n",
       " 'also',\n",
       " 'keep',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'ambiguities',\n",
       " 'because',\n",
       " 'we',\n",
       " 'assume',\n",
       " 'the',\n",
       " 'receiver',\n",
       " 'or',\n",
       " 'the',\n",
       " 'hearer',\n",
       " 'could',\n",
       " 'know',\n",
       " 'how',\n",
       " 'to',\n",
       " 'decipher',\n",
       " 'an',\n",
       " 'ambiguous',\n",
       " 'word',\n",
       " 'based',\n",
       " 'on',\n",
       " 'the',\n",
       " 'knowledge',\n",
       " 'or',\n",
       " 'the',\n",
       " 'context',\n",
       " 'theres',\n",
       " 'no',\n",
       " 'need',\n",
       " 'to',\n",
       " 'demand',\n",
       " 'different',\n",
       " 'words',\n",
       " 'for',\n",
       " 'different',\n",
       " 'meanings',\n",
       " 'we',\n",
       " 'could',\n",
       " 'overload',\n",
       " 'the',\n",
       " 'same',\n",
       " 'word',\n",
       " 'with',\n",
       " 'different',\n",
       " 'meanings',\n",
       " 'without',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'because',\n",
       " 'of',\n",
       " 'these',\n",
       " 'reasons',\n",
       " 'this',\n",
       " 'makes',\n",
       " 'every',\n",
       " 'step',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'of',\n",
       " 'processing',\n",
       " 'difficult',\n",
       " 'for',\n",
       " 'computers',\n",
       " 'ambiguity',\n",
       " 'is',\n",
       " 'the',\n",
       " 'main',\n",
       " 'difficulty',\n",
       " 'and',\n",
       " 'common',\n",
       " 'sense',\n",
       " 'and',\n",
       " 'reasoning',\n",
       " 'is',\n",
       " 'often',\n",
       " 'required',\n",
       " 'thats',\n",
       " 'also',\n",
       " 'hard',\n",
       " 'so',\n",
       " 'let',\n",
       " 'me',\n",
       " 'give',\n",
       " 'you',\n",
       " 'some',\n",
       " 'examples',\n",
       " 'of',\n",
       " 'challenges',\n",
       " 'here',\n",
       " 'consider',\n",
       " 'the',\n",
       " 'word',\n",
       " 'level',\n",
       " 'ambiguity',\n",
       " 'the',\n",
       " 'same',\n",
       " 'word',\n",
       " 'can',\n",
       " 'have',\n",
       " 'different',\n",
       " 'syntactic',\n",
       " 'categories',\n",
       " 'for',\n",
       " 'example',\n",
       " 'design',\n",
       " 'can',\n",
       " 'be',\n",
       " 'a',\n",
       " 'noun',\n",
       " 'or',\n",
       " 'a',\n",
       " 'verb',\n",
       " 'the',\n",
       " 'word',\n",
       " 'of',\n",
       " 'root',\n",
       " 'may',\n",
       " 'have',\n",
       " 'multiple',\n",
       " 'meanings',\n",
       " 'so',\n",
       " 'square',\n",
       " 'root',\n",
       " 'in',\n",
       " 'math',\n",
       " 'sense',\n",
       " 'or',\n",
       " 'the',\n",
       " 'root',\n",
       " 'of',\n",
       " 'a',\n",
       " 'plant',\n",
       " 'you',\n",
       " 'might',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['Transcript_Cleaned'][0] #['the']\n",
    "df['Transcript_Cleaned'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "g7-szEIjWOc2"
   },
   "outputs": [],
   "source": [
    "# now I will make embeddings for my words, let's see if it works\n",
    "# Replace Transcript_Cleaned with grams if using that method\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import  torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "results = set()\n",
    "df['Transcript_Cleaned'].apply(results.update)\n",
    "vocab_size = len(results)\n",
    "\n",
    "# Create a vocabulary dictionary\n",
    "word_to_index = {word: idx for idx, word in enumerate(results)}\n",
    "\n",
    "# Convert words to indices in your DataFrame\n",
    "# AKA Encode these\n",
    "# df['Words_indices'] = df['Transcript_Cleaned'].apply(lambda x: [word_to_index[word] for word in x])\n",
    "def words_to_indices(words):\n",
    "    return [word_to_index[word] for word in words]\n",
    "df['Words_indices'] = df['Transcript_Cleaned'].apply(words_to_indices)\n",
    "\n",
    "# Create a reverse dictionary\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "# Function to convert indices back to words\n",
    "def indices_to_words(indices):\n",
    "    return [index_to_word[idx] for idx in indices]\n",
    "\n",
    "# Aka Decode this column\n",
    "# df['Decoded_Words'] = df['Words_indices'].apply(indices_to_words)\n",
    "\n",
    "# Pad sequences to a specified length (e.g., maxlen)\n",
    "maxlen = 200  # You can adjust this based on your data\n",
    "padded_indices = pad_sequence([torch.LongTensor(seq) for seq in df['Words_indices']], batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEqXuUrsWOc_",
    "outputId": "02d6b8c7-186d-4b14-ea97-207a81511463"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "tensor([[ 351, 3038, 2255,  ...,  538, 2255, 1643],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [1643, 1779, 2964,  ...,  551,  351,  173],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [3741, 3349,  685,  ...,  685, 2846,  587],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0]])\n",
      "tensor([[3038, 2255, 2365,  ..., 2255, 1643, 3587],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [1779, 2964,  685,  ...,  351,  173, 2255],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,    0,    0,    0],\n",
      "        [3349,  685, 1412,  ..., 2846,  587, 3759],\n",
      "        [   0,    0,    0,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "# make a batch and set up parameters\n",
    "block_size = 256\n",
    "batch_size = 128\n",
    "max_iters = 10000\n",
    "learning_rate = 1e-4\n",
    "eval_iters = 250\n",
    "# new\n",
    "n_embd = 128\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "n_head = 6\n",
    "# reduce gpu usage\n",
    "accumulation_steps = 6  # Accumulate gradients over 4 batches before performing optimization step\n",
    "\n",
    "# change to gpu\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# Flatten the padded indices used to identify each word\n",
    "data = flattened_indices = padded_indices.view(-1)\n",
    "n = int(0.8*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "# print(len(data))\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# The result has many zeros, which is normal for a padded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K28XGU2jWOdB",
    "outputId": "dcbe6df0-114d-4075-fc31-8d61f83219af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0009)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data == 0) / sum(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pNM5HczWOdF"
   },
   "source": [
    "# LLM Start Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "uQ_MlMqZw09V"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import  torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "77_3MzZVWOda"
   },
   "outputs": [],
   "source": [
    "# Estimating losses function\n",
    "@torch.no_grad()\n",
    "\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# This function is able to estimate the losses for the training iterations of the data\n",
    "# uing model.eval in order to prevent some aspects of the model to run during that time\n",
    "# It monitors the performance of loss and if it decreases per every iteration\n",
    "# These losses print out when model is running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bYchx6UhWOdc",
    "outputId": "30bf87c0-cd67-43d6-85a2-adaed3202d3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(block_size, block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "wPptlyWgWOdo"
   },
   "outputs": [],
   "source": [
    "# Scaled dot product attention\n",
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        # These 3 variables, key, query, and value, are all very important in calculating attention\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        # This \"buffer\" is used as a mask for future tokens in attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        # B = batch size, T = sequence length, C = original embedding dimension\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # Even though k and q are initialized in the same way, nn.Linear randomizes the initial weights to create these tensors\n",
    "\n",
    "        # create attention scores\n",
    "        weights = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        # this is used to caclulate attention scores, a standard formula of getting dot product of q and k\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        # this sets attention scores for future tokens to -inf (negative infinity), so that the model only pays attention to the old scores and not the new ones\n",
    "        #  it overwrites the attention scores calculated in the previous step for future tokens. In each iteration, the model is exposed to a partially\n",
    "        # revealed sequence, allowing it to attend only to past tokens. This is a form of autoregressive training.\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        # normalized step\n",
    "        drop = self.dropout(weights)\n",
    "        # step to randomly ignore random nodes in order to prevent overfitting and codependence\n",
    "\n",
    "        # weighted aggregation of values\n",
    "        v = self.value(x)\n",
    "        out = drop @ v\n",
    "        # the attention scores in drop are dot product with the value tesnor to get the final outputs\n",
    "        return out\n",
    "\n",
    "# Note:\n",
    "# In transformers, the key and query vectors are typically designed to be similar to capture relevant information in both directions. They are used to\n",
    "# calculate the attention scores, indicating how much each element in the sequence should attend to every other element. The similarity between key and\n",
    "# query helps the model learn dependencies in both directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Mby0-kk4WOdr"
   },
   "outputs": [],
   "source": [
    "# Multi-head attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        # this is a linear layer that concats all the heads created and mushes them togetherinto shape n_embd\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # for every head, x is passed into self.heads aka the Head class\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "J6b2bNzpWOdv"
   },
   "outputs": [],
   "source": [
    "# Creating a feedforward class\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd*4, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Not super complicated, just a simple part of the transformer structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "PfxehI2jWOdy"
   },
   "outputs": [],
   "source": [
    "# Creating a transformer block\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.attention = MultiHeadAttention(n_head, head_size)\n",
    "        self.feedforward = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.attention(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.feedforward(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x\n",
    "\n",
    "# As we can see, the initialization of this class basically includes the previous structures we made\n",
    "# So for the forward fucntion, everytime we pass our input through either multiheadattention, or feedforward\n",
    "# we need to linearize it, so that iw what we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "3Jk66UQpWOdz"
   },
   "outputs": [],
   "source": [
    "# Now to make a GPT model\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # Create a self vocab_size variable to save it into the class\n",
    "        self.vocab_size = vocab_size\n",
    "        # Make an embedding table\n",
    "        self.token_embedding_table = nn.Embedding(self.vocab_size, n_embd).to(device)\n",
    "        # Adding a positional embedding table as well\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_embd).to(device)  # added new parameter, n_embd\n",
    "        # Adding 4 decoder layers\n",
    "        self.blocks = nn.Sequential(*(Block(n_embd, n_head=n_head).to(device) for _ in range(n_layer)))\n",
    "        # final layer normalization\n",
    "        self.lm_f = nn.LayerNorm(n_embd).to(device)\n",
    "        # unsure what this is below\n",
    "        self.lm_head = nn.Linear(n_embd, self.vocab_size).to(device)\n",
    "\n",
    "        # std variables to help training converge better\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    # Linear layers are initialized with normal distribution, and embedding layers are initialized with normal distribution as well.\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "        # index represents the sequence of tokens\n",
    "\n",
    "        # Add in token and positional embeddings\n",
    "        token_embd = self.token_embedding_table(index)  # (B, T, C)\n",
    "        # This layer is an embedding table for token embeddings. Given an input index (representing a token), it retrieves the corresponding embedding vector from the table.\n",
    "        pos_embd = self.positional_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
    "        # Embedding table for positional embeddings. It helps the model take into account the order or position of tokens in the sequence\n",
    "        x = token_embd + pos_embd  # (B, T, C)\n",
    "        x = self.blocks(x)  # (B, T, C)\n",
    "        x = self.lm_f(x)  # (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # model is iteratively called to predict the next token, and the predicted token is concatenated to the input sequence\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self.forward(index)\n",
    "            logits = logits[:, -1, :]\n",
    "            # print(f\"Logits shape: {logits.shape}\")\n",
    "            # These are the raw scores produced by the model before applying the softmax function. Each entry in the logits tensor represents\n",
    "            # the model's prediction for the likelihood of a particular token in the vocabulary. The dimensions of logits are (B, T, vocab_size),\n",
    "            #  where B is the batch size, T is the sequence length, and vocab_size is the size of the vocabulary.\n",
    "            probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Ensure generated index is within the vocabulary size\n",
    "            valid_indices = torch.arange(self.vocab_size).to(device)\n",
    "            # print('vocab size', vocab_size)\n",
    "            index_next = torch.multinomial(probabilities[:, valid_indices], num_samples=1)\n",
    "            index_next = valid_indices[index_next]  # Map back to the original indices\n",
    "\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "\n",
    "        return index\n",
    "\n",
    "# model = GPTLanguageModel(vocab_size)\n",
    "\n",
    "# To explain the positional and token embeddings:\n",
    "# In a Transformer, each position in the input sequence has a unique positional embedding associated with it. This positional embedding is added\n",
    "# to the token embedding of the corresponding word. If you didn't have positional embeddings and used a single embedding table for both tokens and positions,\n",
    "# the model might struggle to distinguish between words based on their positions in the sequence.\n",
    "\n",
    "# Having separate tables allows the model to learn distinct embeddings for tokens and positions. The positional embeddings can then be added to the\n",
    "# token embeddings during processing, ensuring that the model can effectively capture both the semantic content of words and their positions in the sequence.\n",
    "\n",
    "# So, even if you're working with a single sequence (no batches), having separate token and positional embeddings is still beneficial for the\n",
    "# Transformer model's ability to understand and leverage both semantic and positional information.\n",
    "\n",
    "# in order to deal with a prompt, make the GPT model encounter a prompt size of around 50.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "zU8Uj0fFWOd1"
   },
   "outputs": [],
   "source": [
    "# # Do not run this and next 2 cells, if loading in model\n",
    "\n",
    "# # create the model\n",
    "# max_prompt_size = 50\n",
    "# model = GPTLanguageModel(vocab_size + max_prompt_size)\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "atcHZOpFWOeJ"
   },
   "outputs": [],
   "source": [
    "# # Creating an Optimizer\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # save the best params\n",
    "# best_val_loss = float('inf')  # Initialize with a large value\n",
    "# best_params = None\n",
    "\n",
    "# for iter in range(max_iters):\n",
    "#     if iter % eval_iters == 0:\n",
    "#         losses = estimate_loss()\n",
    "#         train_loss = losses['train']\n",
    "#         val_loss = losses['val']\n",
    "#         print(f\"step {iter}, train loss: {train_loss}, val loss: {val_loss}\")\n",
    "#     xb, yb = get_batch('train')\n",
    "\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         best_params = model.state_dict()  # Save the current model parameters\n",
    "#         print(\"Updated parameters \", best_params.keys())\n",
    "\n",
    "\n",
    "#     logits, loss = model.forward(xb, yb)\n",
    "#     optimizer.zero_grad(set_to_none=True)\n",
    "#     # clears the gradients of all optimized parameters\n",
    "#     loss.backward()\n",
    "#     # implement accumulation stepse\n",
    "#     if (iter + 1) % accumulation_steps == 0:\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "# print(loss.item())\n",
    "\n",
    "# # What we are doing here is the same thing as training the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "dYgjNOFTWOeO"
   },
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Save the model into a pickle file\n",
    "# with open('/content/drive/MyDrive/model-06.pkl', 'wb') as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "u8Km20rF37Jc"
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "\n",
    "# # Convert tensors to NumPy arrays in the model's state dictionary\n",
    "# model_state_dict_np = {key: value.cpu().numpy().tolist() if isinstance(value, torch.Tensor) else value for key, value in model_state_dict.items()}\n",
    "\n",
    "# # Save the model's state dictionary into a JSON file\n",
    "# json_path = '/content/drive/MyDrive/model-06.json'\n",
    "# with open(json_path, 'w') as json_file:\n",
    "#     json.dump(model_state_dict_np, json_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZ6C7GD4robq"
   },
   "source": [
    "# Run Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "VlE8l3AFWOeS"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import io\n",
    "\n",
    "#Change the path if needed\n",
    "path = '/content/drive/MyDrive/model-06.pkl'\n",
    "\n",
    "# Load the model, if necessary\n",
    "class CPU_Unpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "            return lambda b: torch.load(io.BytesIO(b), map_location='cpu')\n",
    "        else: return super().find_class(module, name)\n",
    "\n",
    "with open(path, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    #Use the line below instead of the line above if an error occurs \n",
    "    #model = CPU_Unpickler(f).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "-H7604xrWOeT"
   },
   "outputs": [],
   "source": [
    "# # adjust the dictionaries (ONLY RUN ONCE)\n",
    "# index_to_word.update({0: ''})\n",
    "# word_to_index[''] = word_to_index.pop('block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "zPFMqG0DWOeT"
   },
   "outputs": [],
   "source": [
    "# Model 3\n",
    "\n",
    "# So what I have done is I have removed the stopwords from being in df['Transcript_Cleaned']\n",
    "# Not entirely sure if this is a good idea but it is worth a shot\n",
    "# I am doing this in order to generate a good response from GPT form the prompt, and to do\n",
    "# that it must include words to form an actual sentence\n",
    "\n",
    "# Model 4\n",
    "\n",
    "# like Model 3, but I trained it on a GPU so should work much better\n",
    "\n",
    "# Model 5\n",
    "\n",
    "# Batch size 64, number of heads = 4\n",
    "\n",
    "# Model 6\n",
    "\n",
    "# different parameters\n",
    "# batch_size = 128 # learning_rate = 1e-4 # n_layer = 6 # n_head = 6 # accumulation_steps = 6\n",
    "# Rand this one all the way, 1000 iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "cUDGOzZsWOeV"
   },
   "outputs": [],
   "source": [
    "# Add the prompt into the dictionary used for the training dataset\n",
    "# prompt = 'Can you give me an overview on Probabilistic Latent Semantic Analysis'.split()\n",
    "prompt = 'Word association mining'\n",
    "\n",
    "# Find the maximum key in the existing dictionaries\n",
    "max_key = max(word_to_index.values()) if word_to_index else -1\n",
    "\n",
    "# Enumerate through the new words and add them to the dictionaries\n",
    "for word in prompt:\n",
    "    if word not in word_to_index:\n",
    "        max_key += 1\n",
    "        word_to_index[word] = max_key\n",
    "        index_to_word[max_key] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "98nabemcWOeW"
   },
   "outputs": [],
   "source": [
    "# Create result from prompt, as a chatbot would\n",
    "context = torch.tensor(words_to_indices(prompt), dtype=torch.long, device=device)\n",
    "# context = torch.zeros((1, 1), dtype=torch.long)\n",
    "generated_terms = indices_to_words(model.generate(context.unsqueeze(0), max_new_tokens=50)[0].tolist())\n",
    "# make sure max_new_tokens is less than block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5yQhG4pbWOeX",
    "outputId": "9170e696-c93f-49ab-de27-6fc93cf235d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "byproduct resort regime surprising himself problems disk nouns problems task grow nouns observable initiating mixture losing associated uncertain whereas management customers calculating advertisement highlight plug presentation observable agencies properly lose seek information cement resolve numerator neutral equation nouns losing typed december among problems alternate study campaigns passing continuing plug comfortable\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(generated_terms[len(prompt):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQXT4TxIWOeX",
    "outputId": "7c8a879c-9c9b-4420-944b-701fcf7456a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Vocabulary Size: 4023\n",
      "Model Vocabulary Size: 4011\n",
      "Embedding Dimension: 128\n",
      "Max Index in padded_indices: 4010\n",
      "Min Index in padded_indices: 0\n",
      "Context vector tensor([4011, 4012, 4013, 4014, 4015, 1643, 4016, 4016, 4012, 4017, 4018, 1643,\n",
      "        4019, 4018, 4012, 4020, 4015, 4021, 4018, 4020, 4018, 4020, 4022])\n",
      "Model [4011, 4012, 4013, 4014, 4015, 1643, 4016, 4016, 4012, 4017, 4018, 1643, 4019, 4018, 4012, 4020, 4015, 4021, 4018, 4020, 4018, 4020, 4022, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Prompt Indices: [4011, 4012, 4013, 4014, 4015, 1643, 4016, 4016, 4012, 4017, 4018, 1643, 4019, 4018, 4012, 4020, 4015, 4021, 4018, 4020, 4018, 4020, 4022]\n"
     ]
    }
   ],
   "source": [
    "# debugging issues with sizes and lengths\n",
    "# Check vocabulary size\n",
    "print(\"Actual Vocabulary Size:\", len(word_to_index))\n",
    "print(\"Model Vocabulary Size:\", vocab_size)\n",
    "\n",
    "# Check embedding dimension\n",
    "print(\"Embedding Dimension:\", n_embd)\n",
    "\n",
    "# Check index values\n",
    "print(\"Max Index in padded_indices:\", torch.max(padded_indices).item())\n",
    "print(\"Min Index in padded_indices:\", torch.min(padded_indices).item())\n",
    "\n",
    "print(\"Context vector\", context)\n",
    "print(\"Model\", model.generate(context.unsqueeze(0), max_new_tokens=50)[0].tolist())\n",
    "\n",
    "# Check indices added from prompt\n",
    "prompt_indices = words_to_indices(prompt)\n",
    "print(\"Prompt Indices:\", prompt_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "46Cm_UduWOea"
   },
   "outputs": [],
   "source": [
    "### Notes on model\n",
    "# model 1 is on just the grams\n",
    "# model 2 is on the full transcript training\n",
    "# model 3, I stopped removing stopwords, in order to generate a better response from the context vector\n",
    "\n",
    "# model 1 prompt: Can you give me an overview on Probabilistic Latent Semantic Analysis\n",
    "# result: word_distribution promise profile allocation justification minimize edge briefly mix environment sky\n",
    "# interest root research present tilde york engine domain light popularity likelihood bye bridge summary understood\n",
    "# separating pick quantitate sub polarity encounter observation right setting future order Can distinguish accurate\n",
    "# stick aggregate doesnt apply party photo message scientist transpose categorize\n",
    "\n",
    "# model 2 prompt: Can you give me an overview on Probabilistic Latent Semantic Analysis\n",
    "# result: assign goal clutch discovering simplification meal bomb discovery generality recalibration confidence front\n",
    "# request implement anticipate percent suppose perspective choice attribute state development sit uncertainty choose\n",
    "# viewer play regime baring doubt hash table tolerance exploitation bit guess causal moreover letter area web profile\n",
    "# rating algorithm incomplete incomplete well imbalance event motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDsZ1dgrWOeb"
   },
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "aOVCt1zD8dA0"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import GPTNeoForCausalLM, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# # Check if CUDA (GPU) is available\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# torch.cuda.set_per_process_memory_fraction(0.85)  # or any smaller fraction\n",
    "\n",
    "# model_name = 'EleutherAI/gpt-neo-1.3B'\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# model = GPTNeoForCausalLM.from_pretrained(model_name).to(device)  # Move the model to CUDA if available\n",
    "\n",
    "# train_dataset = TextDataset(\n",
    "#     tokenizer=tokenizer,\n",
    "#     file_path='/content/drive/MyDrive/all_lectures.csv',  # Replace with your fine-tuning dataset\n",
    "#     block_size=64,\n",
    "# )\n",
    "\n",
    "# data_collator = DataCollatorForLanguageModeling(\n",
    "#     tokenizer=tokenizer,\n",
    "#     mlm=False\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=TrainingArguments(\n",
    "#         output_dir='/content/drive/MyDrive/fine-tuned-model',\n",
    "#         overwrite_output_dir=True,\n",
    "#         num_train_epochs=1,\n",
    "#         per_device_train_batch_size=2,  # Adjust batch size based on GPU memory\n",
    "#         save_steps=10,  # Adjust the frequency of saving checkpoints\n",
    "#         gradient_accumulation_steps=8  # or any larger value\n",
    "#     ),\n",
    "#     data_collator=data_collator,\n",
    "#     train_dataset=train_dataset\n",
    "# )\n",
    "\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWtWMv0Yb9ei"
   },
   "source": [
    "# Flask Frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(question):\n",
    "    prompt = question\n",
    "\n",
    "    max_key = max(word_to_index.values()) if word_to_index else -1\n",
    "\n",
    "    for word in prompt:\n",
    "        if word not in word_to_index:\n",
    "            max_key += 1\n",
    "            word_to_index[word] = max_key\n",
    "            index_to_word[max_key] = word\n",
    "\n",
    "    context = torch.tensor(words_to_indices(prompt), dtype=torch.long, device=device)\n",
    "    generated_terms = indices_to_words(model.generate(context.unsqueeze(0), max_new_tokens=50)[0].tolist())\n",
    "\n",
    "    return ' '.join(generated_terms[len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Flask in c:\\users\\chris\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from Flask) (2.0.3)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from Flask) (2.11.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from Flask) (2.0.1)\n",
      "Requirement already satisfied: click>=5.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from Flask) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\chris\\anaconda3\\lib\\site-packages (from click>=5.1->Flask) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from Jinja2>=2.10.1->Flask) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [14/Dec/2023 13:41:37] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Dec/2023 13:41:48] \"POST /update HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [14/Dec/2023 13:42:02] \"POST /update HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "#This code creates the initial website\n",
    "@app.route(\"/\")\n",
    "def html():\n",
    "    return render_template(\"index.html\", question = \"Hello, welcome to our LLM! Ask a question about course material above.\", answer=\"\")\n",
    "\n",
    "#This code will update the website when the user submits a question\n",
    "@app.route(\"/update\", methods=[\"GET\",\"POST\"])\n",
    "def update():\n",
    "    question = request.form['input']\n",
    "    response = answer(question)\n",
    "    return render_template(\"index.html\", question='Question: '+question, answer='Answer: '+response)\n",
    "\n",
    "app.run()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
